%===============================================================================
% A Consolidated Overview of Evaluation and Performance Metrics for
%  Machine Learning and Computer Vision
%===============================================================================
% v0.3 - 01.07.2024 (current version): general text and figure work
% v0.2 - 11.05.2024: general text work
% v0.1 - 01.11.2023: init
%
% 2023-2024 Tobias Schlosser, Michael Friedrich, Trixy Meyer, and Danny Kowerko
%
% This manuscript is licensed under the Creative Commons Attribution 4.0
%  International (CC BY 4.0) license
%  https://creativecommons.org/licenses/by/4.0/deed
%===============================================================================




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Preamble %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% documentclass

\documentclass{article}

\usepackage[left=3cm, right=3cm, top=3cm, bottom=3cm]{geometry}


% usepackages

\usepackage{lmodern, mathtools, microtype, textgreek}

\usepackage[main=english, ngerman]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage[hyphens]{url}
\usepackage[colorlinks, citecolor=LimeGreen, pagebackref=true]{hyperref}
\usepackage[dvipsnames, table]{xcolor}

\usepackage{cite}

% https://ctan.net/macros/latex/contrib/hyperref/doc/hyperref-doc.pdf
\renewcommand*{\backref}[1]{}
\renewcommand*{\backrefalt}[4]{%
    \ifnum #3 > 0 {%
        \ifnum #3 = 1 {%
            \ifnum #1 = 1 {%
                \\ (1 citation on 1 page: #2)
            } \else {%
                \\ (1 citation on #1 pages: #2)
            } \fi
        } \else {%
            \ifnum #1 = 1 {%
                \\ (#3 citations on 1 page: #2)
            } \else {%
                \\ (#3 citations on #1 pages: #2)
            } \fi
        } \fi
    } \fi
}

\usepackage{tikz}
\usetikzlibrary{
    arrows,
    backgrounds,
    calc,
    decorations,
    calligraphy,
    fit,
    positioning,
    shapes,
    spy
}

\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usepgfplotslibrary{colorbrewer, fillbetween}

\usepackage[hang]{caption}

\usepackage{array, diagbox, float, makecell, multirow, subfig, tabularx, xfrac}

\usepackage[export]{adjustbox}

\usepackage[defaultlines=10, all]{nowidow}

\setlength{\parskip}{   1em }
\setlength{\parindent}{ 0em }

\addtolength{\skip\footins}{1cm}

\renewcommand*{\arraystretch}{1.33}

\linespread{1.1}\selectfont

\newcolumntype{P}[1]{>{ \centering  \arraybackslash }p{#1}}
\newcolumntype{Q}[1]{>{ \raggedleft \arraybackslash }p{#1}}
\newcolumntype{M}[1]{>{ \centering  \arraybackslash }m{#1}}
\newcolumntype{N}[1]{>{ \raggedleft \arraybackslash }m{#1}}
\newcolumntype{B}[1]{>{ \centering  \arraybackslash }b{#1}}
\newcolumntype{C}[1]{>{ \raggedleft \arraybackslash }b{#1}}


% ORCID

\usepackage{fontawesome5}

% With / without "\thinspace" before "\textsuperscript{...}"
% \newcommand{\ORCID}[1]{\thinspace\textsuperscript{\href{https://orcid.org/#1}{\textcolor[HTML]{A6CE39}{\faOrcid}}}}
\newcommand{\ORCID}[1]{\textsuperscript{\href{https://orcid.org/#1}{\textcolor[HTML]{A6CE39}{\faOrcid}}}}

\newcommand{\ORCIDSchlosser}{0000-0002-0682-4284} % ORCID Tobias Schlosser
\newcommand{\ORCIDFriedrich}{0000-0001-6326-4749} % ORCID Michael Friedrich
\newcommand{\ORCIDMeyer}{0000-0002-3372-1619}     % ORCID Trixy Meyer
\newcommand{\ORCIDKowerko}{0000-0002-4538-7814}   % ORCID Danny Kowerko


% Page numbering
\usepackage{fancyhdr, lastpage}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\fancypagestyle{plain}{\fancyfoot[C]{\thepage/\pageref{LastPage}}}
\fancyfoot[C]{\thepage/\pageref{LastPage}}


% Miscellaneous

\usepackage[subfigure]{tocloft}

% "How to write a perfect equation parameters description?"
% https://tex.stackexchange.com/questions/95838/how-to-write-a-perfect-equation-parameters-description
\newenvironment{conditions}[1][where:]
    {\hspace{0.02\textwidth} #1 \begin{tabular}[t]{>{$}l<{$} @{${}={}$} l}}
    {\end{tabular}\\[\belowdisplayskip]}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Preamble %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\begin{document}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Title %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{\Huge A Consolidated Overview of Evaluation and Performance Metrics for Machine Learning and Computer Vision}

\author{
    Tobias Schlosser\ORCID{\ORCIDSchlosser}, Michael Friedrich\ORCID{\ORCIDFriedrich}, Trixy Meyer\ORCID{\ORCIDMeyer}, and Danny Kowerko\ORCID{\ORCIDKowerko} \\[1ex]
    Junior Professorship of Media Computing, \\
    Chemnitz University of Technology, \\
    09107 Chemnitz, Germany, \\
    \texttt{\{firstname.lastname\}@cs.tu-chemnitz.de}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Title %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\maketitle




\section*{Abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In the rapidly evolving fields of machine learning (ML) and computer vision (CV), selecting and applying appropriate evaluation and performance metrics are crucial for advancing and validating innovative models. These metrics not only guide the development and fine-tuning of algorithms but also shape the interpretation of outcomes in diverse applications, such as automated disease diagnosis and autonomous driving. This contribution is motivated by recent advancements in artificial intelligence (AI) and deep learning (DL), particularly in big data analysis for tasks such as object detection and classification using learning-based approaches, including artificial neural networks (ANN) and deep neural networks (DNN). We present well-established evaluation metrics, detailing their advantages, disadvantages, and origins. By consolidating current knowledge and providing insights into the effective use of these metrics, this contribution aims to equip researchers and practitioners with the essential tools to critically evaluate and enhance the robustness and reliability of their models.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\clearpage




\renewcommand{\contentsname}{Table of Contents}

\tableofcontents




\clearpage




\section{Introduction and Motivation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In recent years, the fields of machine learning (ML) and deep learning (DL) have experienced substantial growth, driven by the advancements in algorithms, data availability, and computational power (see also Figures~\ref{figure:AI_publications} and \ref{figure:AI_publications_by_field}) \cite{krizhevsky2012imagenet, lecun2015deep, schmidhuber2015deep, szegedy2015going, goodfellow2016deep, bronstein2017geometric, sejnowski2018deep, liu2021survey, zaidi2022survey}. This progress has led to significant improvements in various applications, notably object detection and classification, through the adoption of artificial neural networks (ANN) and deep neural networks (DNN), which are highly regarded for their powerful feature extraction and learning capabilities. However, this rapid technological development has also introduced new challenges in evaluating and optimizing performance, particularly regarding model interpretability, accountability, and robustness \cite{doshi2017towards, samek2017explainable, gilpin2018explaining, carvalho2019machine, zhou2021evaluating, cooper2022accountability}. The effectiveness of ML and DL models in real-world scenarios depends on their ability to generalize from training data to unseen data, necessitating robust evaluation metrics that accurately reflect model performance across diverse conditions and data sets.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{The need for comprehensive evaluation metrics}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Evaluation metrics serve as fundamental tools that provide insights into the effectiveness of machine learning models, guiding the selection, tuning, and optimization of models in scientific research as well as various applications. In computer vision (CV), the complexity of tasks such as image segmentation, object recognition, and motion analysis further complicates the assessment of model performance. Traditional metrics such as accuracy or error rate are often insufficient to capture the nuances of model behavior, especially in data sets with imbalanced classes. Moreover, the dynamic nature of ML and CV, with continually evolving models and techniques, demands a standardized yet comprehensive set of metrics that can adapt to new challenges. This need in turn highlights the importance of utilizing metrics that can offer deeper insights and facilitate the effective comparison of state-of-the-art techniques \cite{ferri2009experimental}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Objectives of this work}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
This contribution provides a comprehensive overview of evaluation and performance metrics suitable for ML and CV applications. It details established metrics, examining their strengths, weaknesses, and concepts, thereby aiding in their appropriate selection and application. By consolidating an overview of traditional and novel metrics, this manuscript therefore serves as a comprehensive resource, supporting researchers and practitioners in accurately assessing and enhancing model performance.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{figure}[tbp]
    \centering

    \begin{tikzpicture}
        \colorlet{draw_color}{LimeGreen}
        \colorlet{fill_color}{LimeGreen!50}

        \begin{axis}[
            ybar, bar width=0.5cm,
            xtick=data, ymin=0, ymax=260,
            enlarge x limits=0.1,
            width=\textwidth, height=0.6\textwidth,
            symbolic x coords={2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022},
            xlabel={Year}, ylabel={Number of publications [thousand]},
            nodes near coords,
            every node near coord/.append style={font=\footnotesize, /pgf/number format/.cd, fixed zerofill, precision=2}
        ]
            % Data: https://drive.google.com/drive/folders/1jcFRe6edMYPwmNIHZiY2JHiNEaj4-xWG, "fig_1.1.1.csv"
            \addplot[draw=draw_color, fill=fill_color] coordinates {
                (2010,  87.797) (2011,  87.277) (2012,  89.097) (2013,  92.240) (2014,  97.377) 
                (2015, 104.872) (2016, 108.645) (2017, 120.519) (2018, 143.396) (2019, 177.182) 
                (2020, 201.635) (2021, 239.594) (2022, 242.289)
            };
        \end{axis}
    \end{tikzpicture}

    \caption{Number of AI publications worldwide from 2010 to 2022 \cite[p. 31]{perrault2024artificial}.}
    \label{figure:AI_publications}
\end{figure}


\begin{figure}[tbp]
    \centering

    \begin{tikzpicture}
        \begin{axis}[
            xtick=data, ymin=0, ymax=80,
            enlarge x limits=0.1, enlarge y limits=0.1,
            width=\textwidth, height=0.6\textwidth,
            symbolic x coords={2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022},
            xlabel={Year}, ylabel={Number of publications [thousand]},
            xtick align=outside,
            legend pos=north west, legend cell align=left
        ]
            % Data: https://drive.google.com/drive/folders/1jcFRe6edMYPwmNIHZiY2JHiNEaj4-xWG, "fig_1.1.3.csv"


            \addplot coordinates {(2010,6.229) (2011,6.237) (2012,6.894) (2013,7.614) (2014,8.676) (2015,10.504) (2016,12.78) (2017,17.539) (2018,27.132) (2019,39.165) (2020,50.81) (2021,65.248) (2022,72.229)};
            \addlegendentry{Machine learning}

            \addplot coordinates {(2010,10.355) (2011,10.948) (2012,11.456) (2013,12.386) (2014,12.634) (2015,13.246) (2016,12.034) (2017,12.178) (2018,13.598) (2019,15.83) (2020,16.59) (2021,19.562) (2022,21.309)};
            \addlegendentry{Computer vision}


            \addplot coordinates {(2010,9.839) (2011,10.068) (2012,10.604) (2013,11.404) (2014,12.274) (2015,12.814) (2016,12.397) (2017,12.791) (2018,13.967) (2019,16.142) (2020,16.646) (2021,19.284) (2022,19.841)};
            \addlegendentry{Pattern recognition}

            \addplot coordinates {(2010,11.337) (2011,11.556) (2012,11.95) (2013,12.157) (2014,12.165) (2015,12.56) (2016,13.063) (2017,12.848) (2018,14.516) (2019,15.226) (2020,15.045) (2021,12.478) (2022,12.052)};
            \addlegendentry{Process management}

            \addplot coordinates {(2010,1.099) (2011,1.034) (2012,1.089) (2013,1.172) (2014,1.248) (2015,1.268) (2016,1.702) (2017,2.721) (2018,4.658) (2019,6.816) (2020,7.954) (2021,9.756) (2022,10.386)};
            \addlegendentry{Computer network}

            \addplot coordinates {(2010,5.172) (2011,5.559) (2012,5.912) (2013,6.397) (2014,6.776) (2015,7.065) (2016,6.897) (2017,6.871) (2018,7.391) (2019,8.154) (2020,8.253) (2021,9.059) (2022,9.171)};
            \addlegendentry{Control theory}

            \addplot coordinates {(2010,6.341) (2011,6.072) (2012,6.237) (2013,6.381) (2014,6.579) (2015,6.6) (2016,6.051) (2017,6.183) (2018,6.702) (2019,7.222) (2020,7.258) (2021,8.214) (2022,8.309)};
            \addlegendentry{Algorithm}

            \addplot coordinates {(2010,3.771) (2011,3.815) (2012,4.237) (2013,4.233) (2014,4.857) (2015,4.782) (2016,4.854) (2017,4.504) (2018,5.002) (2019,5.396) (2020,6.399) (2021,6.807) (2022,7.176)};
            \addlegendentry{Linguistics}

            \addplot coordinates {(2010,2.742) (2011,2.797) (2012,2.929) (2013,3.237) (2014,3.455) (2015,3.914) (2016,3.85) (2017,4.071) (2018,4.622) (2019,5.504) (2020,6.093) (2021,6.777) (2022,6.832)};
            \addlegendentry{Mathematical optimization}
        \end{axis}
    \end{tikzpicture}

    \caption{Number of AI publications worldwide from 2010 to 2022 by field of study \cite[p. 33]{perrault2024artificial}.}
    \label{figure:AI_publications_by_field}
\end{figure}




\clearpage




% "Table of equations like list of figures"
% https://tex.stackexchange.com/questions/173102/table-of-equations-like-list-of-figures

\newcommand{\listequationsname}{\Large List of Equations}
\newlistof{myequations}{equ}{\listequationsname}
\newcommand{\myequations}[1]{\addcontentsline{equ}{myequations}{\protect\numberline{\theequation}#1}\par}
\setlength{\cftmyequationsnumwidth}{2.5em}

% \listofmyequations




\clearpage




\section{Machine Learning}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In the continuously evolving landscape of machine learning, the evaluation of predictive models through various evaluation metrics is integral to understanding their performance and guiding their optimization. Evaluation and performance metrics such as precision, recall, and accuracy provide foundational insights, while more nuanced metrics address specific aspects of model behavior and class imbalances.

For example, precision \cite{altman1994statistics, fletcher2019clinical}, also known as the positive predictive value, gauges the accuracy of positive predictions made by a model, indicating the proportion of true positive results in all positive classifications. This evaluation metric is vital in contexts where the cost of false positives is high, such as in medical diagnostics \cite{kononenko2001machine, ahsan2022machine} or fraud detection \cite{ashtiani2021intelligent, ali2022financial}. To accommodate the characteristics of diverse data sets, precision is often calculated in one of three forms: macro average \cite{yang1999evaluation, sebastiani2002machine, zhu2004recall, he2018local}, micro average \cite{yang1999evaluation, sebastiani2002machine}, or weighted average precision \cite{han2014rule}. Macro average precision treats all classes equally, averaging the precision calculated for each class, which in turn prevents dominant classes from overshadowing smaller ones. Micro average precision aggregates the contributions of all classes to compute the overall precision, which is ideal for handling class imbalances. Weighted average precision, however, assigns a weight to each class's precision based on its representation in the data set, providing a balance between treating all classes equally and recognizing their different sizes.

Recall \cite{yerushalmy1947statistical, altman1994diagnostic}, or the true positive rate, complements precision by measuring the ability to detect all actual positives. It is crucial for applications where missing a positive instance carries severe consequences, such as detecting rare diseases. Like precision, recall can also be categorized into macro \cite{yang1999evaluation, sebastiani2002machine, rosenberg2012classifying, yang2020edgernn}, micro \cite{yang1999evaluation, sebastiani2002machine}, and weighted averages \cite{gordon1988effect, han2014rule} to adapt to various needs and data set structures. Negative predictive value \cite{altman1994statistics, fletcher2019clinical} and true negative rate \cite{yerushalmy1947statistical, altman1994diagnostic} measure the accuracy of negative predictions and the ability to identify negatives, respectively, which may help in rounding out the model's ability to correctly predict both classes. Accuracy \cite{metz1978basic, taylor1997introduction} provides an overall measure of model performance but can be misleading in unbalanced data sets. Balanced accuracy \cite{brodersen2010balanced, kelleher2020fundamentals} and its variations, including weighted balanced accuracy \cite{salman2017detection, infante2023factors}, provide a more trustworthy assessment by considering sensitivity (recall) and specificity (true negative rate) adjusted for prevalence \cite{rothman2012epidemiology, bruce2018quantitative}. The F-score \cite{van2004geometry, taha2015metrics}, including its different variations such as the F1-score \cite{van2004geometry, taha2015metrics} and the F2-score \cite{van2004geometry, taha2015metrics}, combines precision and recall in a harmonic mean, balancing the trade-off between them. It is especially useful when seeking a model that reliably balances false positives and false negatives. Error metrics including the false discovery rate \cite{benjamini1995controlling, benjamini2001control} and false omission rate \cite{zafar2017fairness} delve deeper into the types of errors a model may show, providing insights that can be critical for refining model performance. The false discovery rate, for example, focuses on the proportion of false positives among the positive results, which is particularly relevant in scientific testing where validation is costly.

Further sophistication in evaluation metrics is seen in measures such as the diagnostic odds ratio \cite{glas2003diagnostic, doust2004systematic}, which compares the odds of positive test results, and the Fowlkes--Mallows index \cite{fowlkes1983method, halkidi2001clustering}, which measures the similarity between the predicted and true classifications. These metrics provide additional layers of understanding in scenarios where a simple accuracy is insufficient. Further metrics including the positive likelihood ratio \cite{swets1973relative, deeks2004diagnostic} and the negative likelihood ratio \cite{swets1973relative, deeks2004diagnostic} provide evidence strength that a given condition or feature corresponds to a class, which is useful in diagnostic applications. The informedness or bookmaker informedness \cite{peirce1884numerical, youden1950index} provides, as opposed to random guessing, a measure of decision effectiveness.

Collectively, these metrics offer a multidimensional framework. They not only assess basic performance but also provide insights into the behavior of models across different scenarios, fostering more informed and effective machine learning applications.

The following evaluation metrics within the context of machine learning are motivated by the contributions of \textit{Metz} \cite{metz1978basic} and \textit{Fawcett} \cite{fawcett2006introduction}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{General}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Figure~\ref{figure:ML_general} and Table~\ref{table:ML_general} give an overview of our general definitions for ML-related metrics.

\begin{figure}[H]
    \centering

    \scalebox{0.9}{\begin{tikzpicture}[
     c/.style={draw, circle},
     r/.style={draw, rectangle}]
        \node[r, fill=Yellow!90,   minimum width=6.5cm, minimum height=7.5cm] (r) at (0, 0) {};
        \node[c, fill=YellowGreen, minimum size=5.5cm, thick]                 (c) at (0, 0) {};
        \draw (0, -3.75) -- (0, 3.75);

        \node (ae) at (6,  3) {all elements};
        \node (se) at (6, -3) {selected elements};
        \draw (ae) -- (r);
        \draw (se) -- (c);

        \node (P)  at (-2, 4.25) {P};
        \node (N)  at ( 2, 4.25) {N};
        \node (TP) at (-1, 0)    {TP};
        \node (TN) at ( 2, 3)    {TN};
        \node (FP) at ( 1, 0)    {FP};
        \node (FN) at (-2, 3)    {FN};
    \end{tikzpicture}}

    \caption{General definitions machine learning.}
    \label{figure:ML_general}
\end{figure}

\begin{table}[H]
    \centering

    \begin{tabular}{|c|c|}
        \hline
        Abbreviation & Meaning \\
        \hline
        %
        \hline
        $\textit{T}$  & Total                       \\
        $\textit{P}$  & Positives                   \\
        $\textit{N}$  & Negatives                   \\
        $\textit{TP}$ & True Positives              \\
        $\textit{TN}$ & True Negatives              \\
        $\textit{FP}$ & False Positives             \\
        $\textit{FN}$ & False Negatives             \\
        $n$           & Number of values or classes \\
        \hline
    \end{tabular}

    \caption{General definitions machine learning.}
    \label{table:ML_general}
\end{table}


\subsection[Precision~/ positive predictive value (PPV)]{Precision~/ positive predictive value (PPV) \cite{altman1994statistics, fletcher2019clinical}}

Precision \cite{altman1994statistics, fletcher2019clinical} is an essential evaluation metric used to assess the accuracy of classification models, particularly focusing on the correctness of positive predictions. It measures the proportion of predicted positive instances that are truly positive, therefore providing critical insights into the model's performance in contexts where the cost of a false positive is significant, such as in spam detection or legal proceedings. By quantifying how many of the model's positive classifications are accurate, precision determines the reliability of the model's positive predictions. A high precision score indicates that the model is effective in minimizing false positives, thereby ensuring that most of its positive predictions are trustworthy. This metric is particularly valuable when the consequences of erroneous positive predictions are costly or disruptive. However, maximizing precision alone can sometimes lead to a model that is overly conservative in its positive predictions, potentially missing genuine positive cases (low recall). Therefore, in practice, precision is often considered alongside recall to optimize both the accuracy and coverage of the model's predictions, commonly evaluated through the F1-score, which harmonizes the trade-off between precision and recall to provide a more holistic view of model performance.

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Fraction of true positives and false positives. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & Minimizes false positives. \\
        \textcolor{Green}{$+$} & Useful for critical false positive sensitive applications. \\
        \textcolor{Red}{$-$}   & Ignores true negatives and false negatives. \\
        \textcolor{Red}{$-$}   & May overlook recall for higher precision.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{Precision} = \dfrac{\textit{TP}}{\textit{TP} + \textit{FP}}
%
    \label{equation:precision}
\end{equation}
\myequations{Precision / positive predictive value (PPV)}


\subsubsection[Macro average precision (APmacro)]{Macro average precision (AP\textsubscript{macro}) \cite{yang1999evaluation, sebastiani2002machine, zhu2004recall, he2018local}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Macro average precision calculates the average precision for each class separately and takes the average over all classes. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & Each class is weighted equally regardless of its frequency in the data set, therefore allowing each class to be evaluated individually. \\
        \textcolor{Green}{$+$} & Advantageous when the recognition rate of TP compared to FP is of interest. \\
        \textcolor{Red}{$-$}   & Rare classes have the same weight as frequent classes. This can lead to bias. \\
        \textcolor{Red}{$-$}   & Only partially evaluates a model's classification capabilities.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{AP}_\textit{macro} = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \textit{Precision}_i = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \dfrac{\textit{TP}_i}{\textit{TP}_i + \textit{FP}_i}
%
    \label{equation:MAAP}
\end{equation}
\myequations{Macro average precision (AP\textsubscript{macro})}


\subsubsection[Micro average precision (APmicro)]{Micro average precision (AP\textsubscript{micro}) \cite{yang1999evaluation, sebastiani2002machine}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Micro average precision calculates the precision across all classes. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & Use with inconsistent data sets / class distributions. Provides overview of overall model performance. \\
        \textcolor{Red}{$-$}   & Neglects infrequent classes and overestimates frequent classes.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{AP}_\textit{micro} = \dfrac{\sum\nolimits_{i = 1}^n \textit{TP}_i}{\sum\nolimits_{i = 1}^n (\textit{TP}_i + \textit{FP}_i)}
%
    \label{equation:MIAP}
\end{equation}
\myequations{Micro average precision (AP\textsubscript{micro})}


\subsubsection[Weighted average precision (APweighted)]{Weighted average precision (AP\textsubscript{weighted}) \cite{han2014rule}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Weighted average precision calculates the weighted precision across all classes. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & Use with inconsistent data sets / class distributions. Provides overview of overall model performance. \\
        \textcolor{Red}{$-$}   & Neglects under-weighted classes and overestimates over-weighted classes.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{AP}_\textit{weighted} = \dfrac{1}{\sum\nolimits_{i = 1}^n w_i} \cdot \sum\nolimits_{i = 1}^n w_i \cdot \textit{Precision}_i = \dfrac{1}{\sum\nolimits_{i = 1}^n w_i} \cdot \sum\nolimits_{i = 1}^n w_i \cdot \dfrac{\textit{TP}_i}{\textit{TP}_i + \textit{FP}_i}
%
    \label{equation:WAP}
\end{equation}
%
\begin{conditions}
    w_i & samples per class or the relation of samples per class to the total of all samples
\end{conditions}
\myequations{Weighted average precision (AP\textsubscript{weighted})}


\subsection[Negative predictive value (NPV)]{Negative predictive value (NPV) \cite{altman1994statistics, fletcher2019clinical}}

The negative predictive value (NPV) \cite{altman1994statistics, fletcher2019clinical} is a crucial evaluation metric in diagnostic testing, quantifying the likelihood that a negative test result accurately reflects the absence of a condition. NPV is defined as the ratio of true negatives to the total number of negative test results (true negatives plus false negatives). It is significant in medical diagnostics and epidemiology, as it directly impacts clinical decision-making and patient management. High NPV indicates that a negative test result can reliably exclude a disease, thereby reducing unnecessary treatments and patient anxiety. Conversely, low NPV suggests a higher chance of false negatives, potentially leading to missed diagnoses and subsequent harm. NPV is dependent on the prevalence of the condition within the tested population, with higher prevalence typically reducing NPV. While NPV provides valuable insights into test performance, it must be interpreted alongside other metrics such as the positive predictive value (PPV), the true positive rate (TPR), and the true negative rate (TNR) for a comprehensive evaluation of diagnostic accuracy. Understanding and optimizing NPV is therefore important for developing reliable screening tools and enhancing public health outcomes.

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Fraction of true negatives and false negatives. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & Measures true negatives accuracy. \\
        \textcolor{Green}{$+$} & Useful for low-prevalence conditions. \\
        \textcolor{Red}{$-$}   & Neglects true positives and false positives. \\
        \textcolor{Red}{$-$}   & Less informative with high false negatives.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{NPV} = \dfrac{\textit{TN}}{\textit{TN} + \textit{FN}}
%
    \label{equation:NPV}
\end{equation}
\myequations{Negative predictive value (NPV)}


\subsection[Recall~/ true positive rate (TPR)~/ sensitivity~/ hit rate]{Recall~/ true positive rate (TPR)~/ sensitivity~/ hit rate \cite{yerushalmy1947statistical, altman1994diagnostic}}

Recall \cite{yerushalmy1947statistical, altman1994diagnostic} particularly emphasizes the model's ability to identify all relevant instances within a data set. Commonly used in contexts where the cost of missing a positive case (false negative) is high, such as in medical diagnostics or fraud detection, recall calculates the proportion of actual positives that have been correctly identified by the model. It serves as a critical measure by quantifying the model's sensitivity to detecting positive samples amidst a pool of negatives, thereby highlighting potential weaknesses in capturing all pertinent cases. A high recall score indicates that the model effectively captures the majority of positive cases, which is paramount in scenarios where failing to detect positives could have severe consequences. Consequently, optimizing for recall is essential in fine-tuning the performance of models deployed in high-stakes environments, ensuring that few positive cases go unnoticed. However, focusing solely on recall can sometimes lead to a decrease in precision, whereas the model might also increase the number of false positives. Therefore, recall is often balanced with other metrics such as precision to provide a more comprehensive understanding of a model's overall performance.

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Fraction of true positives and false negatives. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & Ensures minimal false negatives. \\
        \textcolor{Green}{$+$} & Critical for identifying rare positive cases. \\
        \textcolor{Red}{$-$}   & Ignores true negatives and false positives. \\
        \textcolor{Red}{$-$}   & May overlook precision for higher recall.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{Recall} = \dfrac{\textit{TP}}{\textit{P}} = \dfrac{\textit{TP}}{\textit{TP} + \textit{FN}}
%
    \label{equation:recall}
\end{equation}
\myequations{Recall / true positive rate (TPR) / sensitivity / hit rate}


\subsubsection[Macro average recall (ARmacro)]{Macro average recall (AR\textsubscript{macro}) \cite{yang1999evaluation, sebastiani2002machine, rosenberg2012classifying, yang2020edgernn}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Macro average recall calculates the average recall for each class separately and takes the average over all classes. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & Each class is weighted equally regardless of its frequency in the data set, therefore allowing each class to be evaluated individually. \\
        \textcolor{Green}{$+$} & Advantageous when the recognition rate of TP compared to FN is of interest. \\
        \textcolor{Red}{$-$}   & Rare classes have the same weight as frequent classes. This can lead to bias. \\
        \textcolor{Red}{$-$}   & Only partially evaluates a model's classification capabilities.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{AR}_\textit{macro} = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \textit{Recall}_i = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \dfrac{\textit{TP}_i}{\textit{TP}_i + \textit{FN}_i}
%
    \label{equation:MAAR}
\end{equation}
\myequations{Macro average recall (AR\textsubscript{macro})}


\subsubsection[Micro average recall (ARmicro)]{Micro average recall (AR\textsubscript{micro}) \cite{yang1999evaluation, sebastiani2002machine}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Micro average recall calculates the recall across all classes. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & Use with inconsistent data sets / class distributions. Provides overview of overall model performance. \\
        \textcolor{Red}{$-$}   & Neglects infrequent classes and overestimates frequent classes.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{AR}_\textit{micro} = \dfrac{\sum\nolimits_{i = 1}^n \textit{TP}_i}{\sum\nolimits_{i = 1}^n (\textit{TP}_i + \textit{FN}_i)}
%
    \label{equation:MIAR}
\end{equation}
\myequations{Micro average recall (AR\textsubscript{micro})}


\subsubsection[Weighted average recall (ARweighted)]{Weighted average recall (AR\textsubscript{weighted}) \cite{gordon1988effect, han2014rule}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Weighted average recall calculates the weighted recall across all classes. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & Use with inconsistent data sets / class distributions. Provides overview of overall model performance. \\
        \textcolor{Red}{$-$}   & Neglects under-weighted classes and overestimates over-weighted classes.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{AR}_\textit{weighted} = \dfrac{1}{\sum\nolimits_{i = 1}^n w_i} \cdot \sum\nolimits_{i = 1}^n w_i \cdot \textit{Recall}_i = \dfrac{1}{\sum\nolimits_{i = 1}^n w_i} \cdot \sum\nolimits_{i = 1}^n w_i \cdot \dfrac{\textit{TP}_i}{\textit{TP}_i + \textit{FN}_i}
%
    \label{equation:WAR}
\end{equation}
%
\begin{conditions}
    w_i & samples per class or the relation of samples per class to the total of all samples
\end{conditions}
\myequations{Weighted average recall (AR\textsubscript{weighted})}


\subsection[True negative rate (TNR)~/ specificity~/ selectivity]{True negative rate (TNR)~/ specificity~/ selectivity \cite{yerushalmy1947statistical, altman1994diagnostic}}

The true negative rate (TNR) \cite{yerushalmy1947statistical, altman1994diagnostic} measures the proportion of correctly identified negatives. It is calculated as the ratio of true negatives to the sum of true negatives and false positives, providing insight into a test's ability to, for instance, exclude individuals without a condition. High TNR indicates that the test is effective in accurately identifying individuals without a condition, reducing the risk of false positives that could lead to unnecessary further testing and treatment. TNR is particularly valuable in screening programs where overdiagnosis must be minimized. However, as noted before, metrics such as TNR must be interpreted in conjunction with metrics such as the true positive rate (TPR), as a test with high TNR but low TPR may fail to detect many true cases, leading to false security. TNR is essential for a balanced evaluation of diagnostic tools, ensuring that they not only identify the diseased population effectively but also protect healthy individuals from the repercussions of incorrect positive results.

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Fraction of true negatives and false positives. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & Emphasizes the accuracy of classifying true negatives in relation to false positives. \\
        \textcolor{Green}{$+$} & Reduces unnecessary follow-up tests and treatments in medical diagnostics. \\
        \textcolor{Red}{$-$}   & Neglects true positives and false negatives. \\
        \textcolor{Red}{$-$}   & Must be balanced with TPR.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{TNR} = \dfrac{\textit{TN}}{\textit{N}} = \dfrac{\textit{TN}}{\textit{TN} + \textit{FP}}
%
    \label{equation:TNR}
\end{equation}
\myequations{True negative rate (TNR) / specificity / selectivity}


\subsection[Prevalence]{Prevalence \cite{rothman2012epidemiology, bruce2018quantitative}}

Prevalence \cite{rothman2012epidemiology, bruce2018quantitative} refers to the proportion of instances in a data set belonging to a specific class. It is a crucial metric for understanding the baseline distribution of classes, particularly in classification problems. Prevalence directly impacts model performance and evaluation, as imbalanced data sets, where one class significantly outweighs others, can lead to biased models that perform well on the majority class but poorly on minority classes. The accurate measurement of prevalence is essential for selecting appropriate algorithms, designing effective sampling strategies, and applying techniques such as resampling, class weighting, or synthetic data generation to mitigate imbalance. Moreover, prevalence influences evaluation metrics such as accuracy, precision, and recall, which may be misleading in the presence of class imbalance. For instance, high accuracy in an imbalanced data set may simply reflect the model's ability to predict the majority class correctly, ignoring minority class performance. Hence, understanding and addressing prevalence in machine learning data sets is fundamental to developing robust, fair, and generalizable models that perform well across all classes. Properly accounting for prevalence ensures a more accurate model evaluation and a better real-world applicability.

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Fraction of positives with positives and negatives. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & Identifies class distribution imbalances. \\
        \textcolor{Green}{$+$} & Guides selection of appropriate algorithms and techniques. \\
        \textcolor{Red}{$-$}   & Imbalance can bias model performance. \\
        \textcolor{Red}{$-$}   & Neglects TP, TN, FP, and FN.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{Prevalence} = \dfrac{\textit{P}}{\textit{T}} = \dfrac{\textit{P}}{\textit{P} + \textit{N}}
%
    \label{equation:prevalence}
\end{equation}
\myequations{Prevalence}


\subsection[Accuracy (A)]{Accuracy (A) \cite{metz1978basic, taylor1997introduction}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Fraction of correct classifications. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & Easy to interpret and calculate. \\
        \textcolor{Green}{$+$} & Suitable for balanced data sets. \\
        \textcolor{Red}{$-$}   & Misleading with imbalanced data. \\
        \textcolor{Red}{$-$}   & Ignores false positive and negative impacts.
    \end{tabularx}
\end{table}

\begin{equation}
    A = \dfrac{\textit{TP} + \textit{TN}}{\textit{T}} = \dfrac{\textit{TP} + \textit{TN}}{\textit{P} + \textit{N}} = \dfrac{\textit{TP} + \textit{TN}}{\textit{TP} + \textit{TN} + \textit{FP} + \textit{FN}}
%
    \label{equation:A}
\end{equation}
\myequations{Accuracy (A)}


\subsection[Balanced accuracy (BA)]{Balanced accuracy (BA) \cite{brodersen2010balanced, kelleher2020fundamentals}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Balanced fraction of correct classifications. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & Less biased by class distribution. \\
        \textcolor{Red}{$-$}   & Neglects, e.g., FPR and FNR.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{BA}_\textit{binary} = \dfrac{\textit{TPR} + \textit{TNR}}{2} = \dfrac{\dfrac{\textit{TP}}{\textit{TP} + \textit{FN}} + \dfrac{\textit{TN}}{\textit{TN} + \textit{FP}}}{2}
%
    \label{equation:BA_binary}
\end{equation}
\myequations{Balanced accuracy (BA) for binary classification}

\begin{equation}
    \textit{BA}_\textit{multi} = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \textit{Recall}_i
%
    \label{equation:BA_multi}
\end{equation}
\myequations{Balanced accuracy (BA) for multi-class classification}


\subsection[Balanced accuracy weighted (BAW)]{Balanced accuracy weighted (BAW) \cite{salman2017detection, infante2023factors}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Building upon the balanced accuracy approach, an additional class weighting is added. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & Ideal for multi-class problems and robust against class imbalance. \\
        \textcolor{Red}{$-$}   & Requires precise weight calculation.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{BAW} = \dfrac{1}{\sum\nolimits_{i = 1}^n w_i} \cdot \sum\nolimits_{i = 1}^n w_i \cdot \textit{Recall}_i = \dfrac{1}{\sum\nolimits_{i = 1}^n w_i} \cdot \sum\nolimits_{i = 1}^n w_i \cdot \dfrac{\textit{TP}_i}{\textit{TP}_i + \textit{FN}_i}
%
    \label{equation:BAW}
\end{equation}
%
\begin{conditions}
    w_i & samples per class or the relation of samples per class to the total of all samples
\end{conditions}
\myequations{Balanced accuracy weighted (BAW)}


\subsection[Average accuracy (AA)]{Average accuracy (AA) \cite{brodersen2010balanced, huang2019ecg}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Average accuracy over all classes. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & Ideal for data sets with balanced classes. \\
        \textcolor{Red}{$-$}   & Can yield poor results when a biased classifier is tested on imbalanced data.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{AA} = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n A_i = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \dfrac{\textit{TP}_i + \textit{TN}_i}{\textit{TP}_i + \textit{FP}_i + \textit{TN}_i + \textit{FN}_i}
%
    \label{equation:AA}
\end{equation}
\myequations{Average accuracy (AA)}


\subsection[Average class accuracy (ACA)]{Average class accuracy (ACA) \cite{bhowan2011developing, devarriya2020unbalanced}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Calculates a weighted average classification accuracy based on a minority accuracy corresponding to TPR and a majority accuracy corresponding to TNR. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & Suitable when a significant class imbalance between minority and majority classes exists. \\
        \textcolor{Red}{$-$}   & Difficult to choose a good weighting factor.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{ACA} = w \cdot \textit{TPR} + (1 - w) \cdot \textit{TNR} = w \cdot \dfrac{\textit{TP}}{\textit{TP} + \textit{FN}} + (1 - w) \cdot \dfrac{\textit{TN}}{\textit{TN} + \textit{FP}}
%
    \label{equation:ACA}
\end{equation}
%
\begin{conditions}
    w & weight of the positive class in relation to the data set, $0 \le w \le 1$
\end{conditions}
\myequations{Average class accuracy (ACA)}


\subsection[Error rate (ER)]{Error rate (ER) \cite{hand1986recent, asri2016using}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Complementary metric to accuracy. All faulty classifications are divided by the total number of classifications. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & Suitable for problems where the evaluation of error recognition is of importance. \\
        \textcolor{Red}{$-$}   & Poor performance for imbalanced data.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{ER} = \dfrac{\textit{FP} + \textit{FN}}{\textit{T}} = \dfrac{\textit{FP} + \textit{FN}}{\textit{P} + \textit{N}} = \dfrac{\textit{FP} + \textit{FN}}{\textit{TP} + \textit{FP} + \textit{TN} + \textit{FN}}
%
    \label{equation:ER}
\end{equation}
\myequations{Error rate (ER)}


\subsection[Average error rate (AER)]{Average error rate (AER) \cite{hamamoto1998gabor, han2016variable}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Average error rate over all classes. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & Suitable for problems where the evaluation of error recognition is of importance. \\
        \textcolor{Red}{$-$}   & Poor performance for imbalanced data.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{AER} = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \textit{ER}_i = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \dfrac{\textit{FP}_i + \textit{FN}_i}{\textit{TP}_i + \textit{FP}_i + \textit{TN}_i + \textit{FN}_i}
%
    \label{equation:AER}
\end{equation}
\myequations{Average error rate (AER)}


\subsection[F-score~/ F\textbeta-score]{F-score~/ F\textbeta-score \cite{van2004geometry, taha2015metrics}}

The F-score \cite{van2004geometry, taha2015metrics}, or F\textbeta-score, serves as a versatile evaluation metric in classification tasks, allowing for the weighted balancing of precision and recall according to specific application needs. It introduces the parameter \textbeta, which adjusts the emphasis placed on recall relative to precision. A \textbeta{} greater than one prioritizes recall, making it particularly useful in contexts where missing a positive instance carries severe repercussions, such as in medical diagnostics or fraud detection. Conversely, a \textbeta{} less than one shifts the focus towards precision, suitable for applications where false positives are more detrimental, such as in spam filtering. By calculating the weighted harmonic mean of precision and recall, the F\textbeta-score quantifies the trade-offs between both metrics, providing a nuanced view of model performance. High values of the F\textbeta-score indicate that the model not only performs well in terms of the chosen emphasis on precision or recall but also maintains a reasonable balance according to the specificities dictated by \textbeta, thereby ensuring model decisions align closely with the intended requirements.

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Measures the user-defined classification effectiveness. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & Balances precision and recall. \\
        \textcolor{Green}{$+$} & Adjustable for precision or recall emphasis. \\
        \textcolor{Red}{$-$}   & Harder to interpret than individual components. \\
        \textcolor{Red}{$-$}   & Not sensitive to data distribution changes.
    \end{tabularx}
\end{table}

\begin{equation}
    F_\beta = (1 + \beta^2) \cdot \dfrac{\textit{Precision} \cdot \textit{Recall}}{(\beta^2 \cdot \textit{Precision}) + \textit{Recall}} = \dfrac{(1 + \beta^2) \cdot \textit{TP}}{(1 + \beta^2) \cdot \textit{TP} + \beta^2 \cdot \textit{FN} + \textit{FP}}
%
    \label{equation:F-score}
\end{equation}
\myequations{F-score / F\textbeta-score}


\subsubsection[Macro average F-score]{Macro average F-score \cite{mohammad2013nrc, takahashi2022confidence}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Averaged F-score over all classes. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & Values all classes equally. \\
        \textcolor{Red}{$-$}   & Valuing all classes equally can be detrimental in case of class imbalance.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{Macro average F-score} = \dfrac{1}{\textit{n}} \cdot \sum\nolimits_{i = 1}^n \textit{F}_i
%
    \label{equation:MAAF}
\end{equation}
%
\begin{conditions}
    F_i & F-score of class $i$ with a chosen $\beta$
\end{conditions}
\myequations{Macro average F-score}


\subsubsection[Micro average F-score]{Micro average F-score \cite{goutte2005probabilistic, takahashi2022confidence}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Averaged F-score based on the micro average of precision and recall. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & Useful when the performance of larger classes is of importance. \\
        \textcolor{Red}{$-$}   & Prone to issues with class imbalance.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{Micro average F-score} = 2 \cdot \dfrac{\textit{AP}_\textit{micro} \cdot \textit{AR}_\textit{micro}}{\textit{AP}_\textit{micro} + \textit{AR}_\textit{micro}}
%
    \label{equation:MIAF}
\end{equation}
\myequations{Micro average F-score}


\subsubsection[Weighted average F-score]{Weighted average F-score \cite{al2016lili, alswaidan2020hybrid}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Weighted averaged F-score over all classes. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & More robust against class imbalance. \\
        \textcolor{Red}{$-$}   & Not widely used within the literature.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{Weighted average F-score} = \dfrac{1}{\sum\nolimits_{i = 1}^n w_i} \cdot \sum\nolimits_{i = 1}^n (w_i \cdot F_i)
%
    \label{equation:WAF}
\end{equation}
%
\begin{conditions}
    w_i & samples per class or the relation of samples per class to the total of all samples \\
    F_i & F-score of class $i$
\end{conditions}
\myequations{Weighted average F-score}


\subsubsection[F0-score]{F0-score \cite{van2004geometry, taha2015metrics}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{The recall is weighted 0 times as important as the precision. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & The importance of the recall is maximized. \\
        \textcolor{Red}{$-$}   & The importance of the precision is minimized.
    \end{tabularx}
\end{table}

\begin{equation}
    F_0 = (1 + 0^2) \cdot \dfrac{\textit{Precision} \cdot \textit{Recall}}{(0^2 \cdot \textit{Precision}) + \textit{Recall}} = \dfrac{\textit{Precision} \cdot \textit{Recall}}{\textit{Recall}}
%
    \label{equation:F0-score}
\end{equation}
\myequations{F0-score}


\subsubsection[F0.5-score]{F0.5-score \cite{van2004geometry, taha2015metrics}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{The recall is weighted 0.5 times as important as the precision. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & The importance of the recall is increased. \\
        \textcolor{Red}{$-$}   & The importance of the precision is decreased.
    \end{tabularx}
\end{table}

\begin{equation}
    F_{0.5} = (1 + 0.5^2) \cdot \dfrac{\textit{Precision} \cdot \textit{Recall}}{(0.5^2 \cdot \textit{Precision}) + \textit{Recall}} = 1.25 \cdot \dfrac{\textit{Precision} \cdot \textit{Recall}}{(0.25 \cdot \textit{Precision}) + \textit{Recall}}
%
    \label{equation:F0.5-score}
\end{equation}
\myequations{F0.5-score}


\subsubsection[F1-score]{F1-score \cite{van2004geometry, taha2015metrics}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Measures the harmonic mean of the precision and the recall. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & Equal importance of precision and recall. \\
        \textcolor{Red}{$-$}   & Does not consider true negatives.
    \end{tabularx}
\end{table}

\begin{equation}
    F_1 = (1 + 1^2) \cdot \dfrac{\textit{Precision} \cdot \textit{Recall}}{(1^2 \cdot \textit{Precision}) + \textit{Recall}} = 2 \cdot \dfrac{\textit{Precision} \cdot \textit{Recall}}{\textit{Precision} + \textit{Recall}}
%
    \label{equation:F1-score}
\end{equation}
\myequations{F1-score}


\subsubsection[F2-score]{F2-score \cite{van2004geometry, taha2015metrics}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{The recall is weighted 2 times as important as the precision. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & The importance of the recall is decreased. \\
        \textcolor{Red}{$-$}   & The importance of the precision is increased.
    \end{tabularx}
\end{table}

\begin{equation}
    F_2 = (1 + 2^2) \cdot \dfrac{\textit{Precision} \cdot \textit{Recall}}{(2^2 \cdot \textit{Precision}) + \textit{Recall}} = 5 \cdot \dfrac{\textit{Precision} \cdot \textit{Recall}}{(4 \cdot \textit{Precision}) + \textit{Recall}}
%
    \label{equation:F2-score}
\end{equation}
\myequations{F2-score}


\subsection[False discovery rate (FDR)]{False discovery rate (FDR) \cite{benjamini1995controlling, benjamini2001control}}

The false discovery rate (FDR) \cite{benjamini1995controlling, benjamini2001control} is an evaluation metric used predominantly in multiple hypothesis testing to measure the proportion of false positives among the rejected hypotheses. This is particularly crucial in fields such as genomics, where large numbers of simultaneous tests are conducted and distinguishing between truly significant results and those due to chance. FDR quantifies the expected proportion of erroneous rejections (false discoveries) in a set of claimed findings. This focus on controlling the rate of false positives means that FDR is particularly useful when the cost of a false positive is significant. A lower FDR value indicates a more reliable rejection of null hypotheses, implying that a higher proportion of identified significant results are likely to be truly significant. FDR provides a balance between discovery and error, which allows to maximize true discoveries while maintaining a controlled rate of false positives. This balance is especially important in exploratory research where the ability to detect genuine effects without being overwhelmed by spurious findings is crucial.

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Fraction of false positives and true positives. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & Focuses on proportion of false positives. \\
        \textcolor{Green}{$+$} & Useful in multiple comparison scenarios. \\
        \textcolor{Red}{$-$}   & Ignores true negatives and false negatives. \\
        \textcolor{Red}{$-$}   & Sensitive to class imbalance.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{FDR} = \dfrac{\textit{FP}}{\textit{FP} + \textit{TP}}
%
    \label{equation:FDR}
\end{equation}
\myequations{False discovery rate (FDR)}


\subsubsection{Macro average FDR (FDRmacro)}

\begin{equation}
    \textit{FDR}_\textit{macro} = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \textit{FDR}_i = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \dfrac{\textit{FP}_i}{\textit{FP}_i + \textit{TP}_i}
%
    \label{equation:MAAFDR}
\end{equation}
\myequations{Macro average FDR (FDR\textsubscript{macro})}


\subsubsection{Micro average FDR (FDRmicro)}

\begin{equation}
    \textit{FDR}_\textit{micro} = \dfrac{\sum\nolimits_{i = 1}^n \textit{FP}_i}{\sum\nolimits_{i = 1}^n (\textit{FP}_i + \textit{TP}_i)}
%
    \label{equation:MIAFDR}
\end{equation}
\myequations{Micro average FDR (FDR\textsubscript{micro})}


\subsubsection{Weighted average FDR (FDRweighted)}

\begin{equation}
    \textit{FDR}_\textit{weighted} = \dfrac{1}{\sum\nolimits_{i = 1}^n w_i} \cdot \sum\nolimits_{i = 1}^n w_i \cdot \textit{FDR}_i = \dfrac{1}{\sum\nolimits_{i = 1}^n w_i} \cdot \sum\nolimits_{i = 1}^n w_i \cdot \dfrac{\textit{FP}_i}{\textit{FP}_i + \textit{TP}_i}
%
    \label{equation:WAFDR}
\end{equation}
\myequations{Weighted average FDR (FDR\textsubscript{weighted})}


\subsection[False omission rate (FOR)]{False omission rate (FOR) \cite{zafar2017fairness}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Fraction of false negatives and true negatives. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & Emphasizes the accuracy of classifying false negatives in relation to true negatives. \\
        \textcolor{Red}{$-$}   & Neglects true positives and false positives.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{FOR} = \dfrac{\textit{FN}}{\textit{FN} + \textit{TN}}
%
    \label{equation:FOR}
\end{equation}
\myequations{False omission rate (FOR)}


\subsection[False positive rate (FPR)]{False positive rate (FPR) \cite{banerjee2009hypothesis}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Fraction of false positives and true negatives. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & Emphasizes the accuracy of classifying false positives in relation to true negatives. \\
        \textcolor{Red}{$-$}   & Neglects true positives and false negatives.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{FPR} = \dfrac{\textit{FP}}{\textit{N}} = \dfrac{\textit{FP}}{\textit{FP} + \textit{TN}}
%
    \label{equation:FPR}
\end{equation}
\myequations{False positive rate (FPR)}


\subsection[False negative rate (FNR)~/ miss rate]{False negative rate (FNR)~/ miss rate \cite{banerjee2009hypothesis}}

The false negative rate (FNR) \cite{banerjee2009hypothesis}, which is an important metric in the field of statistical classification, measures the proportion of positive instances that a model incorrectly classifies as negative. This rate is critical in scenarios where the consequences of missing a positive detection are severe, such as in medical diagnosis or security surveillance systems \cite{sreenu2019intelligent, xue2020machine}. FNR is particularly valuable in contexts where ensuring the capture of all positive cases is paramount to prevent harmful outcomes. By quantifying the likelihood that true positives are overlooked by a model, the FNR provides essential insights into the sensitivity of a classification system. A high FNR indicates that a significant number of positive cases are being missed, potentially leading to hazardous oversights. Consequently, minimizing FNR is crucial in high-stakes environments where the cost of a false negative is high, such as failing to diagnose a serious illness or missing a security threat. It helps in tuning the thresholds of detection algorithms to better suit specific operational requirements, ensuring a more reliable performance in critical applications. Therefore, the FNR is integral to developing robust predictive models that effectively manage the risks associated with incorrect negative classifications.

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Fraction of false negatives and true positives. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & Emphasizes identifying missed positive cases. \\
        \textcolor{Green}{$+$} & Important for recall-focused assessments. \\
        \textcolor{Red}{$-$}   & Ignores true negatives and false positives. \\
        \textcolor{Red}{$-$}   & May lead to overemphasis on recall.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{FNR} = \dfrac{\textit{FN}}{\textit{P}} = \dfrac{\textit{FN}}{\textit{FN} + \textit{TP}}
%
    \label{equation:FNR}
\end{equation}
\myequations{False negative rate (FNR) / miss rate}


\subsubsection{Macro average FNR (FNRmacro)}

\begin{equation}
    \textit{FNR}_\textit{macro} = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \textit{FNR}_i = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \dfrac{\textit{FN}_i}{\textit{FN}_i + \textit{TP}_i}
%
    \label{equation:MAAFNR}
\end{equation}
\myequations{Macro average FNR (FNR\textsubscript{macro})}


\subsubsection{Micro average FNR (FNRmicro)}

\begin{equation}
    \textit{FNR}_\textit{micro} = \dfrac{\sum\nolimits_{i = 1}^n \textit{FN}_i}{\sum\nolimits_{i = 1}^n (\textit{FN}_i + \textit{TP}_i)}
%
    \label{equation:MIAFNR}
\end{equation}
\myequations{Micro average FNR (FNR\textsubscript{micro})}


\subsubsection{Weighted average FNR (FNRweighted)}
\begin{equation}
    \textit{FNR}_\textit{weighted} = \dfrac{1}{\sum\nolimits_{i = 1}^n w_i} \cdot \sum\nolimits_{i = 1}^n w_i \cdot \textit{FNR}_i = \dfrac{1}{\sum\nolimits_{i = 1}^n w_i} \cdot \sum\nolimits_{i = 1}^n w_i \cdot \dfrac{\textit{FN}_i}{\textit{FN}_i + \textit{TP}_i}
%
    \label{equation:WAFNR}
\end{equation}
\myequations{Weighted average FNR (FNR\textsubscript{weighted})}


\subsection[Positive likelihood ratio (LR+)]{Positive likelihood ratio (LR$+$) \cite{swets1973relative, deeks2004diagnostic}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{In medicine, LR$+$ describes the probability of, e.g., a positive result in a sick person (true positive) in relation to the probability of a positive result in a healthy person (false positive). (range: $[0, \infty)$)} \\
        \textcolor{Green}{$+$} & Emphasizes the accuracy of the TPR in relation to the FPR. \\
        \textcolor{Red}{$-$}   & Neglects, e.g., TNR and FNR (see also diagnostic odds ratio).
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{LR}+ = \dfrac{\textit{Sensitivity}}{1 - \textit{Specificity}} = \dfrac{\textit{TPR}}{\textit{FPR}}
%
    \label{equation:LR+}
\end{equation}
\myequations{Positive likelihood ratio (LR$+$)}


\subsection[Negative likelihood ratio (LR-)]{Negative likelihood ratio (LR$-$) \cite{swets1973relative, deeks2004diagnostic}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{In medicine, LR$-$ describes the probability of, e.g., a negative result in a sick person (false negative) in relation to the probability of a negative result in a healthy person (true negative). (range: $[0, \infty)$)} \\
        \textcolor{Green}{$+$} & Emphasizes the accuracy of the FNR in relation to the TNR. \\
        \textcolor{Red}{$-$}   & Neglects, e.g., TPR and FPR (see also diagnostic odds ratio).
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{LR}- = \dfrac{1 - \textit{Sensitivity}}{\textit{Specificity}} = \dfrac{\textit{FNR}}{\textit{TNR}}
%
    \label{equation:LR-}
\end{equation}
\myequations{Negative likelihood ratio (LR$-$)}


\subsection[Diagnostic odds ratio (DOR)]{Diagnostic odds ratio (DOR) \cite{glas2003diagnostic, doust2004systematic}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{In medicine, DOR measures the classification effectiveness of a diagnostic test. \cite{glas2003diagnostic} (range: $[0, \infty)$)} \\
        \textcolor{Green}{$+$} & Balances sensitivity and specificity. \\
        \textcolor{Green}{$+$} & Independent of disease prevalence. \\
        \textcolor{Red}{$-$}   & Requires true positive, true negative, false positive, false negative counts. \\
        \textcolor{Red}{$-$}   & Increased calculation and interpretation complexity.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{DOR} = \dfrac{\textit{LR}+}{\textit{LR}-} = \dfrac{\textit{TP} \cdot \textit{TN}}{\textit{FP} \cdot \textit{FN}}
%
    \label{equation:DOR}
\end{equation}
\myequations{Diagnostic odds ratio (DOR)}


\subsection[Fowlkes--Mallows index (FM)]{Fowlkes--Mallows index (FM) \cite{fowlkes1983method, halkidi2001clustering}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Measures the geometric mean of the precision and the recall. In clustering, FM measures the similarity of two clusters. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & Increased robustness to noise. \\
        \textcolor{Red}{$-$}   & Less well known and used.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{FM} = \sqrt{\textit{PPV} \cdot \textit{TPR}} = \sqrt{\textit{Precision} \cdot \textit{Recall}}
%
    \label{equation:FM}
\end{equation}
\myequations{Fowlkes--Mallows index (FM)}


\subsection[Informedness~/ bookmaker informedness (BM)~/ Youden's J statistic~/ Youden's index]{Informedness~/ bookmaker informedness (BM)~/ Youden's J statistic~/ Youden's index \cite{peirce1884numerical, youden1950index}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Quantifies the probability of an ``informed decision''. (range: $[-1, 1]$)} \\
        \textcolor{Green}{$+$} & Takes all predictions into account. \\
        \textcolor{Red}{$-$}   & Its result ranges from $-1$ to $1$.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{Informedness} = \textit{Sensitivity} + \textit{Specificity} - 1 = \textit{TPR} + \textit{TNR} - 1
%
    \label{equation:informedness}
\end{equation}
\myequations{Informedness / bookmaker informedness (BM) / Youden's J statistic / Youden's index}


\subsection[Markedness (MK)]{Markedness (MK) \cite{powers2020evaluation}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Quantifies the ``markedness'', i.e., the state of being irregular or uncommon. (range: $[-1, 1]$)} \\
        \textcolor{Green}{$+$} & Takes all predictions into account. \\
        \textcolor{Red}{$-$}   & Its result ranges from $-1$ to $1$.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{MK} = \textit{PPV} + \textit{NPV} - 1
%
    \label{equation:MK}
\end{equation}
\myequations{Markedness (MK)}


\subsection[Matthews correlation coefficient (MCC)~/ phi coefficient~/ Yule phi coefficient]{Matthews correlation coefficient (MCC)~/ phi coefficient~/ Yule phi coefficient \cite{yule1912methods, matthews1975comparison, cramer1999mathematical, chicco2020advantages}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{A balanced measure of TPR, TNR, PPV, and NPV. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & Takes all predictions into account. \\
        \textcolor{Red}{$-$}   & Increased calculation and interpretation complexity.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{MCC} = \sqrt{\textit{TPR} \cdot \textit{TNR} \cdot \textit{PPV} \cdot \textit{NPV}}
%
    \label{equation:MCC}
\end{equation}
\myequations{Matthews correlation coefficient (MCC) / phi coefficient / Yule phi coefficient}


\subsection[Jaccard index (JI)~/ threat score (TS)~/ critical success index (CSI)]{Jaccard index (JI)~/ threat score (TS)~/ critical success index (CSI) \cite{jaccard1912distribution, murphy1996finley}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Fraction of TP with TP, FN, and FP. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & Emphasizes the accuracy of classifying TP in relation to FN and FP. \\
        \textcolor{Red}{$-$}   & Neglects TN.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{JI} = \dfrac{\textit{TP}}{\textit{TP} + \textit{FN} + \textit{FP}}
%
    \label{equation:JI}
\end{equation}
\myequations{Jaccard index (JI) / threat score (TS) / critical success index (CSI)}


\subsection[Receiver operating characteristic curve (ROC curve) and area under the curve (AUC)]{Receiver operating characteristic curve (ROC curve) and area under the curve (AUC) \cite{green1966signal, zweig1993receiver, fawcett2006introduction}}

The receiver operating characteristic curve (ROC curve) and its associated metric \cite{green1966signal, zweig1993receiver, fawcett2006introduction}, the area under the curve (AUC), are pivotal evaluation tools used in the analysis of classification models \cite{bradley1997use, mandrekar2010receiver, jimenez2012insights}. The ROC curve is a graphical representation that plots the true positive rate (sensitivity) against the false positive rate (specificity) at various threshold settings, providing a comprehensive view of a model's ability across a spectrum of conditions. The AUC, a scalar value derived from the area under the ROC curve, quantifies the overall ability of the model to discriminate between classes irrespective of any specific threshold. A higher AUC value indicates better model performance, with a value of 1.0 representing perfect discrimination and 0.5 denoting no discriminative ability (equivalent to random guessing). By evaluating the trade-offs between sensitivity and specificity without committing to a specific classification threshold, ROC and AUC facilitate an objective comparison of model performance when robust and reliable predictions are of central importance. However, critics also highlight that AUC ignores decision thresholds critical for real-world applications, fails to consider the actual costs of false positives and false negatives, and focuses on ranking rather than probability calibration \cite{hilden1991area, hand2010evaluating}. Additionally, AUC can obscure the practical performance of a classifier at specific operating points and may vary across data sets with different class distributions, complicating performance comparisons (see also H-measure \cite{hand2009measuring, hand2023notes}).

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Calculates the area under the so-called receiver operating characteristic curve by adjusting the confidence threshold for, e.g., classification, detection, or segmentation. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & Evaluates model performance across various decision thresholds. \\
        \textcolor{Green}{$+$} & Suitable for imbalanced data sets. \\
        \textcolor{Red}{$-$}   & Insensitive to class distribution changes. \\
        \textcolor{Red}{$-$}   & Does not directly address practical performance thresholds.
    \end{tabularx}
\end{table}

\begin{figure}[H]
    \centering

    \begin{tikzpicture}
        \colorlet{draw_color}{LimeGreen}
        \colorlet{fill_color}{LimeGreen!50}

        \begin{axis}[
         width=0.5\textwidth, height=0.5\textwidth,
         xlabel={False positive rate (FPR)}, ylabel={Recall / true positive rate (TPR) / sensitivity},
         ticklabel style={/pgf/number format/.cd, fixed, fixed zerofill, precision=1},
         legend columns=-1, legend style={at={(0.5, 1.25)}, anchor=north, column sep=1ex}]
            \addplot[domain=0:1, dashed] {x};
            \addplot[name path=f, domain=0:1, samples=512, smooth, draw_color] {1 - (x - 1)^2};
            \path[name path=axis] (0, 0) -- (1, 0);
            \addplot[fill_color] fill between[of=f and axis];
            \legend{$x$, Exemplary ROC curve, AUC}
        \end{axis}
    \end{tikzpicture}

    \caption{Receiver operating characteristic curve (ROC curve) and area under the curve (AUC).}
    \label{figure:ROC_AUC}
\end{figure}

\begin{equation}
    \textit{AUC} = \int_{x = 0}^1 \textit{TPR}(\textit{FPR}^{\,-1}(x)) \,dx = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \textit{TPR}(\textit{FPR}_i)
%
    \label{equation:AUC}
\end{equation}
\myequations{Area under the curve (AUC)}


\subsection[Average precision (AP)]{Average precision (AP) \cite{manning2009introduction, everingham2010pascal}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Calculates the area under the precision-recall curve by adjusting the confidence threshold for, e.g., classification, detection, or segmentation \cite{ozenne2015precision, sofaer2019area, cook2020consult}. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & Evaluates model performance across various decision thresholds. \\
        \textcolor{Green}{$+$} & Suitable for imbalanced data sets. \\
        \textcolor{Red}{$-$}   & Insensitive to class distribution changes. \\
        \textcolor{Red}{$-$}   & Does not directly address practical performance thresholds.
    \end{tabularx}
\end{table}

\begin{figure}[H]
    \centering

    \begin{tikzpicture}
        \colorlet{draw_color}{LimeGreen}
        \colorlet{fill_color}{LimeGreen!50}

        \begin{axis}[
         width=0.5\textwidth, height=0.5\textwidth,
         xlabel={Recall / true positive rate (TPR) / sensitivity}, ylabel={Precision / positive predictive value (PPV)},
         ticklabel style={/pgf/number format/.cd, fixed, fixed zerofill, precision=1},
         legend columns=-1, legend style={at={(0.5, 1.25)}, anchor=north, column sep=1ex}]
            \addplot[domain=0:1, dashed] {-x + 1};
            \addplot[name path=f, domain=0:1, samples=512, smooth, draw_color] {1 - x^2};
            \path[name path=axis] (0, 0) -- (1, 0);
            \addplot[fill_color] fill between[of=f and axis];
            \legend{$-x + 1$, Exemplary precision-recall curve, AUC}
        \end{axis}
    \end{tikzpicture}

    \caption{Precision-recall curve and area under the curve (AUC).}
    \label{figure:PRC_AUC}
\end{figure}

\begin{equation}
    \textit{AP} = \int_{x = 0}^1 \textit{Precision}(\textit{Recall}^{\,-1}(x)) \,dx = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \textit{Precision}(\textit{Recall}_i)
%
    \label{equation:AP}
\end{equation}
\myequations{Average precision (AP)}


\subsection[Mean average precision (mAP)]{Mean average precision (mAP) \cite{manning2009introduction, everingham2010pascal}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Averages the area under the precision-recall curve over multiple classes. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & Allows for an even more meaningful evaluation over multiple classes. \\
        \textcolor{Red}{$-$}   & Increased calculation complexity.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{mAP} = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \textit{AP}_i
%
    \label{equation:mAP}
\end{equation}
\myequations{Mean average precision (mAP)}


\subsection[H-measure (H)]{H-measure (H) \cite{hand2009measuring, hand2023notes}}

The H-measure \cite{hand2009measuring, hand2023notes} is designed to address the limitations in traditional metrics such as the receiver operating characteristic curve (ROC curve) and the area under the curve (AUC) \cite{hilden1991area, hand2010evaluating}. Unlike AUC, which is insensitive to class imbalance and disregards the actual costs associated with misclassifications, the H-measure integrates these costs into the evaluation process, providing a more comprehensive assessment. It incorporates a cost function that reflects the relative importance of false positives and false negatives, evaluating model performance over a range of decision thresholds. By weighting these thresholds according to a specified distribution, $\pi(\theta)$, the H-measure accounts for varying conditions, offering a nuanced analysis that aligns more closely with practical requirements. This makes the H-measure particularly valuable in domains where misclassification costs are unequal and need careful consideration, such as medical diagnostics or fraud detection. However, despite its advantages, the H-measure's complexity and sensitivity to the chosen cost function and parameters can pose challenges. Yet, its ability to provide a cost-sensitive and balanced evaluation of performance makes it a powerful tool for informed decision-making.

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Integrates misclassification costs over a range of thresholds, providing a cost-sensitive assessment of model performance on a normalized scale. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & Integrates costs of false positives and false negatives (cost sensitivity). \\
        \textcolor{Green}{$+$} & Accounts for class prevalence (class imbalance handling). \\
        \textcolor{Red}{$-$}   & More difficult to understand and implement (complexity). \\
        \textcolor{Red}{$-$}   & Results vary with cost function choice (parameter sensitivity).
    \end{tabularx}
\end{table}

\begin{equation}
    \begin{aligned}
        \textit{EC}(\theta) &= C(\theta)_\textit{FP} \cdot P(\textit{FP}\, | \theta) + C(\theta)_\textit{FN} \cdot P(\textit{FN}\, | \theta) \\
        H                   &= \int_{\theta = 0}^1 \pi(\theta) \cdot \textit{EC}(\theta) \,d\theta
%
        \label{equation:H-measure}
    \end{aligned}
\end{equation}
%
\begin{conditions}
    C(\theta)_\textit{FP}, C(\theta)_\textit{FN} & costs of FP and FN at threshold $\theta$ \\
    \textit{EC}(\theta)                          & expected cost at each threshold \\
    \pi(\theta)                                  & weight distribution, reflecting the importance of different thresholds
\end{conditions}
\myequations{H-measure}


\subsection[Cohen's kappa for binary classification]{Cohen's kappa for binary classification \cite{cohen1960coefficient, ranganathan2017common, chicco2021matthews}}

Cohen's kappa \cite{cohen1960coefficient, ranganathan2017common, chicco2021matthews} is a common evaluation metric for the assessment of inter-rater reliability. It is applicable to both binary and categorical data by providing a quantitative measure. Cohen's kappa is especially useful for the training of raters and adjustments in methodologies, particularly in complex settings with multiple raters. However, the main argument against Cohen's kappa centers on the problematic interpretations it can foster \cite{pontius2011death, olofsson2014good, foody2020explaining}. Critics highlight that Cohen's kappa can be misleading due to its dependence on the prevalence of the attribute being measured and the possible bias of the raters. For instance, even when multiple raters are in substantial agreement, kappa scores can be unexpectedly low if the prevalence of a certain categorization is particularly high or low. This has led to concerns that Cohen's kappa may not always provide a robust or accurate measure of agreement. Hence, it could be misleading when forming a decisions based on its result.

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Ranges from $-1$ for a fully wrong prediction to $+1$ for a completely correct prediction. (range: $[-1, 1]$)} \\
        \textcolor{Green}{$+$} & Accounts for chance agreement. \\
        \textcolor{Green}{$+$} & Useful for inter-rater agreement assessment. \\
        \textcolor{Red}{$-$}   & Sensitivity to class imbalance. \\
        \textcolor{Red}{$-$}   & Interpretation complexity with multiple raters.
    \end{tabularx}
\end{table}

\begin{equation}
    \begin{aligned}
        \kappa &= \dfrac{P_o - P_e}{1 - P_e} \\
        P_e    &= \left(\dfrac{\textit{TP} + \textit{FP}}{T}\right) \cdot \left(\dfrac{\textit{TP} + \textit{FN}}{T}\right) + \left(\dfrac{\textit{TN} + \textit{FN}}{T}\right) \cdot \left(\dfrac{\textit{TN} + \textit{FP}}{T}\right)
%
        \label{equation:k}
    \end{aligned}
\end{equation}
%
\begin{conditions}
    \kappa & Cohen's kappa \\
    P_e    & expected accuracy \\
    P_o    & observed accuracy
\end{conditions}
\myequations{Cohen's kappa for binary classification}


\subsection[Gini impurity]{Gini impurity \cite{gini1912variabilita, breiman1984classification, manek2017aspect}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{The Gini impurity indicates the probability that two randomly selected samples from a data set have different original label types. A lower value indicates a higher purity of the data set. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & Well-suited for application in decision tree algorithms. \\
        \textcolor{Red}{$-$}   & Favors binary decisions, which can result in decision trees of reduced complexity.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{Gini impurity}(D) = \sum\nolimits_{i = 1}^n \sum\nolimits_{i' \not= i} p_i p_{i'} = 1 - \sum\nolimits_{i = 1}^n p^2_i
%
    \label{equation:Gini_impurity}
\end{equation}
%
\begin{conditions}
    D & data set \\
    p & probability of samples
\end{conditions}
\myequations{Gini impurity}


\subsection[P4 metric]{P\textsubscript{4} metric \cite{sitarz2023extending}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{The P\textsubscript{4} metric covers four probabilities, precision, recall, specificity, and NPV, at once, forming their harmonic mean. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & Tends to zero when any of the four conditional probabilities tends to zero. Tends to one when all four conditional probabilities tend to one. \\
        \textcolor{Green}{$+$} & It is symmetrical with respect to data set labels swapping. \\
        \textcolor{Red}{$-$}   & Does not take weighting. \\
        \textcolor{Red}{$-$}   & Currently barely used metric.
    \end{tabularx}
\end{table}

\begin{equation}
    P_4 = \dfrac{4}{\cfrac{1}{\textit{Precision}} + \cfrac{1}{\textit{Recall}} + \cfrac{1}{\textit{Specificity}} + \cfrac{1}{\textit{NPV}}} = \dfrac{4 \cdot \textit{TP} \cdot \textit{TN}}{4 \cdot \textit{TP} \cdot \textit{TN} + (\textit{TP} + \textit{TN}) \cdot (\textit{FP} + \textit{FN})}
%
    \label{equation:P4_metric}
\end{equation}
\myequations{P\textsubscript{4} metric}


\subsection[Skill score (SS)]{Skill score (SS) \cite{murphy1988skill}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Measures the quality of a prediction against a reference. (range: $(-\infty, 1]$)} \\
        \textcolor{Green}{$+$} & Intuitive way to rank model performance. \\
        \textcolor{Red}{$-$}   & Scales indefinitely into the negative for larger variations.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{SS} = 1 - \dfrac{\textit{Metric}_\textit{GT}}{\textit{Metric}_P}
%
    \label{equation:SS}
\end{equation}
%
\begin{conditions}
    \textit{Metric}_\textit{GT} & best possible expectation for a model based on a given metric \\
    \textit{Metric}_P           & actual prediction of a model based on a given metric
\end{conditions}
\myequations{Skill score (SS)}


\subsection[Relative improvement factor]{Relative improvement factor \cite{schlosser2022improving}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Measures the relative quality of a prediction against a reference. (range: $[0, \infty)$)} \\
        \textcolor{Green}{$+$} & Intuitive way to rank model performance. \\
        \textcolor{Red}{$-$}   & Scales indefinitely into the positive for larger variations.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{Relative improvement factor} = \dfrac{1 - \textit{Metric}_\textit{GT}}{1 - \textit{Metric}_P}
%
    \label{equation:relative_improvement_factor}
\end{equation}
%
\begin{conditions}
    \textit{Metric}_\textit{GT} & best possible expectation for a model based on a given metric \\
    \textit{Metric}_P           & actual prediction of a model based on a given metric
\end{conditions}
\myequations{Relative improvement factor}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\clearpage




\subsection{Examples}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Figure~\ref{figure:ML_example1} shows an example result for a classification problem with 3 imbalanced classes, while Fig.~\ref{figure:ML_example2} illustrates an example result for a classification problem with 3 balanced classes created from the Iris flower data set of \textit{Fisher} \cite{fisher1936use}. Both examples show an exemplary confusion matrix on the top left together with the number of samples per class. The metrics recall and false negative rate (FNR) with their equations and example calculations are aligned to the rows, whereas precision and false discovery rate (FDR) and their respective equations and calculations are aligned with the columns of the confusion matrix. Accuracy as well as micro, macro, and weighted precision, recall, and F1-scores are shown on the bottom right. Note that micro and macro average recall, precision, and F1-score remain constant for the balanced data set. This is not the case for the imbalanced data set. Since the examples discuss 3-class problems, we set $n = 3$ in the equations for, e.g., macro average precision, recall, and F1-score (Eq.~\ref{equation:MAAP}, \ref{equation:MAAR}, and \ref{equation:MAAF}, respectively).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{figure}[H]
    \centering

    \resizebox{\textwidth}{!}{\begin{tikzpicture}[r/.style={draw, rectangle, minimum size=1cm}]
        \begin{scope}
            \node[r, fill=RoyalBlue, text=white] at (-1,  1) {41};
            \node[r, fill=Apricot!50]            at ( 0,  1) {11};
            \node[r, fill=Apricot!50]            at ( 1,  1) {12};
            \node[r, fill=Apricot!50]            at (-1,  0) {62};
            \node[r, fill=RoyalBlue, text=white] at ( 0,  0) {22};
            \node[r, fill=Apricot!50]            at ( 1,  0) {23};
            \node[r, fill=Apricot!50]            at (-1, -1) {31};
            \node[r]                             at ( 0, -1) {0};
            \node[r, fill=RoyalBlue, text=white] at ( 1, -1) {52};

            \node            at ( 0, 2) {Predicted class};
            \node[rotate=90] at (-2, 0) {Ground truth class};

            \node[rotate=60] at (-1, 3) {1: \textit{cat}};
            \node[rotate=60] at ( 0, 3) {2: \textit{fox}};
            \node[rotate=60] at ( 1, 3) {3: \textit{dog}};

            \node at (-3,  1) {1: \textit{cat}};
            \node at (-3,  0) {2: \textit{fox}};
            \node at (-3, -1) {3: \textit{dog}};
        \end{scope}

        \begin{scope}[xshift=3.5cm, inner sep=0, font=\footnotesize]
            \node[r] at (-1,    1) {64};
            \node[r] at ( 0.5,  1) {64.1\%};
            \node[r] at ( 1.6,  1) {35.9\%};
            \node[r] at (-1,    0) {107};
            \node[r] at ( 0.5,  0) {20.6\%};
            \node[r] at ( 1.6,  0) {79.4\%};
            \node[r] at (-1,   -1) {83};
            \node[r] at ( 0.5, -1) {62.7\%};
            \node[r] at ( 1.6, -1) {37.4\%};

            \node at (-1,   2) {$\sum$};
            \node at ( 0.5, 2) {$\textit{Recall}_i$};
            \node at ( 1.6, 2) {$\textit{FNR}_i$};

            \node at (6, 0.5) {
                \renewcommand*{\arraystretch}{3}\begin{tabular}{cccc}
                    {Eq.~\{\ref{equation:recall}\}} & $\dfrac{\textit{TP}}{\textit{TP} + \textit{FN}}$ & {Eq.~\{\ref{equation:FNR}\}} & $\dfrac{\textit{FN}}{\textit{FN} + \textit{TP}}$ \\
                    \hline
                    $\textit{Recall}_1$ & $\dfrac{41}{41 + 11 + 12}$ & $\textit{FNR}_1$ & $\dfrac{11 + 12}{11 + 12 + 41}$ \\
                    \hline
                    $\textit{Recall}_2$ & $\dfrac{22}{22 + 62 + 23}$ & $\textit{FNR}_2$ & $\dfrac{62 + 23}{62 + 23 + 22}$ \\
                    \hline
                    $\textit{Recall}_3$ & $\dfrac{52}{52 + 31 + 0}$ & $\textit{FNR}_3$ & $\dfrac{31 + 0}{31 + 0 + 52}$ \\
                    \hline
                \end{tabular}};
        \end{scope}

        \begin{scope}[yshift=-3.5cm, inner sep=0, font=\footnotesize]
            \node[r] at (-1,  1)   {134};
            \node[r] at ( 0,  1)   {33};
            \node[r] at ( 1,  1)   {87};
            \node[r] at (-1, -0.5) {30.6\%};
            \node[r] at ( 0, -0.5) {66.7\%};
            \node[r] at ( 1, -0.5) {59.8\%};
            \node[r] at (-1, -1.6) {69.4\%};
            \node[r] at ( 0, -1.6) {33.3\%};
            \node[r] at ( 1, -1.6) {40.2\%};

            \node[r] at (2.5, 1) {254};

            \node at (-2.5,  1)   {$\sum$};
            \node at (-2.5, -0.5) {$\textit{Precision}_i$};
            \node at (-2.5, -1.6) {$\textit{FDR}_i$};

            \node at (-0.5, -5) {
                \renewcommand*{\arraystretch}{3}\begin{tabular}{c|c|c|c|}
                    Eq.~\ref{equation:precision} & $\textit{Precision}_1$ & $\textit{Precision}_2$ & $\textit{Precision}_3$ \\
                    $\dfrac{\textit{TP}}{\textit{TP} + \textit{FP}}$ & $\dfrac{41}{41 + 62 + 31}$ & $\dfrac{22}{22 + 11 + 0}$ & $\dfrac{52}{52 + 12 + 23}$ \\
                    Eq.~\ref{equation:FDR} & $\textit{FDR}_1$ & $\textit{FDR}_2$ & $\textit{FDR}_3$ \\
                    $\dfrac{\textit{FP}}{\textit{FP} + \textit{TP}}$ & $\dfrac{62 + 31}{62 + 31 + 41}$ & $\dfrac{11 + 0}{11 + 0 + 22}$ & $\dfrac{12 + 23}{12 + 23 + 52}$ \\
                \end{tabular}};
        \end{scope}

        \begin{scope}[shift={(9, -5)}, inner sep=0, font=\footnotesize]
            \node at (0, 0) {$\textit{Accuracy} = \dfrac{\textit{TP} + \textit{TN}}{\textit{TP} + \textit{TN} + \textit{FP} + \textit{FN}} = \dfrac{41 + 22 + 52}{41 + 22 + 52 + 11 + 12 + 62 + 23 + 31 + 0} = 45.28\%$};

            \node at (0, -3) {
                \begin{tabular}{|P{2cm}|P{1.75cm}|P{1.75cm}|P{1.75cm}|P{0.5cm}|}
                    \hline
                    Class & \textit{Precision} [\%] & \textit{Recall} [\%] & $F_1$ [\%] & $\sum$ \\
                    \hline
                    \noalign{\vskip 2pt}

                    \hline
                    1 & 30.60 & 64.06 & 41.41 & 64 \\
                    \hline
                    2 & 66.67 & 20.56 & 31.43 & 107 \\
                    \hline
                    3 & 59.77 & 62.65 & 61.18 & 83 \\
                    \hline
                    \noalign{\vskip 2pt}

                    \cline{1-4}
                    Macro average & 52.34 & 49.09 & 44.67 & \multicolumn{1}{c}{} \\
                    \cline{1-4}
                    Micro average & 45.28 & 45.28 & 45.28 & \multicolumn{1}{c}{} \\
                    \cline{1-4}
                    W. average    & 55.32 & 45.28 & 43.67 & \multicolumn{1}{c}{} \\
                    \cline{1-4}
                    \noalign{\vskip 2pt}

                    \cline{2-4}
                    \multicolumn{1}{c}{} & \multicolumn{1}{|c|}{Eq.~\{\ref{equation:precision},\ref{equation:MAAP}--\ref{equation:WAP}\}} & \multicolumn{1}{c|}{Eq.~\{\ref{equation:recall},\ref{equation:MAAR}--\ref{equation:WAR}\}} & \multicolumn{1}{c|}{Eq.~\{\ref{equation:F1-score},\ref{equation:MAAF}--\ref{equation:WAF}\}} & \multicolumn{1}{c}{} \\
                    \cline{2-4}
                \end{tabular}};
        \end{scope}
    \end{tikzpicture}}

    \caption{Classification result example (1) showing a 3-class problem with imbalanced classes.}
    \label{figure:ML_example1}
\end{figure}


\begin{figure}[H]
    \centering

    \resizebox{\textwidth}{!}{\begin{tikzpicture}[r/.style={draw, rectangle, minimum size=1cm}]
        \begin{scope}
            \node[r, fill=RoyalBlue, text=white] at (-1,  1) {50};
            \node[r]                             at ( 0,  1) {0};
            \node[r]                             at ( 1,  1) {0};
            \node[r]                             at (-1,  0) {0};
            \node[r, fill=RoyalBlue, text=white] at ( 0,  0) {47};
            \node[r, fill=Apricot!50]            at ( 1,  0) {3};
            \node[r]                             at (-1, -1) {0};
            \node[r, fill=Apricot!50]            at ( 0, -1) {2};
            \node[r, fill=RoyalBlue, text=white] at ( 1, -1) {48};

            \node            at ( 0, 2) {Predicted class};
            \node[rotate=90] at (-2, 0) {Ground truth class};

            \node[rotate=60] at (-1, 3.5) {1: \textit{setosa}};
            \node[rotate=60] at ( 0, 3.5) {2: \textit{versicolor}};
            \node[rotate=60] at ( 1, 3.5) {3: \textit{virginica}};

            \node at (-3.5,  1) {1: \textit{setosa}};
            \node at (-3.5,  0) {2: \textit{versicolor}};
            \node at (-3.5, -1) {3: \textit{virginica}};
        \end{scope}

        \begin{scope}[xshift=3.5cm, inner sep=0, font=\footnotesize]
            \node[r] at (-1,    1) {50};
            \node[r] at ( 0.5,  1) {100\%};
            \node[r] at ( 1.6,  1) {0\%};
            \node[r] at (-1,    0) {50};
            \node[r] at ( 0.5,  0) {94\%};
            \node[r] at ( 1.6,  0) {6\%};
            \node[r] at (-1,   -1) {50};
            \node[r] at ( 0.5, -1) {96\%};
            \node[r] at ( 1.6, -1) {4\%};

            \node at (-1,   2) {$\sum$};
            \node at ( 0.5, 2) {$\textit{Recall}_i$ };
            \node at ( 1.6, 2) {$\textit{FNR}_i$};

            \node at (6, 0.5) {
                \renewcommand*{\arraystretch}{3}\begin{tabular}{cccc}
                    Eq.~\ref{equation:recall} & $\dfrac{\textit{TP}}{\textit{TP} + \textit{FN}}$ & Eq.~\ref{equation:FNR} & $\dfrac{\textit{FN}}{\textit{FN} + \textit{TP}}$ \\
                    \hline
                    $\textit{Recall}_1$ & $\dfrac{50}{50 + 0 + 0}$ & $\textit{FNR}_1$ & $\dfrac{0 + 0}{0 + 0 + 50}$ \\
                    \hline
                    $\textit{Recall}_2$ & $\dfrac{47}{47 + 0 + 3}$ & $\textit{FNR}_2$ & $\dfrac{0 + 3}{0 + 3 + 47}$ \\
                    \hline
                    $\textit{Recall}_3$ & $\dfrac{48}{48 + 0 + 2}$ & $\textit{FNR}_3$ & $\dfrac{0 + 2}{0 + 2 + 48}$ \\
                    \hline
                \end{tabular}};
        \end{scope}

        \begin{scope}[yshift=-3.5cm, inner sep=0, font=\footnotesize]
            \node[r] at (-1,  1)   {50};
            \node[r] at ( 0,  1)   {49};
            \node[r] at ( 1,  1)   {51};
            \node[r] at (-1, -0.5) {100\%};
            \node[r] at ( 0, -0.5) {95.9\%};
            \node[r] at ( 1, -0.5) {94.1\%};
            \node[r] at (-1, -1.6) {0\%};
            \node[r] at ( 0, -1.6) {4.1\%};
            \node[r] at ( 1, -1.6) {5.9\%};

            \node[r] at (2.5, 1) {150};

            \node at (-2.5,  1)   {$\sum$};
            \node at (-2.5, -0.5) {$\textit{Precision}_i$};
            \node at (-2.5, -1.6) {$\textit{FDR}_i$};

            \node at (-0.5, -5) {
                \renewcommand*{\arraystretch}{3}\begin{tabular}{c|c|c|c|}
                    Eq.~\ref{equation:precision} & $\textit{Precision}_1$ & $\textit{Precision}_2$ & $\textit{Precision}_3$ \\
                    $\dfrac{\textit{TP}}{\textit{TP} + \textit{FP}}$ & $\dfrac{50}{50 + 0 + 0}$ & $\dfrac{47}{47 + 0 + 2}$ & $\dfrac{48}{48 + 0 + 3}$ \\
                    Eq.~\ref{equation:FDR} & $\textit{FDR}_1$ & $\textit{FDR}_2$ & $\textit{FDR}_3$ \\
                    $\dfrac{\textit{FP}}{\textit{FP} + \textit{TP}}$ & $\dfrac{0 + 0}{0 + 0 + 50}$ & $\dfrac{0 + 2}{0 + 2 + 47}$ & $\dfrac{0 + 3}{0 + 3 + 48}$ \\
                \end{tabular}};
        \end{scope}

        \begin{scope}[shift={(9, -5)}, inner sep=0, font=\footnotesize]
            \node at (0, 0) {$\textit{Accuracy} = \dfrac{\textit{TP} + \textit{TN}}{\textit{TP} + \textit{TN} + \textit{FP} + \textit{FN}} = \dfrac{50 + 47 + 48}{50 + 47 + 48 + 0 + 0 + 0 + 3 + 0 + 2} = 96.7\%$};

            \node at (0, -3) {
                \begin{tabular}{|P{2cm}|P{1.75cm}|P{1.75cm}|P{1.75cm}|P{0.5cm}|}
                    \hline
                    Class & \textit{Precision} [\%] & \textit{Recall} [\%] & $F_1$ [\%] & $\sum$ \\
                    \hline
                    \noalign{\vskip 2pt}

                    \hline
                    1 & 100.00 & 100.00 & 100.00 & 50 \\
                    \hline
                    2 &  95.92 &  94.00 &  94.95 & 50 \\
                    \hline
                    3 &  94.12 &  96.00 &  95.05 & 50 \\
                    \hline
                    \noalign{\vskip 2pt}

                    \cline{1-4}
                    Macro average & 96.68 & 96.67 & 96.67 & \multicolumn{1}{c}{} \\
                    \cline{1-4}
                    Micro average & 96.67 & 96.67 & 96.67 & \multicolumn{1}{c}{} \\
                    \cline{1-4}
                    W. average    & 96.68 & 96.67 & 96.67 & \multicolumn{1}{c}{} \\
                    \cline{1-4}
                    \noalign{\vskip 2pt}

                    \cline{2-4}
                    \multicolumn{1}{c}{} & \multicolumn{1}{|c|}{Eq.~\{\ref{equation:precision},\ref{equation:MAAP}--\ref{equation:WAP}\}} & \multicolumn{1}{c|}{Eq.~\{\ref{equation:recall},\ref{equation:MAAR}--\ref{equation:WAR}\}} & \multicolumn{1}{c|}{Eq.~\{\ref{equation:F1-score},\ref{equation:MAAF}--\ref{equation:WAF}\}} & \multicolumn{1}{c}{} \\
                    \cline{2-4}
                \end{tabular}};
        \end{scope}
    \end{tikzpicture}}

    \caption{Classification result example (2) showing a 3-class problem with balanced classes.}
    \label{figure:ML_example2}
\end{figure}




\clearpage




\subsection{Available implementations}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The following table gives an overview of machine learning metrics commonly used with the Python programming language machine learning libraries scikit-learn\footnote{scikit-learn project page, \url{https://scikit-learn.org/stable/}}, TensorFlow\footnote{TensorFlow project page, \url{https://www.tensorflow.org/}} (and Keras\footnote{Keras project page, \url{https://keras.io/}}), and PyTorch\footnote{PyTorch project page, \url{https://pytorch.org/}}\textsuperscript{,}\footnote{TorchMetrics project page, \url{https://torchmetrics.readthedocs.io/en/stable/}}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{table}[H]
    \centering

    \resizebox{\textwidth}{!}{\begin{tabular}{|l|c|c|c|}
        \hline
        Equation & scikit-learn & TensorFlow & PyTorch \\
        \hline
        %
        \hline

        True Positives
        &
        /
        &
        \href{https://www.tensorflow.org/api_docs/python/tf/keras/metrics/TruePositives}{TruePositives}
        &
        /
        \\

        True Negatives
        &
        /
        &
        \href{https://www.tensorflow.org/api_docs/python/tf/keras/metrics/TrueNegatives}{TrueNegatives}
        &
        /
        \\

        False Positives
        &
        /
        &
        \href{https://www.tensorflow.org/api_docs/python/tf/keras/metrics/FalsePositives}{FalsePositives}
        &
        /
        \\

        False Negatives
        &
        /
        &
        \href{https://www.tensorflow.org/api_docs/python/tf/keras/metrics/FalseNegatives}{FalseNegatives}
        &
        /
        \\

        \hline
        %
        \hline

        Precision (\ref{equation:precision})
        &
        \href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html\#sklearn-metrics-precision-score}{precision\_score}
        &
        \href{https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Precision}{Precision}
        &
        \href{https://torchmetrics.readthedocs.io/en/latest/classification/precision.html}{Precision}
        \\

        Recall (\ref{equation:recall})
        &
        \href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html\#sklearn.metrics.recall_score}{recall\_score}
        &
        \href{https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Recall}{Recall}
        &
        \href{https://torchmetrics.readthedocs.io/en/latest/classification/recall.html}{Recall}
        \\

        True negative rate (TNR) (\ref{equation:TNR})
        &
        /
        &
        /
        &
        \href{https://torchmetrics.readthedocs.io/en/latest/classification/specificity.html}{Specificity}
        \\

        Macro average precision (AP\textsubscript{macro}) (\ref{equation:MAAP})
        &
        \href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html\#sklearn.metrics.average_precision_score}{average\_precision\_score}
        &
        /
        &
        \href{https://torchmetrics.readthedocs.io/en/latest/classification/average_precision.html}{AveragePrecision}
        \\

        Micro average precision (AP\textsubscript{micro}) (\ref{equation:MIAP})
        &
        \href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html\#sklearn.metrics.average_precision_score}{average\_precision\_score}
        &
        /
        &
        \href{https://torchmetrics.readthedocs.io/en/latest/classification/average_precision.html}{AveragePrecision}
        \\

        Weighted average precision (AP\textsubscript{weighted}) (\ref{equation:WAP})
        &
        \href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html\#sklearn.metrics.average_precision_score}{average\_precision\_score}
        &
        /
        &
        \href{https://torchmetrics.readthedocs.io/en/latest/classification/average_precision.html}{AveragePrecision}
        \\

        Accuracy (A) (\ref{equation:A})
        &
        \href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\#sklearn.metrics.accuracy_score}{accuracy\_score}
        &
        \href{https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Accuracy}{Accuracy}
        &
        \href{https://torchmetrics.readthedocs.io/en/latest/classification/accuracy.html}{Accuracy}
        \\

        Balanced accuracy (BA) (\ref{equation:BA_binary},\ref{equation:BA_multi})
        &
        \href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html\#sklearn.metrics.balanced_accuracy_score}{balanced\_accuracy\_score}
        &
        /
        &
        /
        \\

        F-score (\ref{equation:F-score})
        &
        \href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html\#sklearn.metrics.fbeta_score}{fbeta\_score}
        &
        \href{https://www.tensorflow.org/api_docs/python/tf/keras/metrics/FBetaScore}{FBetaScore}
        &
        \href{https://torchmetrics.readthedocs.io/en/latest/classification/fbeta_score.html}{FBetaScore}
        \\

        F1-score (\ref{equation:F1-score})
        &
        \href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\#sklearn.metrics.f1_score}{f1\_score}
        &
        \href{https://www.tensorflow.org/api_docs/python/tf/keras/metrics/F1Score}{F1Score}
        &
        \href{https://torchmetrics.readthedocs.io/en/latest/classification/f1_score.html}{F1Score}
        \\

        Positive likelihood ratio (LR$+$) (\ref{equation:LR+})
        &
        \href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.class_likelihood_ratios.html\#sklearn.metrics.class_likelihood_ratios}{class\_likelihood\_ratios}
        &
        /
        &
        /
        \\

        Negative likelihood ratio (LR$-$) (\ref{equation:LR-})
        &
        \href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.class_likelihood_ratios.html\#sklearn.metrics.class_likelihood_ratios}{class\_likelihood\_ratios}
        &
        /
        &
        /
        \\

        Fowlkes--Mallows index (FM) (\ref{equation:FM})
        &
        \href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fowlkes_mallows_score.html\#sklearn.metrics.fowlkes_mallows_score}{fowlkes\_mallows\_score}
        &
        /
        &
        /
        \\

        Matthews correlation coefficient (MCC) (\ref{equation:MCC})
        &
        \href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html\#sklearn.metrics.matthews_corrcoef}{matthews\_corrcoef}
        &
        /
        &
        \href{https://torchmetrics.readthedocs.io/en/latest/classification/matthews_corr_coef.html}{MatthewsCorrCoef}
        \\

        Jaccard index (JI) (\ref{equation:JI})
        &
        \href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html\#sklearn.metrics.jaccard_score}{jaccard\_score}
        &
        /
        &
        \href{https://torchmetrics.readthedocs.io/en/latest/classification/jaccard_index.html}{JaccardIndex}
        \\

        Receiver operating characteristic curve (ROC curve)
        &
        \href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html\#sklearn.metrics.roc_curve}{roc\_curve}
        &
        /
        &
        \href{https://torchmetrics.readthedocs.io/en/latest/classification/roc.html}{ROC}
        \\

        Area under the curve (AUC) (\ref{equation:AUC})
        &
        \href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html\#sklearn.metrics.auc}{auc}
        &
        \href{https://www.tensorflow.org/api_docs/python/tf/keras/metrics/AUC}{AUC}
        &
        \href{https://torchmetrics.readthedocs.io/en/latest/classification/auroc.html}{AUROC}
        \\

        Average precision (AP) (\ref{equation:AP})
        &
        \href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html\#sklearn.metrics.average_precision_score}{average\_precision\_score}
        &
        /
        &
        \href{https://torchmetrics.readthedocs.io/en/latest/classification/average_precision.html}{AveragePrecision}
        \\

        Cohen's kappa (\ref{equation:k})
        &
        \href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html\#sklearn.metrics.cohen_kappa_score}{cohen\_kappa\_score}
        &
        /
        &
        \href{https://torchmetrics.readthedocs.io/en/latest/classification/cohen_kappa.html}{CohenKappa}
        \\

        \hline
    \end{tabular}}

    \caption{Selection of function calls for the available metrics in scikit-learn, TensorFlow, and PyTorch. The call for the respective metrics follows the corresponding scheme: scikit-learn~-- sklearn.metrics.<metric>, TensorFlow~-- tf.keras.metrics.<metric>, and PyTorch~-- torchmetrics.<metric>.}
    \label{table:ML_functions}
\end{table}




\clearpage




\section{Computer Vision}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In the realm of computer vision, the evaluation of algorithms through various error and similarity metrics is essential for ensuring the reliability and accuracy of models applied in areas such as image processing, object recognition, and scene understanding. These metrics serve as fundamental tools for quantifying the performance of vision systems across a range of real-world tasks.

The core of these evaluations begins with the concept of error measurement. The error, abbreviated as E, provides a basic indication of the deviation of predicted values from the ground truth. Based on E, the absolute error (AE) \cite{richardson2004h} quantifies the absolute difference between each predicted and actual value. By aggregating these errors, a sense of overall error magnitude without accounting for the direction of errors can be obtained. This metric is particularly useful in applications where errors of different orientations, or signs, are equally detrimental. Complementing AE, the relative absolute error (RAE) \cite{armstrong1992error, armstrong2000another, rodrigues2017machine} normalizes the absolute error by the magnitude of the true value, providing a scale-independent measure. The mean error (ME) \cite{fisher1920012, anjali2019temperature} and the mean absolute error (MAE) \cite{willmott2005advantages, hyndman2006another} extend these concepts by averaging errors across all predictions, with MAE providing a robust measure against outliers by using the absolute values of errors. For a percentage-based perspective, the mean percentage error (MPE) \cite{pearson1895x, jiang2008prediction} and the mean absolute percentage error (MAPE) \cite{armstrong1992error, hyndman2006another} express errors as a percentage of the actual values, which is valuable for comparing performance across data sets of varying scales. The mean absolute scaled error (MASE) \cite{hyndman2006another, mohan2018deep} offers an advanced normalization by scaling MAE against the in-sample MAE. It is especially suitable for time-series predictions in vision tasks involving motion or tracking.

Expanding on squared errors, the squared error (SE) \cite{draper1998applied} and the mean squared error (MSE) \cite{bickel2015mathematical} provide a measure where larger errors are exponentially penalized, making these metrics sensitive to outliers but valuable in applications where large errors are particularly undesirable. Based on the MSE metric, the root mean square error (RMSE) \cite{willmott2005advantages, hyndman2006another, pontius2008components} brings these scales back to the original units of measurement. Subsequently, the normalized root mean square error (NRMSE) \cite{chang2004air, kim2005missing} offers a relative measure by normalizing RMSE against the range of observed data, potentially enhancing comparability across different scales and data sets. The root mean squared logarithmic error (RMSLE) \cite{nafees2021predictive} addresses scenarios where proportional differences are more significant than absolute differences. Therefore, it is more frequently used in depth and logarithmic scale predictions. In terms of assessing the quality of visual outputs, the peak signal-to-noise ratio (PSNR) \cite{salomon2004data, huynh2008scope} is pivotal in image processing, especially in lossy compression, by comparing the level of desired signals to the background noise. In comparison to these metrics, the structural similarity (SSIM) \cite{wang2004image, ghodrati2019mr} and its counterpart, the structural dissimilarity (DSSIM) \cite{wang2004image, ghodrati2019mr}, evaluate the visual impact of changes to, e.g., luminance, contrast, and structure, providing a comprehensive measure of image quality degradation due to compression or other distortions.

In tasks including image segmentation, metrics such as the intersection over union (IoU) \cite{jaccard1912distribution, murphy1996finley, rezatofighi2019generalized, zou2023object}, the Dice coefficient (DC) \cite{dice1945measures, sorenson1948method}, and the overlap coefficient (OC) \cite{szymkiewicz1934conlribution, sempson1947holarctic, bell1962mutual, goodall1978sample} are of central importance. They assess the overlap between predicted and ground truth segments. They are crucial for evaluating the accuracy of boundary detection algorithms within domains such as medical imaging \cite{suzuki2017overview, esteva2021deep}, video surveillance \cite{buch2011review, brunetti2018computer}, as well as autonomous driving systems \cite{feng2021review, zablocki2022explainability}.

Together, these metrics provide a robust framework for diagnosing and enhancing the performance of computer vision systems, ensuring their efficiency and reliability in both controlled and erratic environments.

The following evaluation metrics within the context of machine learning are motivated by the contributions of \textit{Wang et al.} \cite{wang2003multiscale} and \textit{Hore and Ziou} \cite{hore2010image}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{General}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Table~\ref{table:CV_general} gives an overview of our general definitions for CV-related metrics.

\begin{table}[H]
    \centering

    \begin{tabular}{|c|c|}
        \hline
        Abbreviation & Meaning \\
        \hline
        %
        \hline
        $\textit{GT}$ & Ground truth     \\
        $P$           & Prediction       \\
        $n$           & Number of values \\
        \hline
    \end{tabular}

    \caption{General definitions computer vision.}
    \label{table:CV_general}
\end{table}


\subsection{Error (E)}

The error (E) is central for quantifying the performance and accuracy of algorithms in tasks such as object detection, image segmentation, and image classification. It measures the discrepancy between the predicted outputs of a model and the ground truth, providing insight into the model's accuracy and reliability. Common error metrics based on E include the squared error (SE), the mean square error (MSE), and the peak signal-to-noise ratio (PSNR). Lower error values signify higher model accuracy, indicating the model's proficiency in learning from the visual data and making correct predictions. However, error-based metrics must be interpreted carefully, considering the specific context and application, as different tasks may in turn require different types of error assessments. Additionally, error metrics can be sensitive to outliers and may not fully capture the visual quality and perceptual relevance of the results. Therefore, combining metrics based on the error with other evaluation criteria is essential for a comprehensive assessment.

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{The amount by which a prediction differs from the ground truth. (range: $(-\infty, \infty)$)} \\
        \textcolor{Green}{$+$} & Quantifies model prediction accuracy. \\
        \textcolor{Green}{$+$} & Simple and intuitive to interpret. \\
        \textcolor{Red}{$-$}   & Does not distinguish between types of errors. \\
        \textcolor{Red}{$-$}   & Sensitive to outliers.
    \end{tabularx}
\end{table}

\begin{equation}
    E = \textit{GT} - P
%
    \label{equation:E}
\end{equation}
\myequations{Error (E)}


\subsection[Absolute error~/ sum of absolute errors (AE)]{Absolute error~/ sum of absolute errors (AE) \cite{richardson2004h}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Calculates the sum (total) of all absolute errors. (range: $[0, \infty)$)} \\
        \textcolor{Green}{$+$} & Intuitive and straightforward to apply and interpret. \\
        \textcolor{Green}{$+$} & Accounts for absolute magnitude of errors. \\
        \textcolor{Red}{$-$}   & No differentiation depending on the number of compared values is made. \\
        \textcolor{Red}{$-$}   & Large individual differences equal to many small ones (distribution problem).
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{AE} = \sum\nolimits_{i = 1}^n |E_i| = \sum\nolimits_{i = 1}^n |\textit{GT}_i - P_i|
%
    \label{equation:AE}
\end{equation}
\myequations{Absolute error / sum of absolute errors (AE)}


\subsection[Relative absolute error (RAE)]{Relative absolute error (RAE) \cite{armstrong1992error, armstrong2000another, rodrigues2017machine}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Normalization of the absolute error by dividing the total absolute error of the simple predictor. (range: $[0, \infty)$)} \\
        \textcolor{Green}{$+$} & Comparison of models that differ significantly. \\
        \textcolor{Red}{$-$}   & Not sensitive to outliers and scaling.
    \end{tabularx}
\end{table}

\begin{equation}
    \begin{aligned}
        \textit{RAE}           &= \dfrac{\textit{AE}}{\sum\nolimits_{i = 1}^n |\textit{GT}_i - \overline{\textit{GT}}|} = \dfrac{\sum\nolimits_{i = 1}^n |\textit{GT}_i - P_i|}{\sum\nolimits_{i = 1}^n |\textit{GT}_i - \overline{\textit{GT}}|} \\
        \overline{\textit{GT}} &= \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \textit{GT}_i
%
        \label{equation:RAE}
    \end{aligned}
\end{equation}
%
\begin{conditions}
    \overline{\textit{GT}} & average of the ground truth
\end{conditions}
\myequations{Relative absolute error (RAE)}


\subsection[Mean error (ME)]{Mean error (ME) \cite{fisher1920012, anjali2019temperature}}

The mean error (ME) \cite{fisher1920012, anjali2019temperature} quantifies the average deviation between predicted values and ground truth data. It is particularly useful in tasks such as depth estimation and keypoint detection. ME is calculated by averaging the differences between corresponding values in the predicted and actual data sets. This provides a straightforward measure of model accuracy, indicating how well the model's predictions align with the true values. One of the main advantages of ME is its simplicity and ease of interpretation, making it a popular choice for initial model assessment. However, ME has its limitations. It is sensitive to outliers, which can provide a skewed representation of model performance. Moreover, ME does not account for the spatial structure of images, potentially overlooking important perceptual details.

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{The average over all error measurements. (range: $(-\infty, \infty)$)} \\
        \textcolor{Green}{$+$} & Intuitive and straightforward to apply. \\
        \textcolor{Green}{$+$} & Effective for initial model evaluation. \\
        \textcolor{Red}{$-$}   & Positive and negative error values can cancel each other out. \\
        \textcolor{Red}{$-$}   & Ignores spatial and perceptual details.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{ME} = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n E_i = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n (\textit{GT}_i - P_i)
%
    \label{equation:ME}
\end{equation}
\myequations{Mean error (ME)}


\subsection[Mean percentage error (MPE)]{Mean percentage error (MPE) \cite{pearson1895x, jiang2008prediction}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{The average over all error measurements in percentage. (range: $(-\infty\%, \infty\%)$)} \\
        \textcolor{Green}{$+$} & Intuitive overview of the underlying situation. \\
        \textcolor{Red}{$-$}   & Undefined as soon as a single ground truth value is zero.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{MPE} = \dfrac{100}{n} \cdot \sum\nolimits_{i = 1}^n \dfrac{E_i}{\textit{GT}_i} = \dfrac{100}{n} \cdot \sum\nolimits_{i = 1}^n \dfrac{\textit{GT}_i - P_i}{\textit{GT}_i}
%
    \label{equation:MPE}
\end{equation}
\myequations{Mean percentage error (MPE)}


\subsection[Mean absolute error (MAE)]{Mean absolute error (MAE) \cite{willmott2005advantages, hyndman2006another}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Calculates the mean of the sum (total) of all absolute errors. (range: $[0, \infty)$)} \\
        \textcolor{Green}{$+$} & Partially solves the distribution problem. \\
        \textcolor{Red}{$-$}   & No differentiation depending on the maximum assumable error is made.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{MAE} = \dfrac{\textit{AE}}{n} = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n |\textit{GT}_i - P_i|
%
    \label{equation:MAE}
\end{equation}
\myequations{Mean absolute error (MAE)}


\subsection[Mean absolute percentage error (MAPE)]{Mean absolute percentage error (MAPE) \cite{armstrong1992error, hyndman2006another}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{The average over all absolute error measurements in percentage. (range: $[0\%, \infty\%)$)} \\
        \textcolor{Green}{$+$} & Intuitive and scale independent. \\
        \textcolor{Red}{$-$}   & Undefined as soon as a single ground truth value is zero.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{MPE} = \dfrac{100}{n} \cdot \sum\nolimits_{i = 1}^n \dfrac{|E_i|}{|\textit{GT}_i|} = \dfrac{100}{n} \cdot \sum\nolimits_{i = 1}^n \dfrac{|\textit{GT}_i - P_i|}{|\textit{GT}_i|}
%
    \label{equation:MAPE}
\end{equation}
\myequations{Mean absolute percentage error (MAPE)}


\subsection[Mean absolute scaled error (MASE)]{Mean absolute scaled error (MASE) \cite{hyndman2006another, mohan2018deep}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Mean absolute error of the measurements scaled by the mean absolute error of the ground truth. (range: $[0, \infty)$)} \\
        \textcolor{Green}{$+$} & Scale invariant. \\
        \textcolor{Red}{$-$}   & Less sensitive to outliers.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{MASE} = \dfrac{\textit{MPE}}{\left(\dfrac{1}{n} - 1\right) \cdot \sum\nolimits_{i = 2}^n |\textit{GT}_i - \textit{GT}_{i - 1}|} = \dfrac{\dfrac{100}{n} \cdot \sum\nolimits_{i = 1}^n \dfrac{\textit{GT}_i - P_i}{\textit{GT}_i}}{\left(\dfrac{1}{n} - 1\right) \cdot \sum\nolimits_{i = 2}^n |\textit{GT}_i - \textit{GT}_{i - 1}|}
%
    \label{equation:MASE}
\end{equation}
\myequations{Mean absolute scaled error (MASE)}


\subsection[Mean normalized bias (MNB)]{Mean normalized bias (MNB) \cite{yu2006new, tsigaridis2014aerocom}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Calculates the variance between the predicted values and the ground truth values. Divides by the reference variable, subsequently calculating the mean. (range: $(-\infty, \infty)$)} \\
        \textcolor{Green}{$+$} & Enables the specific evaluation of systematic errors across the entire model. \\
        \textcolor{Red}{$-$}   & Does not detect specific errors in individual parts of the model.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{MNB} = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \dfrac{E_i}{P_i} = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \dfrac{\textit{GT}_i - P_i}{P_i}
%
    \label{equation:MNB}
\end{equation}
\myequations{Mean normalized bias (MNB)}


\subsection[Normalized mean bias (NMB)]{Normalized mean bias (NMB) \cite{mebust2003models, yu2006new}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Calculates the average of the variances between the prediction and the reference variable, subsequently normalizing it by the reference variable. (range: $(-\infty, \infty)$)} \\
        \textcolor{Green}{$+$} & Comparison of models independently of scaling. \\
        \textcolor{Red}{$-$}   & Sensitive to outliers.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{NMB} = \dfrac{\sum\nolimits_{i = 1}^n E_i}{\sum\nolimits_{i = 1}^n P_i} = \dfrac{\sum\nolimits_{i = 1}^n (\textit{GT}_i - P_i)}{\sum\nolimits_{i = 1}^n P_i}
%
    \label{equation:NMB}
\end{equation}
\myequations{Normalized mean bias (NMB)}


\subsection[Squared error~/ sum of squared errors (SE)]{Squared error~/ sum of squared errors (SE) \cite{draper1998applied}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Calculates the sum (total) of all squared errors. (range: $[0, \infty)$)} \\
        \textcolor{Green}{$+$} & Emphasizes the contribution of large errors. \\
        \textcolor{Red}{$-$}   & No differentiation depending on the number of compared values is made.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{SE} = \sum\nolimits_{i = 1}^n E_i^2 = \sum\nolimits_{i = 1}^n (\textit{GT}_i - P_i)^2
%
    \label{equation:SE}
\end{equation}
\myequations{Squared error / sum of squared errors (SE)}


\subsection[Mean square error (MSE)]{Mean square error (MSE) \cite{bickel2015mathematical}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Calculates the mean of the sum (total) of all squared errors. (range: $[0, \infty)$)} \\
        \textcolor{Green}{$+$} & Partially solves the distribution problem. \\
        \textcolor{Red}{$-$}   & No differentiation depending on the maximum assumable error is made.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{MSE} = \dfrac{\textit{SE}}{n} = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n (\textit{GT}_i - P_i)^2
%
    \label{equation:MSE}
\end{equation}
\myequations{Mean square error (MSE)}


\subsection[Root mean square error (RMSE)]{Root mean square error (RMSE) \cite{willmott2005advantages, hyndman2006another, pontius2008components}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Calculates the root of the mean of the sum (total) of all squared errors. (range: $[0, \infty)$)} \\
        \textcolor{Green}{$+$} & Provides a result in the range of the compared values. \\
        \textcolor{Red}{$-$}   & No differentiation depending on the maximum assumable error is made.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{RMSE} = \sqrt{\textit{MSE}} = \sqrt{\dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n (\textit{GT}_i - P_i)^2}
%
    \label{equation:RMSE}
\end{equation}
\myequations{Root mean square error (RMSE)}


\subsection[Normalized root mean square error (NRMSE)]{Normalized root mean square error (NRMSE) \cite{chang2004air, kim2005missing}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Normalization of RMSE. (range: $[0, \infty)$)} \\
        \textcolor{Green}{$+$} & Comparison of models that differ significantly. \\
        \textcolor{Red}{$-$}   & Not sensitive to outliers and scaling.
    \end{tabularx}
\end{table}

\begin{equation}
    \begin{aligned}
        \textit{NRMSE}         &= \dfrac{\textit{RMSE}}{\overline{\textit{GT}}} = \dfrac{\sqrt{\dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n (\textit{GT}_i - P_i)^2}}{\overline{\textit{GT}}} \\
        \overline{\textit{GT}} &= \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \textit{GT}_i
%
        \label{equation:NRMSE}
    \end{aligned}
\end{equation}
%
\begin{conditions}
    \overline{\textit{GT}} & average of the ground truth
\end{conditions}
\myequations{Normalized root mean square error (NRMSE)}


\subsection[Root mean squared logarithmic error (RMSLE)]{Root mean squared logarithmic error (RMSLE) \cite{nafees2021predictive}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Calculates the mean squared error of the logarithmized ground truth in comparison to the logarithmized predictions. (range: $[0, \infty)$)} \\
        \textcolor{Green}{$+$} & Robust to outliers. \\
        \textcolor{Red}{$-$}   & Biased penalty. Underestimation is penalized more than overestimation.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{RMSLE} = \sqrt{\dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n (\ln(\textit{GT}_i + 1) - \ln(P_i + 1))^2}
%
    \label{equation:RMSLE}
\end{equation}
\myequations{Root mean squared logarithmic error (RMSLE)}


\subsection[Peak signal-to-noise ratio (PSNR)]{Peak signal-to-noise ratio (PSNR) \cite{salomon2004data, huynh2008scope}}

The peak signal-to-noise ratio (PSNR) \cite{salomon2004data, huynh2008scope} is a widely used evaluation metric in computer vision, particularly for assessing the quality of image and video compression algorithms. PSNR measures the ratio between the maximum possible power of a signal and the power of corrupting noise that affects the fidelity of its representation. It is expressed in decibels (dB). Higher PSNR values indicate better quality with less distortion. PSNR is crucial for applications where maintaining high visual quality is essential, such as medical imaging, video streaming, and surveillance. However, PSNR has limitations as it primarily focuses on pixel-wise differences and may not align with human visual perception, which is more sensitive to structural and contextual information. While it provides a useful quantitative measure for initial assessment, it is often supplemented with perceptual metrics such as structural similarity (SSIM) to gain a more comprehensive evaluation of image quality. Understanding and optimizing PSNR can be essential for developing efficient compression algorithms that balance compression rates with visual fidelity.

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Calculates the MSE in relation to the maximum assumable error. (range: $[0, \infty)$)} \\
        \textcolor{Green}{$+$} & A differentiation depending on the maximum assumable error is made. \\
        \textcolor{Green}{$+$} & Widely accepted standard metric. \\
        \textcolor{Red}{$-$}   & May not reflect perceived visual quality. \\
        \textcolor{Red}{$-$}   & Less effective for complex distortions.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{PSNR} = 10 \cdot \log_{10} \dfrac{E_\textit{max}^2}{\textit{MSE}} = 10 \cdot \log_{10} \dfrac{E_\textit{max}^2}{\frac{1}{n} \cdot \sum\nolimits_{i = 1}^n (\textit{GT}_i - P_i)^2}
%
    \label{equation:PSNR}
\end{equation}
%
\begin{conditions}
    E_\textit{max} & maximum possible error
\end{conditions}
\myequations{Peak signal-to-noise ratio (PSNR)}


\subsection[Structural similarity (SSIM)]{Structural similarity (SSIM) \cite{wang2004image, ghodrati2019mr}}

The structural similarity (SSIM) \cite{wang2004image, ghodrati2019mr} is specifically designed to measure the perceptual similarity between two images. Unlike traditional metrics such as the mean square error (MSE) or the peak signal-to-noise ratio (PSNR), which focus on pixel-wise differences, SSIM may consider changes in structural information, luminance, and contrast. This makes SSIM more aligned with human visual perception. SSIM is often calculated on various windows of an image, comparing local patterns of pixel intensities that have been normalized for luminance and contrast. The resulting SSIM values range from $-1$ to $1$, where $1$ indicates perfect structural similarity and values close to $-1$ indicate no similarity. The SSIM metric is particularly beneficial for assessing the quality of image compression, denoising, and restoration algorithms, providing a more detailed understanding of image degradation than simple pixel-based metrics. However, SSIM has limitations, such as sensitivity to local variations and computational complexity compared to other metrics. Despite these drawbacks, SSIM's ability to correlate highly with perceived visual quality makes it a pivotal tool for developing and evaluating computer vision systems that prioritize human visual experience. Understanding and optimizing SSIM is essential for creating algorithms that deliver high-quality visual outputs.

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Calculates the structural similarity using the mean, variance, and covariance. (range: $[-1, 1]$)} \\
        \textcolor{Green}{$+$} & Correlates with human visual perception. \\
        \textcolor{Green}{$+$} & May measure structural, luminance, and contrast similarities. \\
        \textcolor{Red}{$-$}   & Sensitive to local variations. \\
        \textcolor{Red}{$-$}   & Increased calculation complexity.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{SSIM} = \dfrac{(2 \mu_x \mu_y + c_1) (2 \sigma_{xy} + c_2)}{(\mu_x^2 + \mu_y^2 + c_1) (\sigma_x^2 + \sigma_y^2 + c_2)}
%
    \label{equation:SSIM}
\end{equation}
%
\begin{conditions}
    \mu_x$, $\mu_y       & mean       \\
    \sigma_x$, $\sigma_y & variance   \\
    \sigma_{xy}          & covariance \\
    c_1$, $c_2           & division stabilizers, e.g., $(0.01 \cdot 2^8 - 1)^2$ and $(0.03 \cdot 2^8 - 1)^2$ (8 bits per value)
\end{conditions}
\myequations{Structural similarity (SSIM)}


\subsection[Structural dissimilarity (DSSIM)]{Structural dissimilarity (DSSIM) \cite{wang2004image, ghodrati2019mr}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Calculates the structural dissimilarity using the mean, variance, and covariance. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & Provides more accurate results by considering structural characteristics. \cite{wang2004image} \\
        \textcolor{Red}{$-$}   & Increased calculation complexity.
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{DSSIM} = \dfrac{1 - \textit{SSIM}}{2}
%
    \label{equation:DSSIM}
\end{equation}
\myequations{Structural dissimilarity (DSSIM)}


\subsection[Intersection over union (IoU)]{Intersection over union (IoU) \cite{jaccard1912distribution, murphy1996finley, rezatofighi2019generalized, zou2023object}}

Intersection over union (IoU) \cite{jaccard1912distribution, murphy1996finley, rezatofighi2019generalized, zou2023object} is a fundamental metric used primarily to evaluate the accuracy of object detection algorithms in tasks such as image segmentation and computer vision. IoU assesses the overlap between predicted and ground truth objects by calculating the ratio of the area of overlap to the area of union between the predicted and the actual annotations. It provides a clear, quantitative measure of how closely the contours of detected objects match the true object boundaries, encompassing both the correctness and precision of the detection in a single score. A higher IoU score indicates a greater degree of overlap and, consequently, an improved model performance. A perfect score of 1.0 represents an exact match between the predicted and the ground truth area. Because of its ability to accurately measure the effectiveness of detection models in capturing the true object space, IoU is extensively utilized in evaluating models for tasks such as autonomous driving, aerial image analysis \cite{al2018survey, akbari2021applications}, and medical imaging, where precise localization is critical. Its widespread adoption stems from its simplicity and effectiveness in providing a direct indicator of spatial accuracy.

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Calculates the similarity of two sets of values via the intersection over the union of both sets. In machine learning also known as the Jaccard index (JI). (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & Captures overlap between predicted and true regions. \\
        \textcolor{Green}{$+$} & Commonly used in image segmentation and object detection tasks. \\
        \textcolor{Red}{$-$}   & Sensitive to small region misalignment. \\
        \textcolor{Red}{$-$}   & Does not distinguish between types of errors.
    \end{tabularx}
\end{table}

\begin{figure}[H]
    \centering

    % https://texample.net/tikz/examples/set-operations-illustrated-with-venn-diagrams/
    \begin{tikzpicture}
        \colorlet{draw_color}{LimeGreen}
        \colorlet{fill_color}{LimeGreen!50}
        \newcommand{\SetP}{(-1, 0) circle (1.5)}
        \newcommand{\SetQ}{( 1, 0) circle (1.5)}
        \tikzset{filled/.style={draw=draw_color, fill=fill_color}, outline/.style={draw=draw_color}}

        \node at (-4, 0) {$\textit{IoU} =$};
        \begin{scope}[yshift=2cm]
            \begin{scope}
                \clip \SetP;
                \fill[filled] \SetQ;
            \end{scope}
            \draw[outline] \SetP node {$\textit{GT}$};
            \draw[outline] \SetQ node {$P$};
            \node at (4, 0) {$|\textit{GT} \cap P|$};
        \end{scope}
        \draw (-3, 0) -- (3, 0);
        \begin{scope}[yshift=-2cm]
            \draw[filled] \SetP node {$\textit{GT}$} \SetQ node {$P$};
            \node at (4, 0) {$|\textit{GT} \cup P|$};
        \end{scope}
    \end{tikzpicture}

    \caption{Intersection over union (IoU).}
    \label{figure:IoU}
\end{figure}

\begin{equation}
    \textit{IoU} = \dfrac{|\textit{GT} \cap P|}{|\textit{GT} \cup P|}
%
    \label{equation:IoU}
\end{equation}
\myequations{Intersection over union (IoU)}


\subsection[Dice coefficient (DC)]{Dice coefficient (DC) \cite{dice1945measures, sorenson1948method}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Calculates the similarity of two sets of values via twice the intersection over the sum of both sets. In machine learning also known as the F1-score. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & Can be well used for image segmentation and object detection. \\
        \textcolor{Red}{$-$}   & No differentiation depending on the size of both sets is made.
    \end{tabularx}
\end{table}

\begin{figure}[H]
    \centering

    % https://texample.net/tikz/examples/set-operations-illustrated-with-venn-diagrams/
    \begin{tikzpicture}
        \colorlet{draw_color}{LimeGreen}
        \colorlet{fill_color}{LimeGreen!50}
        \newcommand{\SetP}{(-1, 0) circle (1.5)}
        \newcommand{\SetQ}{( 1, 0) circle (1.5)}
        \tikzset{filled/.style={draw=draw_color, fill=fill_color}, outline/.style={draw=draw_color}}

        \node at (-5, 0) {$\textit{DC} =$};
        \begin{scope}[yshift=2cm]
            \node at (-3, 0) {$2 \cdot$};
            \begin{scope}
                \clip \SetP;
                \fill[filled] \SetQ;
            \end{scope}
            \draw[outline] \SetP node {$\textit{GT}$};
            \draw[outline] \SetQ node {$P$};
            \node at (5, 0) {$2 \cdot |\textit{GT} \cap P|$};
        \end{scope}
        \draw (-4, 0) -- (4, 0);
        \begin{scope}[yshift=-2cm]
            \draw[filled, xshift=-1cm] \SetP node {$\textit{GT}$};
            \node {$+$};
            \draw[filled, xshift=1cm] \SetQ node {$P$};
            \node at (5, 0) {$|\textit{GT}\,| + |P|$};
        \end{scope}
    \end{tikzpicture}

    \caption{Dice coefficient (DC).}
    \label{figure:DC}
\end{figure}

\begin{equation}
    \textit{DC} = \dfrac{2 \cdot |\textit{GT} \cap P|}{|\textit{GT}\,| + |P|}
%
    \label{equation:DC}
\end{equation}
\myequations{Dice coefficient (DC)}


\subsection[Overlap coefficient (OC)]{Overlap coefficient (OC) \cite{szymkiewicz1934conlribution, sempson1947holarctic, bell1962mutual, goodall1978sample}}

\begin{table}[H]\centering
    \begin{tabularx}{\textwidth}{@{}lX}
        \multicolumn{2}{@{}X}{Calculates the similarity of two sets of values via the intersection over the smaller set of both sets. (range: $[0, 1]$)} \\
        \textcolor{Green}{$+$} & A differentiation depending on the size of both sets is made. \\
    \end{tabularx}
\end{table}

\begin{equation}
    \textit{OC} = \dfrac{|\textit{GT} \cap P|}{\min(|\textit{GT}\,|, |P|)}
%
    \label{equation:OC}
\end{equation}
\myequations{Overlap coefficient (OC)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\clearpage




\subsection{Available implementations}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The following table gives an overview of computer vision metrics commonly used with the Python programming language machine learning libraries scikit-learn\footnote{scikit-learn project page, \url{https://scikit-learn.org/stable/}}, TensorFlow\footnote{TensorFlow project page, \url{https://www.tensorflow.org/}} (and Keras\footnote{Keras project page, \url{https://keras.io/}}), and PyTorch\footnote{PyTorch project page, \url{https://pytorch.org/}}\textsuperscript{,}\footnote{TorchMetrics project page, \url{https://torchmetrics.readthedocs.io/en/stable/}}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{table}[H]
    \centering

    \resizebox{\textwidth}{!}{\begin{tabular}{|l|c|c|c|}
        \hline
        Equation & scikit-learn & TensorFlow & PyTorch \\
        \hline
        %
        \hline

        Mean absolute error (MAE) (\ref{equation:MAE})
        &
        \href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html\#sklearn.metrics.mean_absolute_error}{mean\_absolute\_error}
        &
        \href{https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanAbsoluteError}{keras.losses.MeanAbsoluteError}
        &
        \href{https://torchmetrics.readthedocs.io/en/latest/regression/mean_absolute_error.html}{MeanAbsoluteError}
        \\

        Mean absolute percentage error (MAPE) (\ref{equation:MAPE})
        &
        \href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_percentage_error.html\#sklearn.metrics.mean_absolute_percentage_error}{mean\_absolute\_percentage\_error}
        &
        \href{https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanAbsolutePercentageError}{keras.losses.MeanAbsolutePercentageError}        
        &
        \href{https://torchmetrics.readthedocs.io/en/latest/regression/mean_absolute_percentage_error.html}{MeanAbsolutePercentageError}
        \\

        Mean square error (MSE) (\ref{equation:MSE})
        &
        \href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html\#sklearn.metrics.mean_squared_error}{mean\_squared\_error}
        &
        \href{https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanSquaredError}{keras.losses.MeanSquaredError}
        &
        \href{https://torchmetrics.readthedocs.io/en/latest/regression/mean_squared_error.html\#mean-squared-error-mse}{MeanSquaredError}
        \\

        Root mean square error (RMSE) (\ref{equation:RMSE})
        &
        /
        &
        \href{https://www.tensorflow.org/api_docs/python/tf/keras/metrics/RootMeanSquaredError}{keras.metrics.RootMeanSquaredError}
        &
        /
        \\

        Peak signal-to-noise ratio (PSNR) (\ref{equation:PSNR})
        &
        /
        &
        \href{https://www.tensorflow.org/api_docs/python/tf/image/psnr}{image.psnr}
        &
        /
        \\

        Structural similarity (SSIM) (\ref{equation:SSIM})
        &
        /
        &
        \href{https://www.tensorflow.org/api_docs/python/tf/image/ssim}{image.ssim}
        &
        /
        \\

        Intersection over union (IoU) (\ref{equation:IoU})
        &
        /
        &
        \href{https://www.tensorflow.org/api_docs/python/tf/keras/metrics/IoU}{keras.metrics.IoU}
        &
        \href{https://torchmetrics.readthedocs.io/en/latest/detection/intersection_over_union.html}{detection.iou.IntersectionOverUnion}
        \\

        Dice coefficient (DC) (\ref{equation:DC})
        &
        /
        &
        /
        &
        \href{https://torchmetrics.readthedocs.io/en/latest/classification/dice.html}{Dice}
        \\

        \hline
    \end{tabular}}

    \caption{Selection of function calls for the available metrics in scikit-learn, TensorFlow, and PyTorch. The call for the respective metrics follows the corresponding scheme: scikit-learn~-- sklearn.metrics.<metric>, TensorFlow~-- tf.<metric>, and PyTorch~-- torchmetrics.<metric>.}
    \label{table:CV_functions}
\end{table}




\clearpage




\section{Conclusion and Outlook}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In the dynamic and rapidly evolving fields of machine learning (ML) and computer vision (CV), the selection and application of appropriate evaluation and performance metrics are fundamental to the advancement and validation of innovative models. This manuscript provided a comprehensive overview of well-established and novel metrics, detailing their advantages, disadvantages, and origins. By consolidating current knowledge and providing insights into the effective use of these metrics, we aimed to equip researchers and practitioners with the essential tools to critically evaluate and improve the robustness and reliability of their models. We conclude that traditional metrics such as precision, recall, and accuracy, while foundational, often fail to capture the performance of ML and CV models, particularly in cases involving imbalanced data sets or complex classification tasks. Metrics, such as the F-score variants, the receiver operating characteristic curve (ROC curve) and the area under the curve (AUC), as well as the structural similarity and dissimilarity metrics, provide deeper insights and facilitate a more effective model evaluation and comparison. These metrics help to address challenges related to model interpretability, accountability, and robustness, which are crucial for real-world applications.

As the field of artificial intelligence (AI) continues to advance, so will the metrics used to evaluate and benchmark models. Future advancements in ML and CV will introduce novel algorithms and methodologies, necessitating the development of metrics that are able to capture their performance characteristics. The ongoing evolution of big data analysis, driven by advancements in AI and deep learning (DL), will therefore also shape the landscape of evaluation metrics. As models become more sophisticated and data sets more complex, there will be an ever-increasing need for metrics that can provide a holistic view of model performance, accounting for factors such as fairness but also ethical considerations. The further integration of ML and CV into interdisciplinary applications will spur the development of domain-specific needs tailored to particular industries and research areas. This trend will necessitate a collaborative approach, bringing together experts from different fields to develop and standardize metrics that can drive innovation and ensure the reliability of ML and CV systems.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\clearpage




\section*{Acknowledgment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We would like to extend our sincere gratitude to Professor Robert Gilmore Pontius for his valuable feedback on the evaluation metric Cohen's Kappa, and to Professor David John Hand for his insightful comments on the area under the curve (AUC) and the H-measure.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section*{Author contributions}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Tobias Schlosser, Michael Friedrich, and Trixy Meyer conducted this work's conceptualization and writing process, with the help of Danny Kowerko in extending it with examples.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\clearpage




\bibliographystyle{IEEEtran}
\bibliography{library}




\end{document}

