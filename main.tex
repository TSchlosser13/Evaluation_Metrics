%===============================================================================
% A Consolidated Overview of Evaluation and Performance Metrics for
%  Machine Learning and Computer Vision
%===============================================================================
% v0.2 - 11.05.2024 (this version): general text work
% v0.1 - 01.11.2023: init
%
% 2023-2024 Tobias Schlosser, Michael Friedrich, Trixy Meyer, and Danny Kowerko
%
% This manuscript is licensed under the Creative Commons Attribution 4.0
%  International (CC BY 4.0) license
%  https://creativecommons.org/licenses/by/4.0/deed
%===============================================================================




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Preamble %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% documentclass

\documentclass{article}

\usepackage[left=3cm, right=3cm, top=3cm, bottom=3cm]{geometry}


% usepackages

\usepackage{lmodern, mathtools, microtype, textgreek}

\usepackage[main=english, ngerman]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage[hyphens]{url}
\usepackage[colorlinks, citecolor=LimeGreen, pagebackref=true]{hyperref}
\usepackage[dvipsnames, table]{xcolor}

\usepackage{cite}

% https://ctan.net/macros/latex/contrib/hyperref/doc/hyperref-doc.pdf
\renewcommand*{\backref}[1]{}
\renewcommand*{\backrefalt}[4]{%
	\ifnum #3 > 0 {%
		\ifnum #3 = 1 {%
			\ifnum #1 = 1 {%
				\\ (1 citation on 1 page: #2)
			} \else {%
				\\ (1 citation on #1 pages: #2)
			} \fi
		} \else {%
			\ifnum #1 = 1 {%
				\\ (#3 citations on 1 page: #2)
			} \else {%
				\\ (#3 citations on #1 pages: #2)
			} \fi
		} \fi
	} \fi
}

\usepackage{tikz}
\usetikzlibrary{
	arrows,
	backgrounds,
	calc,
	decorations,
	calligraphy,
	fit,
	positioning,
	shapes,
	spy
}

\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usepgfplotslibrary{colorbrewer, fillbetween}

\usepackage[hang]{caption}

\usepackage{array, diagbox, float, makecell, multirow, subfig, tabularx, xfrac}

\usepackage[export]{adjustbox}

\usepackage[defaultlines=10, all]{nowidow}

\setlength{\parskip}{   1em }
\setlength{\parindent}{ 0em }

\addtolength{\skip\footins}{1cm}

\renewcommand*{\arraystretch}{1.33}

\linespread{1.1}\selectfont

\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}


% ORCID

\usepackage{fontawesome5}

% With / without "\thinspace" before "\textsuperscript{...}"
% \newcommand{\ORCID}[1]{\thinspace\textsuperscript{\href{https://orcid.org/#1}{\textcolor[HTML]{A6CE39}{\faOrcid}}}}
\newcommand{\ORCID}[1]{\textsuperscript{\href{https://orcid.org/#1}{\textcolor[HTML]{A6CE39}{\faOrcid}}}}

\newcommand{\ORCIDSchlosser}{0000-0002-0682-4284} % ORCID Tobias Schlosser
\newcommand{\ORCIDFriedrich}{0000-0001-6326-4749} % ORCID Michael Friedrich
\newcommand{\ORCIDMeyer}{0000-0002-3372-1619}     % ORCID Trixy Meyer
\newcommand{\ORCIDKowerko}{0000-0002-4538-7814}   % ORCID Danny Kowerko


% Page numbering
\usepackage{fancyhdr, lastpage}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\fancypagestyle{plain}{\fancyfoot[C]{\thepage/\pageref{LastPage}}}
\fancyfoot[C]{\thepage/\pageref{LastPage}}


% Miscellaneous

\usepackage[subfigure]{tocloft}

% "How to write a perfect equation parameters description?"
% https://tex.stackexchange.com/questions/95838/how-to-write-a-perfect-equation-parameters-description
\newenvironment{conditions}[1][where:]
	{\hspace{0.02\textwidth} #1 \begin{tabular}[t]{>{$}l<{$} @{${}={}$} l}}
	{\end{tabular}\\[\belowdisplayskip]}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Preamble %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\begin{document}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Title %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{\Huge A Consolidated Overview of Evaluation and Performance Metrics for Machine Learning and Computer Vision}

\author{
	Tobias Schlosser\ORCID{\ORCIDSchlosser}, Michael Friedrich\ORCID{\ORCIDFriedrich}, Trixy Meyer\ORCID{\ORCIDMeyer}, and Danny Kowerko\ORCID{\ORCIDKowerko} \\[1ex]
	Junior Professorship of Media Computing, \\
	Chemnitz University of Technology, \\
	09107 Chemnitz, Germany, \\
	\texttt{\{firstname.lastname\}@cs.tu-chemnitz.de}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Title %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\maketitle




\section*{Abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Within the rapidly evolving fields of machine learning (ML) and computer vision (CV), the selection and application of appropriate evaluation and performance metrics are paramount to the advancement and validation of innovative models. These metrics not only guide the development and fine-tuning of algorithms but also shape the interpretation of outcomes in diverse applications, ranging from automated disease diagnosis to autonomous driving. This manuscript is therefore also driven by the recent advancements in artificial intelligence (AI) and deep learning (DL) with a focus on big data analysis in areas such as object detection and classification by utilizing learning-based approaches including artificial neural networks (ANN) such as deep neural networks (DNN). For this purpose, we present well-established evaluation metrics, detailing their different advantages, disadvantages, and related origins. By consolidating current knowledge and offering insights into the effective use of these metrics, the goal of this work is to equip researchers and practitioners with the essential tools to critically evaluate and enhance the robustness and reliability of their models. Since this manuscript is designed to be a living resource, it will be systematically updated to incorporate novel evaluation metrics, ensuring its continued development.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\clearpage




\renewcommand{\contentsname}{Table of Contents}

\tableofcontents




\clearpage




\section{Introduction and Motivation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Within the recent years, the fields of machine learning (ML) and deep learning (DL) have witnessed remarkable growth fueled by advancements in algorithms, data availability, and computational power \cite{sejnowski2018deep, krizhevsky2012imagenet, szegedy2015going, bronstein2017geometric, liu2021survey, zaidi2022survey}. This evolution has led to significant improvements across a variety of applications including object detection and classification. Fundamental to these developments is the use of learning-based approaches such as deep neural networks (DNN), which are increasingly favored due to their powerful feature extraction and learning capabilities. However, the expansion of these technologies also presents new challenges, particularly in evaluating and optimizing performance under aspects of model interpretability, accountability, and robustness \cite{carvalho2019machine, zhou2021evaluating, cooper2022accountability}. The effectiveness of ML models in real-world scenarios depends on their ability to generalize from training data to unseen data, necessitating robust evaluation metrics that can accurately reflect model performance across diverse conditions and data sets.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{The need for comprehensive evaluation metrics}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Evaluation metrics serve as fundamental tools that provide insights into the effectiveness of machine learning models, guiding the selection, tuning, and optimization of models in scientific research and commercial applications. In computer vision (CV), the complexity of tasks such as image segmentation, object recognition, and motion analysis further complicates the assessment of model performance. Traditional metrics such as accuracy or error rate are often insufficient to capture the nuances of model behavior, especially in data sets with imbalanced classes. Moreover, the dynamic nature of ML and CV, with continually evolving models and techniques, demands a standardized yet comprehensive set of metrics that can adapt to new challenges. This need highlights the importance of utilizing well-established metrics that can offer deeper insights and facilitate the effective comparison of state-of-the-art techniques \cite{ferri2009experimental}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Objectives of this manuscript}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
This manuscript provides a comprehensive overview of evaluation and performance metrics suitable for ML and CV applications. It details established metrics, examining their strengths, weaknesses, and foundational concepts, thereby aiding in their appropriate selection and application. By consolidating an overview of traditional as well as novel metrics, this manuscript therefore serves as a comprehensive resource, supporting researchers and practitioners in accurately assessing and enhancing model performance.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\clearpage




% "Table of equations like list of figures"
% https://tex.stackexchange.com/questions/173102/table-of-equations-like-list-of-figures

\newcommand{\listequationsname}{\Large List of Equations}
\newlistof{myequations}{equ}{\listequationsname}
\newcommand{\myequations}[1]{\addcontentsline{equ}{myequations}{\protect\numberline{\theequation}#1}\par}
\setlength{\cftmyequationsnumwidth}{2.5em}

% \listofmyequations




\clearpage




\section{Machine Learning}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In the continuously evolving landscape of machine learning, the evaluation of predictive models through various metrics is integral to understanding their performance and guiding their optimization. Evaluation and performance metrics such as precision, recall, and accuracy provide foundational insights, while more nuanced metrics address specific aspects of model behavior and class imbalances.

For example, precision \cite{altman1994statistics, fletcher2019clinical}, also known as the positive predictive value, gauges the accuracy of positive predictions made by a model, indicating the proportion of true positive results in all positive classifications. This evaluation metric is vital in contexts where the cost of false positives is high, such as in medical diagnostics \cite{kononenko2001machine, ahsan2022machine} or fraud detection \cite{ashtiani2021intelligent, ali2022financial}. To accommodate the characteristics of diverse data sets, precision is often calculated in one of three forms: macro average \cite{yang1999evaluation, sebastiani2002machine, zhu2004recall, he2018local}, micro average \cite{yang1999evaluation, sebastiani2002machine}, or weighted average precision \cite{han2014rule}. Macro average precision treats all classes equally, averaging the precision calculated for each class, which prevents dominant classes from overshadowing smaller ones. Micro average precision aggregates the contributions of all classes to compute the overall precision, which is ideal for handling class imbalances. Weighted average precision, however, assigns a weight to each class's precision based on its representation in the data set, providing a balance between treating all classes equally and recognizing their different sizes.

Recall \cite{yerushalmy1947statistical, altman1994diagnostic}, or the true positive rate, complements precision by measuring the ability to detect all actual positives. It is crucial for applications where missing a positive instance carries severe consequences, such as detecting rare diseases. Like precision, recall can also be categorized into macro \cite{yang1999evaluation, sebastiani2002machine, rosenberg2012classifying, yang2020edgernn}, micro \cite{yang1999evaluation, sebastiani2002machine}, and weighted averages \cite{gordon1988effect, han2014rule} to adapt to various needs and data set structures. Negative predictive value \cite{altman1994statistics, fletcher2019clinical} and true negative rate \cite{yerushalmy1947statistical, altman1994diagnostic} measure the accuracy of negative predictions and the ability to identify negatives, respectively, which may help in rounding out the model's ability to correctly predict both classes. Accuracy \cite{metz1978basic, taylor1997introduction} provides an overall measure of model performance but can be misleading in unbalanced data sets. Balanced accuracy \cite{brodersen2010balanced, kelleher2020fundamentals} and its variations, including weighted balanced accuracy \cite{salman2017detection, infante2023factors}, provide a more trustworthy assessment by considering both sensitivity (recall) and specificity (true negative rate) adjusted for prevalence \cite{rothman2012epidemiology, bruce2018quantitative}. The F-score \cite{van2004geometry, taha2015metrics}, including variations such as the F1-score \cite{van2004geometry, taha2015metrics} and the F2-score \cite{van2004geometry, taha2015metrics}, combines precision and recall in a harmonic mean, balancing the trade-off between them. It is especially useful when seeking a model that reliably balances false positives and false negatives. Error metrics including the false discovery rate \cite{benjamini1995controlling, benjamini2001control} and false omission rate \cite{zafar2017fairness} delve deeper into the types of errors a model may show, providing insights that can be critical for refining model performance. The false discovery rate, for example, focuses on the proportion of false positives among the positive results, which is particularly relevant in scientific testing where validation is costly.

Further sophistication in evaluation metrics is seen in measures such as the diagnostic odds ratio \cite{glas2003diagnostic, doust2004systematic}, which compares the odds of positive test results, and the Fowlkes--Mallows index \cite{fowlkes1983method, halkidi2001clustering}, which measures the similarity between the predicted and true classifications. These metrics provide additional layers of understanding in scenarios where a simple accuracy is insufficient. Metrics including the positive likelihood ratio \cite{swets1973relative, deeks2004diagnostic} and the negative likelihood ratio \cite{swets1973relative, deeks2004diagnostic} provide evidence strength that a given condition or feature corresponds to a specific class, which is useful in diagnostic applications. The informedness or bookmaker informedness \cite{peirce1884numerical, youden1950index} provides, as opposed to random guessing, a measure of decision effectiveness.

Collectively, these metrics offer a multidimensional framework. They not only assess basic performance but also provide insights into the behavior of models across different scenarios, fostering more informed and effective machine learning applications.


The following evaluation metrics within the context of machine learning are motivated by the contributions of \textit{Metz} \cite{metz1978basic} and \textit{Fawcett} \cite{fawcett2006introduction}.


\subsection{General}

Figure~\ref{figure:ML_general} and Table~\ref{table:ML_general} give an overview of our general definitions for ML-related metrics.

\begin{figure}[H]
	\centering

	\scalebox{0.9}{\begin{tikzpicture}[
	 c/.style={draw, circle},
	 r/.style={draw, rectangle}]
		\node[r, fill=Yellow!90,   minimum width=6.5cm, minimum height=7.5cm] (r) at (0, 0) {};
		\node[c, fill=YellowGreen, minimum size=5.5cm, thick]                 (c) at (0, 0) {};
		\draw (0, -3.75) -- (0, 3.75);

		\node (ae) at (6,  3) {all elements};
		\node (se) at (6, -3) {selected elements};
		\draw (ae) -- (r);
		\draw (se) -- (c);

		\node (P)  at (-2, 4.25) {P};
		\node (N)  at ( 2, 4.25) {N};
		\node (TP) at (-1, 0)    {TP};
		\node (TN) at ( 2, 3)    {TN};
		\node (FP) at ( 1, 0)    {FP};
		\node (FN) at (-2, 3)    {FN};
	\end{tikzpicture}}

	\caption{General definitions machine learning.}
	\label{figure:ML_general}
\end{figure}

\begin{table}[H]
	\centering

	\begin{tabular}{|c|c|}
		\hline
		Abbreviation & Meaning \\
		\hline
		%
		\hline
		$\textit{T}$  & Total                       \\
		$\textit{P}$  & Positives                   \\
		$\textit{N}$  & Negatives                   \\
		$\textit{TP}$ & True Positives              \\
		$\textit{TN}$ & True Negatives              \\
		$\textit{FP}$ & False Positives             \\
		$\textit{FN}$ & False Negatives             \\
		$n$           & Number of values or classes \\
		\hline
	\end{tabular}

	\caption{General definitions machine learning.}
	\label{table:ML_general}
\end{table}


\subsection[Precision~/ positive predictive value (PPV)]{Precision~/ positive predictive value (PPV) \cite{altman1994statistics, fletcher2019clinical}}

Precision \cite{altman1994statistics, fletcher2019clinical} is an essential evaluation metric used to assess the accuracy of classification models, particularly focusing on the correctness of positive predictions. It measures the proportion of predicted positive instances that are truly positive, therefore providing critical insights into the model's performance in contexts where the cost of a false positive is significant, such as in spam detection or legal proceedings. By quantifying how many of the model's positive classifications are accurate, precision determines the reliability of the model's positive predictions. A high precision score indicates that the model is effective in minimizing false positives, thereby ensuring that most of its positive predictions are trustworthy. This metric is particularly valuable when the consequences of erroneous positive predictions are costly or disruptive. However, maximizing precision alone can sometimes lead to a model that is overly conservative in its positive predictions, potentially missing genuine positive cases (low recall). Therefore, in practice, precision is often considered alongside recall to optimize both the accuracy and coverage of the model's predictions, commonly evaluated through the F1-score, which harmonizes the trade-off between precision and recall to provide a more holistic view of model performance.

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Fraction of true positives and false positives. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Minimizes false positives. \\
		\textcolor{Green}{$+$} & Useful for critical false positive sensitive applications. \\
		\textcolor{Red}{$-$}   & Ignores true negatives and false negatives. \\
		\textcolor{Red}{$-$}   & May overlook recall for higher precision.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{Precision} = \dfrac{\textit{TP}}{\textit{TP} + \textit{FP}}
%
	\label{equation:precision}
\end{equation}
\myequations{Precision / positive predictive value (PPV)}


\subsubsection[Macro average precision (APmacro)]{Macro average precision (AP\textsubscript{macro}) \cite{yang1999evaluation, sebastiani2002machine, zhu2004recall, he2018local}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Macro average precision calculates the average precision for each class separately and takes the average over all classes. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Each class is weighted equally regardless of its frequency in the data set, therefore allowing each class to be evaluated individually. \\
		\textcolor{Green}{$+$} & Advantageous when the recognition rate of TP compared to FP is of interest. \\
		\textcolor{Red}{$-$}   & Rare classes have the same weight as frequent classes. This can lead to bias. \\
		\textcolor{Red}{$-$}   & Only partially evaluates a model's classification capabilities.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{AP}_\textit{macro} = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \textit{Precision}_i = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \dfrac{\textit{TP}_i}{\textit{TP}_i + \textit{FP}_i}
%
	\label{equation:MAAP}
\end{equation}
\myequations{Macro average precision (AP\textsubscript{macro})}


\subsubsection[Micro average precision (APmicro)]{Micro average precision (AP\textsubscript{micro}) \cite{yang1999evaluation, sebastiani2002machine}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Micro average precision calculates the precision across all classes. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Use with inconsistent data sets / class distributions. Provides overview of overall model performance. \\
		\textcolor{Red}{$-$}   & Neglects infrequent classes and overestimates frequent classes.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{AP}_\textit{micro} = \dfrac{\sum\nolimits_{i = 1}^n \textit{TP}_i}{\sum\nolimits_{i = 1}^n (\textit{TP}_i + \textit{FP}_i)}
%
	\label{equation:MIAP}
\end{equation}
\myequations{Micro average precision (AP\textsubscript{micro})}


\subsubsection[Weighted average precision (APweighted)]{Weighted average precision (AP\textsubscript{weighted}) \cite{han2014rule}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Weighted average precision calculates the weighted precision across all classes. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Use with inconsistent data sets / class distributions. Provides overview of overall model performance. \\
		\textcolor{Red}{$-$}   & Neglects under-weighted classes and overestimates over-weighted classes.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{AP}_\textit{weighted} = \dfrac{1}{\sum\nolimits_{i = 1}^n w_i} \cdot \sum\nolimits_{i = 1}^n w_i \cdot \textit{Precision}_i = \dfrac{1}{\sum\nolimits_{i = 1}^n w_i} \cdot \sum\nolimits_{i = 1}^n w_i \cdot \dfrac{\textit{TP}_i}{\textit{TP}_i + \textit{FP}_i}
%
	\label{equation:WAP}
\end{equation}
%
\begin{conditions}
	w_i & samples per class or the relation of samples per class to the total of all samples
\end{conditions}
\myequations{Weighted average precision (AP\textsubscript{weighted})}


\subsection[Negative predictive value (NPV)]{Negative predictive value (NPV) \cite{altman1994statistics, fletcher2019clinical}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Fraction of true negatives and false negatives. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Emphasizes the accuracy of classifying true negatives in relation to false negatives. \\
		\textcolor{Red}{$-$}   & Neglects true positives and false positives.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{NPV} = \dfrac{\textit{TN}}{\textit{TN} + \textit{FN}}
%
	\label{equation:NPV}
\end{equation}
\myequations{Negative predictive value (NPV)}


\subsection[Recall~/ true positive rate (TPR)~/ sensitivity~/ hit rate]{Recall~/ true positive rate (TPR)~/ sensitivity~/ hit rate \cite{yerushalmy1947statistical, altman1994diagnostic}}

Recall \cite{yerushalmy1947statistical, altman1994diagnostic} particularly emphasizes the model's ability to identify all relevant instances within a data set. Commonly used in contexts where the cost of missing a positive case (false negative) is high, such as in medical diagnostics or fraud detection, recall calculates the proportion of actual positives that have been correctly identified by the model. It serves as a critical measure by quantifying the model's sensitivity to detecting positive samples amidst a pool of negatives, thereby highlighting potential weaknesses in capturing all pertinent cases. A high recall score indicates that the model effectively captures the majority of positive cases, which is paramount in scenarios where failing to detect positives could have severe consequences. Consequently, optimizing for recall is essential in fine-tuning the performance of models deployed in high-stakes environments, ensuring that few positive cases go unnoticed. However, focusing solely on recall can sometimes lead to a decrease in precision, whereas the model might also increase the number of false positives. Therefore, recall is often balanced with other metrics such as precision to provide a more comprehensive understanding of a model's overall performance.

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Fraction of true positives and false negatives. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Ensures minimal false negatives. \\
		\textcolor{Green}{$+$} & Critical for identifying rare positive cases. \\
		\textcolor{Red}{$-$}   & Ignores true negatives and false positives. \\
		\textcolor{Red}{$-$}   & May overlook precision for higher recall.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{Recall} = \dfrac{\textit{TP}}{\textit{P}} = \dfrac{\textit{TP}}{\textit{TP} + \textit{FN}}
%
	\label{equation:recall}
\end{equation}
\myequations{Recall / true positive rate (TPR) / sensitivity / hit rate}


\subsubsection[Macro average recall (ARmacro)]{Macro average recall (AR\textsubscript{macro}) \cite{yang1999evaluation, sebastiani2002machine, rosenberg2012classifying, yang2020edgernn}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Macro average recall calculates the average recall for each class separately and takes the average over all classes. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Each class is weighted equally regardless of its frequency in the data set, therefore allowing each class to be evaluated individually. \\
		\textcolor{Green}{$+$} & Advantageous when the recognition rate of TP compared to FN is of interest. \\
		\textcolor{Red}{$-$}   & Rare classes have the same weight as frequent classes. This can lead to bias. \\
		\textcolor{Red}{$-$}   & Only partially evaluates a model's classification capabilities.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{AR}_\textit{macro} = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \textit{Recall}_i = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \dfrac{\textit{TP}_i}{\textit{TP}_i + \textit{FN}_i}
%
	\label{equation:MAAR}
\end{equation}
\myequations{Macro average recall (AR\textsubscript{macro})}


\subsubsection[Micro average recall (ARmicro)]{Micro average recall (AR\textsubscript{micro}) \cite{yang1999evaluation, sebastiani2002machine}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Micro average recall calculates the recall across all classes. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Use with inconsistent data sets / class distributions. Provides overview of overall model performance. \\
		\textcolor{Red}{$-$}   & Neglects infrequent classes and overestimates frequent classes.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{AR}_\textit{micro} = \dfrac{\sum\nolimits_{i = 1}^n \textit{TP}_i}{\sum\nolimits_{i = 1}^n (\textit{TP}_i + \textit{FN}_i)}
%
	\label{equation:MIAR}
\end{equation}
\myequations{Micro average recall (AR\textsubscript{micro})}


\subsubsection[Weighted average recall (ARweighted)]{Weighted average recall (AR\textsubscript{weighted}) \cite{gordon1988effect, han2014rule}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Weighted average recall calculates the weighted recall across all classes. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Use with inconsistent data sets / class distributions. Provides overview of overall model performance. \\
		\textcolor{Red}{$-$}   & Neglects under-weighted classes and overestimates over-weighted classes.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{AR}_\textit{weighted} = \dfrac{1}{\sum\nolimits_{i = 1}^n w_i} \cdot \sum\nolimits_{i = 1}^n w_i \cdot \textit{Recall}_i = \dfrac{1}{\sum\nolimits_{i = 1}^n w_i} \cdot \sum\nolimits_{i = 1}^n w_i \cdot \dfrac{\textit{TP}_i}{\textit{TP}_i + \textit{FN}_i}
%
	\label{equation:WAR}
\end{equation}
%
\begin{conditions}
	w_i & samples per class or the relation of samples per class to the total of all samples
\end{conditions}
\myequations{Weighted average recall (AR\textsubscript{weighted})}


\subsection[True negative rate (TNR)~/ specificity~/ selectivity]{True negative rate (TNR)~/ specificity~/ selectivity \cite{yerushalmy1947statistical, altman1994diagnostic}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Fraction of true negatives and false positives. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Emphasizes the accuracy of classifying true negatives in relation to false positives. \\
		\textcolor{Red}{$-$}   & Neglects true positives and false negatives.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{TNR} = \dfrac{\textit{TN}}{\textit{N}} = \dfrac{\textit{TN}}{\textit{TN} + \textit{FP}}
%
	\label{equation:TNR}
\end{equation}
\myequations{True negative rate (TNR) / specificity / selectivity}


\subsection[Prevalence]{Prevalence \cite{rothman2012epidemiology, bruce2018quantitative}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Fraction of positives with positives and negatives. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Straightforward to calculate. \\
		\textcolor{Red}{$-$}   & Neglects TP, TN, FP, and FN.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{Prevalence} = \dfrac{\textit{P}}{\textit{T}} = \dfrac{\textit{P}}{\textit{P} + \textit{N}}
%
	\label{equation:prevalence}
\end{equation}
\myequations{Prevalence}


\subsection[Accuracy (A)]{Accuracy (A) \cite{metz1978basic, taylor1997introduction}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Fraction of correct classifications. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Emphasizes the accuracy of classifying true positives and true negatives. \\
		\textcolor{Red}{$-$}   & Misleading for imbalanced data (see also balanced accuracy).
	\end{tabularx}
\end{table}

\begin{equation}
	A = \dfrac{\textit{TP} + \textit{TN}}{\textit{T}} = \dfrac{\textit{TP} + \textit{TN}}{\textit{P} + \textit{N}} = \dfrac{\textit{TP} + \textit{TN}}{\textit{TP} + \textit{TN} + \textit{FP} + \textit{FN}}
%
	\label{equation:A}
\end{equation}
\myequations{Accuracy (A)}


\subsection[Balanced accuracy (BA)]{Balanced accuracy (BA) \cite{brodersen2010balanced, kelleher2020fundamentals}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Balanced fraction of correct classifications. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Normalizes true positives and true negatives. \\
		\textcolor{Red}{$-$}   & Neglects, e.g., FPR and FNR.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{BA}_\textit{binary} = \dfrac{\textit{TPR} + \textit{TNR}}{2} = \dfrac{\dfrac{\textit{TP}}{\textit{TP} + \textit{FN}} + \dfrac{\textit{TN}}{\textit{TN} + \textit{FP}}}{2}
%
	\label{equation:BA_binary}
\end{equation}
\myequations{Balanced accuracy (BA) for binary classification}

\begin{equation}
	\textit{BA}_\textit{multi} = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \textit{Recall}_i
%
	\label{equation:BA_multi}
\end{equation}
\myequations{Balanced accuracy (BA) for multi-class classification}


\subsection[Balanced accuracy weighted (BAW)]{Balanced accuracy weighted (BAW) \cite{salman2017detection, infante2023factors}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Building upon the balanced accuracy approach, an additional class weighting is added. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Suitable for multi-class problems, robust against class imbalance. \\
		\textcolor{Red}{$-$}   & More complex and rarely used metric.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{BAW} = \dfrac{1}{\sum\nolimits_{i = 1}^n w_i} \cdot \sum\nolimits_{i = 1}^n w_i \cdot \textit{Recall}_i = \dfrac{1}{\sum\nolimits_{i = 1}^n w_i} \cdot \sum\nolimits_{i = 1}^n w_i \cdot \dfrac{\textit{TP}_i}{\textit{TP}_i + \textit{FN}_i}
%
	\label{equation:BAW}
\end{equation}
%
\begin{conditions}
	w_i & samples per class or the relation of samples per class to the total of all samples
\end{conditions}
\myequations{Balanced accuracy weighted (BAW)}


\subsection[Average accuracy (AA)]{Average accuracy (AA) \cite{brodersen2010balanced, huang2019ecg}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Average accuracy over all classes. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Suitable for data sets that consist of balanced classes. \\
		\textcolor{Red}{$-$}   & Can yield poor results when a biased classifier is tested on imbalanced data.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{AA} = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n A_i = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \dfrac{\textit{TP}_i + \textit{TN}_i}{\textit{TP}_i + \textit{FP}_i + \textit{TN}_i + \textit{FN}_i}
%
	\label{equation:AA}
\end{equation}
\myequations{Average accuracy (AA)}


\subsection[Average class accuracy (ACA)]{Average class accuracy (ACA) \cite{bhowan2011developing, devarriya2020unbalanced}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Calculates a weighted average classification accuracy based on a minority accuracy corresponding to TPR and a majority accuracy corresponding to TNR. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Suitable when a significant class imbalance between minority and majority classes exists. \\
		\textcolor{Red}{$-$}   & Difficult to choose a good weighting factor.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{ACA} = w \cdot \textit{TPR} + (1 - w) \cdot \textit{TNR} = w \cdot \dfrac{\textit{TP}}{\textit{TP} + \textit{FN}} + (1 - w) \cdot \dfrac{\textit{TN}}{\textit{TN} + \textit{FP}}
%
	\label{equation:ACA}
\end{equation}
%
\begin{conditions}
	w & weight of the positive class in relation to the data set, $0 \le w \le 1$
\end{conditions}
\myequations{Average class accuracy (ACA)}


\subsection[Error rate (ER)]{Error rate (ER) \cite{hand1986recent, asri2016using}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Complementary metric to accuracy. All faulty classifications are divided by the total number of classifications. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Suitable for problems where the evaluation of error recognition is of importance. \\
		\textcolor{Red}{$-$}   & Poor performance for imbalanced data.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{ER} = \dfrac{\textit{FP} + \textit{FN}}{\textit{T}} = \dfrac{\textit{FP} + \textit{FN}}{\textit{P} + \textit{N}} = \dfrac{\textit{FP} + \textit{FN}}{\textit{TP} + \textit{FP} + \textit{TN} + \textit{FN}}
%
	\label{equation:ER}
\end{equation}
\myequations{Error rate (ER)}


\subsection[Average error rate (AER)]{Average error rate (AER) \cite{hamamoto1998gabor, han2016variable}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Average error rate over all classes. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Suitable for problems where the evaluation of error recognition is of importance. \\
		\textcolor{Red}{$-$}   & Poor performance for imbalanced data.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{AER} = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \textit{ER}_i = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \dfrac{\textit{FP}_i + \textit{FN}_i}{\textit{TP}_i + \textit{FP}_i + \textit{TN}_i + \textit{FN}_i}
%
	\label{equation:AER}
\end{equation}
\myequations{Average error rate (AER)}


\subsection[F-score~/ F\textbeta-score]{F-score~/ F\textbeta-score \cite{van2004geometry, taha2015metrics}}

The F-score \cite{van2004geometry, taha2015metrics}, or F\textbeta-score, serves as a versatile evaluation metric in classification tasks, allowing for the weighted balancing of precision and recall according to specific application needs. It introduces the parameter \textbeta, which adjusts the emphasis placed on recall relative to precision. A \textbeta{} greater than one prioritizes recall, making it particularly useful in contexts where missing a positive instance carries severe repercussions, such as in medical diagnostics or fraud detection. Conversely, a \textbeta{} less than one shifts the focus towards precision, suitable for applications where false positives are more detrimental, such as in spam filtering. By calculating the weighted harmonic mean of precision and recall, the F\textbeta-score quantifies the trade-offs between both metrics, providing a nuanced view of model performance. High values of the F\textbeta-score indicate that the model not only performs well in terms of the chosen emphasis on precision or recall but also maintains a reasonable balance according to the specificities dictated by \textbeta, thereby ensuring model decisions align closely with the intended requirements.

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Measures the user-defined classification effectiveness. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Balances precision and recall. \\
		\textcolor{Green}{$+$} & Adjustable for precision or recall emphasis. \\
		\textcolor{Red}{$-$}   & Harder to interpret than individual components. \\
		\textcolor{Red}{$-$}   & Not sensitive to data distribution changes.
	\end{tabularx}
\end{table}

\begin{equation}
	F_\beta = (1 + \beta^2) \cdot \dfrac{\textit{Precision} \cdot \textit{Recall}}{(\beta^2 \cdot \textit{Precision}) + \textit{Recall}} = \dfrac{(1 + \beta^2) \cdot \textit{TP}}{(1 + \beta^2) \cdot \textit{TP} + \beta^2 \cdot \textit{FN} + \textit{FP}}
%
	\label{equation:F-score}
\end{equation}
\myequations{F-score / F\textbeta-score}


\subsubsection[Macro average F-score]{Macro average F-score \cite{mohammad2013nrc, takahashi2022confidence}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Averaged F-score over all classes. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Values all classes equally. \\
		\textcolor{Red}{$-$}   & Valuing all classes equally can be detrimental in case of class imbalance.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{Macro average F-score} = \dfrac{1}{\textit{n}} \cdot \sum\nolimits_{i = 1}^n \textit{F}_i
%
	\label{equation:MAAF}
\end{equation}
%
\begin{conditions}
	F_i & F-score of class $i$ with a chosen $\beta$
\end{conditions}
\myequations{Macro average F-score}


\subsubsection[Micro average F-score]{Micro average F-score \cite{goutte2005probabilistic, takahashi2022confidence}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Averaged F-score based on the micro average of precision and recall. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Useful when the performance of larger classes is of importance. \\
		\textcolor{Red}{$-$}   & Prone to issues with class imbalance.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{Micro average F-score} = 2 \cdot \dfrac{\textit{AP}_\textit{micro} \cdot \textit{AR}_\textit{micro}}{\textit{AP}_\textit{micro} + \textit{AR}_\textit{micro}}
%
	\label{equation:MIAF}
\end{equation}
\myequations{Micro average F-score}


\subsubsection[Weighted average F-score]{Weighted average F-score \cite{al2016lili, alswaidan2020hybrid}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Weighted averaged F-score over all classes. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & More robust against class imbalance. \\
		\textcolor{Red}{$-$}   & Not widely used within the literature.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{Weighted average F-score} = \dfrac{1}{\sum\nolimits_{i = 1}^n w_i} \cdot \sum\nolimits_{i = 1}^n (w_i \cdot F_i)
%
	\label{equation:WAF}
\end{equation}
%
\begin{conditions}
	w_i & samples per class or the relation of samples per class to the total of all samples \\
	F_i & F-score of class $i$
\end{conditions}
\myequations{Weighted average F-score}


\subsubsection[F0-score]{F0-score \cite{van2004geometry, taha2015metrics}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{The recall is weighted 0 times as important as the precision. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & The importance of the recall is maximized. \\
		\textcolor{Red}{$-$}   & The importance of the precision is minimized.
	\end{tabularx}
\end{table}

\begin{equation}
	F_0 = (1 + 0^2) \cdot \dfrac{\textit{Precision} \cdot \textit{Recall}}{(0^2 \cdot \textit{Precision}) + \textit{Recall}} = \dfrac{\textit{Precision} \cdot \textit{Recall}}{\textit{Recall}}
%
	\label{equation:F0-score}
\end{equation}
\myequations{F0-score}


\subsubsection[F0.5-score]{F0.5-score \cite{van2004geometry, taha2015metrics}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{The recall is weighted 0.5 times as important as the precision. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & The importance of the recall is increased. \\
		\textcolor{Red}{$-$}   & The importance of the precision is decreased.
	\end{tabularx}
\end{table}

\begin{equation}
	F_{0.5} = (1 + 0.5^2) \cdot \dfrac{\textit{Precision} \cdot \textit{Recall}}{(0.5^2 \cdot \textit{Precision}) + \textit{Recall}} = 1.25 \cdot \dfrac{\textit{Precision} \cdot \textit{Recall}}{(0.25 \cdot \textit{Precision}) + \textit{Recall}}
%
	\label{equation:F0.5-score}
\end{equation}
\myequations{F0.5-score}


\subsubsection[F1-score]{F1-score \cite{van2004geometry, taha2015metrics}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Measures the harmonic mean of the precision and the recall. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Equal importance of precision and recall. \\
		\textcolor{Red}{$-$}   & Does not consider true negatives.
	\end{tabularx}
\end{table}

\begin{equation}
	F_1 = (1 + 1^2) \cdot \dfrac{\textit{Precision} \cdot \textit{Recall}}{(1^2 \cdot \textit{Precision}) + \textit{Recall}} = 2 \cdot \dfrac{\textit{Precision} \cdot \textit{Recall}}{\textit{Precision} + \textit{Recall}}
%
	\label{equation:F1-score}
\end{equation}
\myequations{F1-score}


\subsubsection[F2-score]{F2-score \cite{van2004geometry, taha2015metrics}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{The recall is weighted 2 times as important as the precision. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & The importance of the recall is decreased. \\
		\textcolor{Red}{$-$}   & The importance of the precision is increased.
	\end{tabularx}
\end{table}

\begin{equation}
	F_2 = (1 + 2^2) \cdot \dfrac{\textit{Precision} \cdot \textit{Recall}}{(2^2 \cdot \textit{Precision}) + \textit{Recall}} = 5 \cdot \dfrac{\textit{Precision} \cdot \textit{Recall}}{(4 \cdot \textit{Precision}) + \textit{Recall}}
%
	\label{equation:F2-score}
\end{equation}
\myequations{F2-score}


\subsection[False discovery rate (FDR)]{False discovery rate (FDR) \cite{benjamini1995controlling, benjamini2001control}}

The false discovery rate (FDR) \cite{benjamini1995controlling, benjamini2001control} is an evaluation metric used predominantly in multiple hypothesis testing to measure the proportion of false positives among the rejected hypotheses. This is particularly crucial in fields such as genomics, where large numbers of simultaneous tests are conducted and distinguishing between truly significant results and those due to chance. FDR quantifies the expected proportion of erroneous rejections (false discoveries) in a set of claimed findings. This focus on controlling the rate of false positives means that FDR is particularly useful when the cost of a false positive is significant. A lower FDR value indicates a more reliable rejection of null hypotheses, implying that a higher proportion of identified significant results are likely to be truly significant. FDR provides a balance between discovery and error, which allows to maximize true discoveries while maintaining a controlled rate of false positives. This balance is especially important in exploratory research where the ability to detect genuine effects without being overwhelmed by spurious findings is crucial.

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Fraction of false positives and true positives. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Focuses on proportion of false positives. \\
		\textcolor{Green}{$+$} & Useful in multiple comparison scenarios. \\
		\textcolor{Red}{$-$}   & Ignores true negatives and false negatives. \\
		\textcolor{Red}{$-$}   & Sensitive to class imbalance.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{FDR} = \dfrac{\textit{FP}}{\textit{FP} + \textit{TP}}
%
	\label{equation:FDR}
\end{equation}
\myequations{False discovery rate (FDR)}


\subsubsection{Macro average FDR (FDRmacro)}

\begin{equation}
	\textit{FDR}_\textit{macro} = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \textit{FDR}_i = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \dfrac{\textit{FP}_i}{\textit{FP}_i + \textit{TP}_i}
%
	\label{equation:MAAFDR}
\end{equation}
\myequations{Macro average FDR (FDR\textsubscript{macro})}


\subsubsection{Micro average FDR (FDRmicro)}

\begin{equation}
	\textit{FDR}_\textit{micro} = \dfrac{\sum\nolimits_{i = 1}^n \textit{FP}_i}{\sum\nolimits_{i = 1}^n (\textit{FP}_i + \textit{TP}_i)}
%
	\label{equation:MIAFDR}
\end{equation}
\myequations{Micro average FDR (FDR\textsubscript{micro})}


\subsubsection{Weighted average FDR (FDRweighted)}

\begin{equation}
	\textit{FDR}_\textit{weighted} = \dfrac{1}{\sum\nolimits_{i = 1}^n w_i} \cdot \sum\nolimits_{i = 1}^n w_i \cdot \textit{FDR}_i = \dfrac{1}{\sum\nolimits_{i = 1}^n w_i} \cdot \sum\nolimits_{i = 1}^n w_i \cdot \dfrac{\textit{FP}_i}{\textit{FP}_i + \textit{TP}_i}
%
	\label{equation:WAFDR}
\end{equation}
\myequations{Weighted average FDR (FDR\textsubscript{weighted})}


\subsection[False omission rate (FOR)]{False omission rate (FOR) \cite{zafar2017fairness}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Fraction of false negatives and true negatives. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Emphasizes the accuracy of classifying false negatives in relation to true negatives. \\
		\textcolor{Red}{$-$}   & Neglects true positives and false positives.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{FOR} = \dfrac{\textit{FN}}{\textit{FN} + \textit{TN}}
%
	\label{equation:FOR}
\end{equation}
\myequations{False omission rate (FOR)}


\subsection[False positive rate (FPR)]{False positive rate (FPR) \cite{banerjee2009hypothesis}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Fraction of false positives and true negatives. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Emphasizes the accuracy of classifying false positives in relation to true negatives. \\
		\textcolor{Red}{$-$}   & Neglects true positives and false negatives.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{FPR} = \dfrac{\textit{FP}}{\textit{N}} = \dfrac{\textit{FP}}{\textit{FP} + \textit{TN}}
%
	\label{equation:FPR}
\end{equation}
\myequations{False positive rate (FPR)}


\subsection[False negative rate (FNR)~/ miss rate]{False negative rate (FNR)~/ miss rate \cite{banerjee2009hypothesis}}

The false negative rate (FNR) \cite{banerjee2009hypothesis}, which is an important metric in the field of statistical classification, measures the proportion of positive instances that a model incorrectly classifies as negative. This rate is critical in scenarios where the consequences of missing a positive detection are severe, such as in medical diagnosis or security surveillance systems \cite{sreenu2019intelligent, xue2020machine}. FNR is particularly valuable in contexts where ensuring the capture of all positive cases is paramount to prevent harmful outcomes. By quantifying the likelihood that true positives are overlooked by a model, the FNR provides essential insights into the sensitivity of a classification system. A high FNR indicates that a significant number of positive cases are being missed, potentially leading to hazardous oversights. Consequently, minimizing FNR is crucial in high-stakes environments where the cost of a false negative is high, such as failing to diagnose a serious illness or missing a security threat. It helps in tuning the thresholds of detection algorithms to better suit specific operational requirements, ensuring a more reliable performance in critical applications. Therefore, the FNR is integral to developing robust predictive models that effectively manage the risks associated with incorrect negative classifications.

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Fraction of false negatives and true positives. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Emphasizes identifying missed positive cases. \\
		\textcolor{Green}{$+$} & Important for recall-focused assessments. \\
		\textcolor{Red}{$-$}   & Ignores true negatives and false positives. \\
		\textcolor{Red}{$-$}   & May lead to overemphasis on recall.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{FNR} = \dfrac{\textit{FN}}{\textit{P}} = \dfrac{\textit{FN}}{\textit{FN} + \textit{TP}}
%
	\label{equation:FNR}
\end{equation}
\myequations{False negative rate (FNR) / miss rate}


\subsubsection{Macro average FNR (FNRmacro)}

\begin{equation}
	\textit{FNR}_\textit{macro} = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \textit{FNR}_i = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \dfrac{\textit{FN}_i}{\textit{FN}_i + \textit{TP}_i}
%
	\label{equation:MAAFNR}
\end{equation}
\myequations{Macro average FNR (FNR\textsubscript{macro})}


\subsubsection{Micro average FNR (FNRmicro)}

\begin{equation}
	\textit{FNR}_\textit{micro} = \dfrac{\sum\nolimits_{i = 1}^n \textit{FN}_i}{\sum\nolimits_{i = 1}^n (\textit{FN}_i + \textit{TP}_i)}
%
	\label{equation:MIAFNR}
\end{equation}
\myequations{Micro average FNR (FNR\textsubscript{micro})}


\subsubsection{Weighted average FNR (FNRweighted)}
\begin{equation}
	\textit{FNR}_\textit{weighted} = \dfrac{1}{\sum\nolimits_{i = 1}^n w_i} \cdot \sum\nolimits_{i = 1}^n w_i \cdot \textit{FNR}_i = \dfrac{1}{\sum\nolimits_{i = 1}^n w_i} \cdot \sum\nolimits_{i = 1}^n w_i \cdot \dfrac{\textit{FN}_i}{\textit{FN}_i + \textit{TP}_i}
%
	\label{equation:WAFNR}
\end{equation}
\myequations{Weighted average FNR (FNR\textsubscript{weighted})}


\subsection[Positive likelihood ratio (LR+)]{Positive likelihood ratio (LR$+$) \cite{swets1973relative, deeks2004diagnostic}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{In medicine, LR$+$ describes the probability of, e.g., a positive result in a sick person (true positive) in relation to the probability of a positive result in a healthy person (false positive). (range: $[0, \infty)$)} \\
		\textcolor{Green}{$+$} & Emphasizes the accuracy of the TPR in relation to the FPR. \\
		\textcolor{Red}{$-$}   & Neglects, e.g., TNR and FNR (see also diagnostic odds ratio).
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{LR}+ = \dfrac{\textit{Sensitivity}}{1 - \textit{Specificity}} = \dfrac{\textit{TPR}}{\textit{FPR}}
%
	\label{equation:LR+}
\end{equation}
\myequations{Positive likelihood ratio (LR$+$)}


\subsection[Negative likelihood ratio (LR-)]{Negative likelihood ratio (LR$-$) \cite{swets1973relative, deeks2004diagnostic}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{In medicine, LR$-$ describes the probability of, e.g., a negative result in a sick person (false negative) in relation to the probability of a negative result in a healthy person (true negative). (range: $[0, \infty)$)} \\
		\textcolor{Green}{$+$} & Emphasizes the accuracy of the FNR in relation to the TNR. \\
		\textcolor{Red}{$-$}   & Neglects, e.g., TPR and FPR (see also diagnostic odds ratio).
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{LR}- = \dfrac{1 - \textit{Sensitivity}}{\textit{Specificity}} = \dfrac{\textit{FNR}}{\textit{TNR}}
%
	\label{equation:LR-}
\end{equation}
\myequations{Negative likelihood ratio (LR$-$)}


\subsection[Diagnostic odds ratio (DOR)]{Diagnostic odds ratio (DOR) \cite{glas2003diagnostic, doust2004systematic}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{In medicine, DOR measures the classification effectiveness of a diagnostic test. \cite{glas2003diagnostic} (range: $[0, \infty)$)} \\
		\textcolor{Green}{$+$} & Balances sensitivity and specificity. \\
		\textcolor{Green}{$+$} & Independent of disease prevalence. \\
		\textcolor{Red}{$-$}   & Requires true positive, true negative, false positive, false negative counts. \\
		\textcolor{Red}{$-$}   & Increased calculation and interpretation complexity.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{DOR} = \dfrac{\textit{LR}+}{\textit{LR}-} = \dfrac{\textit{TP} \cdot \textit{TN}}{\textit{FP} \cdot \textit{FN}}
%
	\label{equation:DOR}
\end{equation}
\myequations{Diagnostic odds ratio (DOR)}


\subsection[Fowlkes--Mallows index (FM)]{Fowlkes--Mallows index (FM) \cite{fowlkes1983method, halkidi2001clustering}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Measures the geometric mean of the precision and the recall. In clustering, FM measures the similarity of two clusters. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Increased robustness to noise. \\
		\textcolor{Red}{$-$}   & Less well known and used.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{FM} = \sqrt{\textit{PPV} \cdot \textit{TPR}} = \sqrt{\textit{Precision} \cdot \textit{Recall}}
%
	\label{equation:FM}
\end{equation}
\myequations{Fowlkes--Mallows index (FM)}


\subsection[Informedness~/ bookmaker informedness (BM)~/ Youden's J statistic~/ Youden's index]{Informedness~/ bookmaker informedness (BM)~/ Youden's J statistic~/ Youden's index \cite{peirce1884numerical, youden1950index}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Quantifies the probability of an ``informed decision''. (range: $[-1, 1]$)} \\
		\textcolor{Green}{$+$} & Takes all predictions into account. \\
		\textcolor{Red}{$-$}   & Its result ranges from $-1$ to $1$.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{Informedness} = \textit{Sensitivity} + \textit{Specificity} - 1 = \textit{TPR} + \textit{TNR} - 1
%
	\label{equation:informedness}
\end{equation}
\myequations{Informedness / bookmaker informedness (BM) / Youden's J statistic / Youden's index}


\subsection[Markedness (MK)]{Markedness (MK) \cite{powers2020evaluation}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Quantifies the ``markedness'', i.e., the state of being irregular or uncommon. (range: $[-1, 1]$)} \\
		\textcolor{Green}{$+$} & Takes all predictions into account. \\
		\textcolor{Red}{$-$}   & Its result ranges from $-1$ to $1$.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{MK} = \textit{PPV} + \textit{NPV} - 1
%
	\label{equation:MK}
\end{equation}
\myequations{Markedness (MK)}


\subsection[Matthews correlation coefficient (MCC)~/ phi coefficient~/ Yule phi coefficient]{Matthews correlation coefficient (MCC)~/ phi coefficient~/ Yule phi coefficient \cite{yule1912methods, matthews1975comparison, cramer1999mathematical}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{A balanced measure of TPR, TNR, PPV, and NPV. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Takes all predictions into account. \\
		\textcolor{Red}{$-$}   & Increased calculation and interpretation complexity.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{MCC} = \sqrt{\textit{TPR} \cdot \textit{TNR} \cdot \textit{PPV} \cdot \textit{NPV}}
%
	\label{equation:MCC}
\end{equation}
\myequations{Matthews correlation coefficient (MCC) / phi coefficient / Yule phi coefficient}


\subsection[Jaccard index (JI)~/ threat score (TS)~/ critical success index (CSI)]{Jaccard index (JI)~/ threat score (TS)~/ critical success index (CSI) \cite{jaccard1912distribution, murphy1996finley}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Fraction of TP with TP, FN, and FP. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Emphasizes the accuracy of classifying TP in relation to FN and FP. \\
		\textcolor{Red}{$-$}   & Neglects TN.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{JI} = \dfrac{\textit{TP}}{\textit{TP} + \textit{FN} + \textit{FP}}
%
	\label{equation:JI}
\end{equation}
\myequations{Jaccard index (JI) / threat score (TS) / critical success index (CSI)}


\subsection[Receiver operating characteristic curve (ROC curve) and area under the curve (AUC)]{Receiver operating characteristic curve (ROC curve) and area under the curve (AUC) \cite{green1966signal, zweig1993receiver, fawcett2006introduction}}

The receiver operating characteristic curve (ROC curve) and its associated metric \cite{green1966signal, zweig1993receiver, fawcett2006introduction}, the area under the curve (AUC), are pivotal evaluation tools used in the analysis of classification models \cite{bradley1997use, mandrekar2010receiver, jimenez2012insights}. The ROC curve is a graphical representation that plots the true positive rate (sensitivity) against the false positive rate (specificity) at various threshold settings, providing a comprehensive view of a model's ability across a spectrum of conditions. The AUC, a scalar value derived from the area under the ROC curve, quantifies the overall ability of the model to discriminate between classes irrespective of any specific threshold. A higher AUC value indicates better model performance, with a value of 1.0 representing perfect discrimination and 0.5 denoting no discriminative ability (equivalent to random guessing). These metrics are particularly valuable in environments where distinguishing between classes is crucial and the costs of different types of classification errors vary significantly. By evaluating the trade-offs between sensitivity and specificity without committing to a specific classification threshold, ROC and AUC facilitate an objective comparison of model performance when robust and reliable predictions are of central importance.

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Calculates the area under the so-called receiver operating characteristic curve by adjusting the confidence threshold for, e.g., classification, detection, or segmentation. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Evaluates model performance across various decision thresholds. \\
		\textcolor{Green}{$+$} & Suitable for imbalanced data sets. \\
		\textcolor{Red}{$-$}   & Insensitive to class distribution changes. \\
		\textcolor{Red}{$-$}   & Does not directly address practical performance thresholds.
	\end{tabularx}
\end{table}

\begin{figure}[H]
	\centering

	\begin{tikzpicture}
		\begin{axis}[
		 width=0.5\textwidth, height=0.5\textwidth,
		 xlabel={False positive rate (FPR)}, ylabel={Recall / true positive rate (TPR) / sensitivity},
		 ticklabel style={/pgf/number format/.cd, fixed, fixed zerofill, precision=1},
		 legend columns=-1, legend style={at={(0.5, 1.25)}, anchor=north, column sep=1ex}]
			\addplot[domain=0:1, dashed] {x};
			\addplot[name path=f, domain=0:1, samples=512, smooth, LimeGreen] {1 - (x - 1)^2};
			\path[name path=axis] (0, 0) -- (1, 0);
			\addplot[LimeGreen!50] fill between[of=f and axis];
			\legend{$x$, Exemplary ROC curve, AUC}
		\end{axis}
	\end{tikzpicture}

	\caption{Receiver operating characteristic curve (ROC curve) and area under the curve (AUC).}
	\label{figure:ROC_AUC}
\end{figure}

\begin{equation}
	\textit{AUC} = \int_{x = 0}^1 \textit{TPR}(\textit{FPR}^{\,-1}(x)) \,dx = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \textit{TPR}(\textit{FPR}_i)
%
	\label{equation:AUC}
\end{equation}
\myequations{Area under the curve (AUC)}


\subsection[Average precision (AP)]{Average precision (AP) \cite{manning2009introduction, everingham2010pascal}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Calculates the area under the precision-recall curve by adjusting the confidence threshold for, e.g., classification, detection, or segmentation \cite{ozenne2015precision, sofaer2019area, cook2020consult}. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Evaluates model performance across various decision thresholds. \\
		\textcolor{Green}{$+$} & Suitable for imbalanced data sets. \\
		\textcolor{Red}{$-$}   & Insensitive to class distribution changes. \\
		\textcolor{Red}{$-$}   & Does not directly address practical performance thresholds.
	\end{tabularx}
\end{table}

\begin{figure}[H]
	\centering

	\begin{tikzpicture}
		\begin{axis}[
		 width=0.5\textwidth, height=0.5\textwidth,
		 xlabel={Recall / true positive rate (TPR) / sensitivity}, ylabel={Precision / positive predictive value (PPV)},
		 ticklabel style={/pgf/number format/.cd, fixed, fixed zerofill, precision=1},
		 legend columns=-1, legend style={at={(0.5, 1.25)}, anchor=north, column sep=1ex}]
			\addplot[domain=0:1, dashed] {-x + 1};
			\addplot[name path=f, domain=0:1, samples=512, smooth, LimeGreen] {1 - x^2};
			\path[name path=axis] (0, 0) -- (1, 0);
			\addplot[LimeGreen!50] fill between[of=f and axis];
			\legend{$-x + 1$, Exemplary precision-recall curve, AUC}
		\end{axis}
	\end{tikzpicture}

	\caption{Precision-recall curve and area under the curve (AUC).}
	\label{figure:PRC_AUC}
\end{figure}

\begin{equation}
	\textit{AP} = \int_{x = 0}^1 \textit{Precision}(\textit{Recall}^{\,-1}(x)) \,dx = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \textit{Precision}(\textit{Recall}_i)
%
	\label{equation:AP}
\end{equation}
\myequations{Average precision (AP)}


\subsection[Mean average precision (mAP)]{Mean average precision (mAP) \cite{manning2009introduction, everingham2010pascal}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Averages the area under the precision-recall curve over multiple classes. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Allows for an even more meaningful evaluation over multiple classes. \\
		\textcolor{Red}{$-$}   & Increased calculation complexity.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{mAP} = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \textit{AP}_i
%
	\label{equation:mAP}
\end{equation}
\myequations{Mean average precision (mAP)}


\subsection[Cohen's kappa for binary classification]{Cohen's kappa for binary classification \cite{cohen1960coefficient, ranganathan2017common, chicco2021matthews}}

Cohen's kappa is a metric for evaluating inter-rater reliability. It is applicable to both binary and categorical data by providing a quantitative measure. It is especially useful for the training of raters and adjustments in methodologies, particularly in complex settings with multiple raters. Yet, the main argument against Kappa centers on the problematic interpretations it can foster. Critics highlight that Kappa can be misleading due to its dependence on the prevalence of the attribute being measured and the possible bias of the raters. For instance, even when multiple different raters are in substantial agreement, kappa scores can be unexpectedly low if the prevalence of a certain categorization is particularly high or low. This has led to concerns that kappa may not always provide a robust or accurate measure of agreement. Hence, it could be misleading when forming a decisions based on its result. \cite{pontius2011death, olofsson2014good, foody2020explaining}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Ranges from $-1$ for a fully wrong prediction to $+1$ for a completely correct prediction. (range: $[-1, 1]$)} \\
		\textcolor{Green}{$+$} & Accounts for chance agreement. \\
		\textcolor{Green}{$+$} & Useful for inter-rater agreement assessment. \\
		\textcolor{Red}{$-$}   & Sensitivity to class imbalance. \\
		\textcolor{Red}{$-$}   & Interpretation complexity with multiple raters.
	\end{tabularx}
\end{table}

\begin{equation}
	\begin{aligned}
		\kappa &= \dfrac{P_o - P_e}{1 - P_e} \\
		P_e    &= \left(\dfrac{\textit{TP} + \textit{FP}}{T}\right) \cdot \left(\dfrac{\textit{TP} + \textit{FN}}{T}\right) + \left(\dfrac{\textit{TN} + \textit{FN}}{T}\right) \cdot \left(\dfrac{\textit{TN} + \textit{FP}}{T}\right)
%
		\label{equation:k}
	\end{aligned}
\end{equation}
%
\begin{conditions}
	\kappa & Cohen's kappa \\
	P_e    & expected accuracy \\
	P_o    & observed accuracy
\end{conditions}
\myequations{Cohen's kappa for binary classification}


\subsection[Gini impurity]{Gini impurity \cite{gini1912variabilita, breiman1984classification, manek2017aspect}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{The Gini impurity indicates the probability that two randomly selected samples from a data set have different original label types. A lower value indicates a higher purity of the data set. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Well-suited for application in decision tree algorithms. \\
		\textcolor{Red}{$-$}   & Favors binary decisions, which can result in decision trees of reduced complexity.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{Gini impurity}(D) = \sum\nolimits_{i = 1}^n \sum\nolimits_{i' \not= i} p_i p_{i'} = 1 - \sum\nolimits_{i = 1}^n p^2_i
%
	\label{equation:Gini_impurity}
\end{equation}
%
\begin{conditions}
	D & data set \\
	p & probability of samples
\end{conditions}
\myequations{Gini impurity}


\subsection[P4 metric]{P\textsubscript{4} metric \cite{sitarz2023extending}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{The P\textsubscript{4} metric covers four probabilities, precision, recall, specificity, and NPV, at once, forming their harmonic mean. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Tends to zero when any of the four conditional probabilities tends to zero. Tends to one when all four conditional probabilities tend to one. \\
		\textcolor{Green}{$+$} & It is symmetrical with respect to data set labels swapping. \\
		\textcolor{Red}{$-$}   & Does not take weighting. \\
		\textcolor{Red}{$-$}   & Currently barely used metric.
	\end{tabularx}
\end{table}

\begin{equation}
	P_4 = \dfrac{4}{\cfrac{1}{\textit{Precision}} + \cfrac{1}{\textit{Recall}} + \cfrac{1}{\textit{Specificity}} + \cfrac{1}{\textit{NPV}}} = \dfrac{4 \cdot \textit{TP} \cdot \textit{TN}}{4 \cdot \textit{TP} \cdot \textit{TN} + (\textit{TP} + \textit{TN}) \cdot (\textit{FP} + \textit{FN})}
%
	\label{equation:P4_metric}
\end{equation}
\myequations{P\textsubscript{4} metric}


\subsection[Skill score (SS)]{Skill score (SS) \cite{murphy1988skill}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Measures the quality of a prediction against a reference. (range: $(-\infty, 1]$)} \\
		\textcolor{Green}{$+$} & Intuitive way to rank model performance. \\
		\textcolor{Red}{$-$}   & Scales indefinitely into the negative for larger variations.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{SS} = 1 - \dfrac{\textit{Metric}_{\textit{GT}}}{\textit{Metric}_P}
%
	\label{equation:SS}
\end{equation}
%
\begin{conditions}
	\textit{Metric}_{\textit{GT}} & best possible expectation for a model based on a given metric \\
	\textit{Metric}_P             & actual prediction of a model based on a given metric
\end{conditions}
\myequations{Skill score (SS)}


\subsection[Relative improvement factor]{Relative improvement factor \cite{schlosser2022improving}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Measures the relative quality of a prediction against a reference. (range: $[0, \infty)$)} \\
		\textcolor{Green}{$+$} & Intuitive way to rank model performance. \\
		\textcolor{Red}{$-$}   & Scales indefinitely into the positive for larger variations.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{Relative improvement factor} = \dfrac{1 - \textit{Metric}_{\textit{GT}}}{1 - \textit{Metric}_P}
%
	\label{equation:relative_improvement_factor}
\end{equation}
%
\begin{conditions}
	\textit{Metric}_{\textit{GT}} & best possible expectation for a model based on a given metric \\
	\textit{Metric}_P             & actual prediction of a model based on a given metric
\end{conditions}
\myequations{Relative improvement factor}




\clearpage




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Examples %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Examples}

Figure~\ref{figure:ML_example1} shows an example result for a classification problem with 3 imbalanced classes, while Fig.~\ref{figure:ML_example2} illustrates an example result for a classification problem with 3 balanced classes created from the Iris flower data set of \textit{Fisher} \cite{fisher1936use}. Both examples show an exemplary confusion matrix on the top left together with the number of samples per class. The metrics recall and false negative rate (FNR) with their equations and example calculations are aligned to the rows, whereas precision and false discovery rate (FDR) and their respective equations and calculations are aligned with the columns of the confusion matrix. Accuracy as well as micro, macro, and weighted precision, recall, and F1-scores are shown on the bottom right. Note that micro and macro average recall, precision, and F1-score remain constant for the balanced data set. This is not the case for the imbalanced data set. Since the examples discuss 3-class problems, we set $n = 3$ in the equations for, e.g., macro average precision, recall, and F1-score (Eq.~\ref{equation:MAAP}, \ref{equation:MAAR}, and \ref{equation:MAAF}, respectively).


\begin{figure}[H]
	\centering

	\resizebox{\textwidth}{!}{\begin{tikzpicture}[r/.style={draw, rectangle, minimum size=1cm}]
		\begin{scope}
			\node[r, fill=RoyalBlue, text=white] at (-1,  1) {41};
			\node[r, fill=Apricot!50]            at ( 0,  1) {11};
			\node[r, fill=Apricot!50]            at ( 1,  1) {12};
			\node[r, fill=Apricot!50]            at (-1,  0) {62};
			\node[r, fill=RoyalBlue, text=white] at ( 0,  0) {22};
			\node[r, fill=Apricot!50]            at ( 1,  0) {23};
			\node[r, fill=Apricot!50]            at (-1, -1) {31};
			\node[r]                             at ( 0, -1) {0};
			\node[r, fill=RoyalBlue, text=white] at ( 1, -1) {52};

			\node            at ( 0, 2) {Predicted class};
			\node[rotate=90] at (-2, 0) {Ground truth class};

			\node[rotate=60] at (-1, 3) {1: \textit{cat}};
			\node[rotate=60] at ( 0, 3) {2: \textit{fox}};
			\node[rotate=60] at ( 1, 3) {3: \textit{dog}};

			\node at (-3,  1) {1: \textit{cat}};
			\node at (-3,  0) {2: \textit{fox}};
			\node at (-3, -1) {3: \textit{dog}};
		\end{scope}

		\begin{scope}[xshift=3.5cm, inner sep=0, font=\footnotesize]
			\node[r] at (-1,    1) {64};
			\node[r] at ( 0.5,  1) {64.1\%};
			\node[r] at ( 1.6,  1) {35.9\%};
			\node[r] at (-1,    0) {107};
			\node[r] at ( 0.5,  0) {20.6\%};
			\node[r] at ( 1.6,  0) {79.4\%};
			\node[r] at (-1,   -1) {83};
			\node[r] at ( 0.5, -1) {62.7\%};
			\node[r] at ( 1.6, -1) {37.4\%};

			\node at (-1,   2) {$\sum$};
			\node at ( 0.5, 2) {$\textit{Recall}_i$};
			\node at ( 1.6, 2) {$\textit{FNR}_i$};

			\node at (6, 0.5) {
				\renewcommand*{\arraystretch}{3}\begin{tabular}{cccc}
					{Eq.~\{\ref{equation:recall}\}} & $\dfrac{\textit{TP}}{\textit{TP} + \textit{FN}}$ & {Eq.~\{\ref{equation:FNR}\}} & $\dfrac{\textit{FN}}{\textit{FN} + \textit{TP}}$ \\
					\hline
					$\textit{Recall}_1$ & $\dfrac{41}{41 + 11 + 12}$ & $\textit{FNR}_1$ & $\dfrac{11 + 12}{11 + 12 + 41}$ \\
					\hline
					$\textit{Recall}_2$ & $\dfrac{22}{22 + 62 + 23}$ & $\textit{FNR}_2$ & $\dfrac{62 + 23}{62 + 23 + 22}$ \\
					\hline
					$\textit{Recall}_3$ & $\dfrac{52}{52 + 31 + 0}$ & $\textit{FNR}_3$ & $\dfrac{31 + 0}{31 + 0 + 52}$ \\
					\hline
				\end{tabular}};
		\end{scope}

		\begin{scope}[yshift=-3.5cm, inner sep=0, font=\footnotesize]
			\node[r] at (-1,  1)   {134};
			\node[r] at ( 0,  1)   {33};
			\node[r] at ( 1,  1)   {87};
			\node[r] at (-1, -0.5) {30.6\%};
			\node[r] at ( 0, -0.5) {66.7\%};
			\node[r] at ( 1, -0.5) {59.8\%};
			\node[r] at (-1, -1.6) {69.4\%};
			\node[r] at ( 0, -1.6) {33.3\%};
			\node[r] at ( 1, -1.6) {40.2\%};

			\node[r] at (2.5, 1) {254};

			\node at (-2.5,  1)   {$\sum$};
			\node at (-2.5, -0.5) {$\textit{Precision}_i$};
			\node at (-2.5, -1.6) {$\textit{FDR}_i$};

			\node at (-0.5, -5) {
				\renewcommand*{\arraystretch}{3}\begin{tabular}{c|c|c|c|}
					Eq.~\ref{equation:precision} & $\textit{Precision}_1$ & $\textit{Precision}_2$ & $\textit{Precision}_3$ \\
					$\dfrac{\textit{TP}}{\textit{TP} + \textit{FP}}$ & $\dfrac{41}{41 + 62 + 31}$ & $\dfrac{22}{22 + 11 + 0}$ & $\dfrac{52}{52 + 12 + 23}$ \\
					Eq.~\ref{equation:FDR} & $\textit{FDR}_1$ & $\textit{FDR}_2$ & $\textit{FDR}_3$ \\
					$\dfrac{\textit{FP}}{\textit{FP} + \textit{TP}}$ & $\dfrac{62 + 31}{62 + 31 + 41}$ & $\dfrac{11 + 0}{11 + 0 + 22}$ & $\dfrac{12 + 23}{12 + 23 + 52}$ \\
				\end{tabular}};
		\end{scope}

		\begin{scope}[shift={(9, -5)}, inner sep=0, font=\footnotesize]
			\node at (0, 0) {$\textit{Accuracy} = \dfrac{\textit{TP} + \textit{TN}}{\textit{TP} + \textit{TN} + \textit{FP} + \textit{FN}} = \dfrac{41 + 22 + 52}{41 + 22 + 52 + 11 + 12 + 62 + 23 + 31 + 0} = 45.28\%$};

			\node at (0, -3) {
				\begin{tabular}{|P{2cm}|P{1.75cm}|P{1.75cm}|P{1.75cm}|P{0.5cm}|}
					\hline
					Class & \textit{Precision} [\%] & \textit{Recall} [\%] & $F_1$ [\%] & $\sum$ \\
					\hline
					\noalign{\vskip 2pt}

					\hline
					1 & 30.60 & 64.06 & 41.41 & 64 \\
					\hline
					2 & 66.67 & 20.56 & 31.43 & 107 \\
					\hline
					3 & 59.77 & 62.65 & 61.18 & 83 \\
					\hline
					\noalign{\vskip 2pt}

					\cline{1-4}
					Macro average & 52.34 & 49.09 & 44.67 & \multicolumn{1}{c}{} \\
					\cline{1-4}
					Micro average & 45.28 & 45.28 & 45.28 & \multicolumn{1}{c}{} \\
					\cline{1-4}
					W. average    & 55.32 & 45.28 & 43.67 & \multicolumn{1}{c}{} \\
					\cline{1-4}
					\noalign{\vskip 2pt}

					\cline{2-4}
					\multicolumn{1}{c}{} & \multicolumn{1}{|c|}{Eq.~\{\ref{equation:precision},\ref{equation:MAAP}--\ref{equation:WAP}\}} & \multicolumn{1}{c|}{Eq.~\{\ref{equation:recall},\ref{equation:MAAR}--\ref{equation:WAR}\}} & \multicolumn{1}{c|}{Eq.~\{\ref{equation:F1-score},\ref{equation:MAAF}--\ref{equation:WAF}\}} & \multicolumn{1}{c}{} \\
					\cline{2-4}
				\end{tabular}};
		\end{scope}
	\end{tikzpicture}}

	\caption{Classification result example (1) showing a 3-class problem with imbalanced classes.}
	\label{figure:ML_example1}
\end{figure}


\begin{figure}[H]
	\centering

	\resizebox{\textwidth}{!}{\begin{tikzpicture}[r/.style={draw, rectangle, minimum size=1cm}]
		\begin{scope}
			\node[r, fill=RoyalBlue, text=white] at (-1,  1) {50};
			\node[r]                             at ( 0,  1) {0};
			\node[r]                             at ( 1,  1) {0};
			\node[r]                             at (-1,  0) {0};
			\node[r, fill=RoyalBlue, text=white] at ( 0,  0) {47};
			\node[r, fill=Apricot!50]            at ( 1,  0) {3};
			\node[r]                             at (-1, -1) {0};
			\node[r, fill=Apricot!50]            at ( 0, -1) {2};
			\node[r, fill=RoyalBlue, text=white] at ( 1, -1) {48};

			\node            at ( 0, 2) {Predicted class};
			\node[rotate=90] at (-2, 0) {Ground truth class};

			\node[rotate=60] at (-1, 3.5) {1: \textit{setosa}};
			\node[rotate=60] at ( 0, 3.5) {2: \textit{versicolor}};
			\node[rotate=60] at ( 1, 3.5) {3: \textit{virginica}};

			\node at (-3.5,  1) {1: \textit{setosa}};
			\node at (-3.5,  0) {2: \textit{versicolor}};
			\node at (-3.5, -1) {3: \textit{virginica}};
		\end{scope}

		\begin{scope}[xshift=3.5cm, inner sep=0, font=\footnotesize]
			\node[r] at (-1,    1) {50};
			\node[r] at ( 0.5,  1) {100\%};
			\node[r] at ( 1.6,  1) {0\%};
			\node[r] at (-1,    0) {50};
			\node[r] at ( 0.5,  0) {94\%};
			\node[r] at ( 1.6,  0) {6\%};
			\node[r] at (-1,   -1) {50};
			\node[r] at ( 0.5, -1) {96\%};
			\node[r] at ( 1.6, -1) {4\%};

			\node at (-1,   2) {$\sum$};
			\node at ( 0.5, 2) {$\textit{Recall}_i$ };
			\node at ( 1.6, 2) {$\textit{FNR}_i$};

			\node at (6, 0.5) {
				\renewcommand*{\arraystretch}{3}\begin{tabular}{cccc}
					Eq.~\ref{equation:recall} & $\dfrac{\textit{TP}}{\textit{TP} + \textit{FN}}$ & Eq.~\ref{equation:FNR} & $\dfrac{\textit{FN}}{\textit{FN} + \textit{TP}}$ \\
					\hline
					$\textit{Recall}_1$ & $\dfrac{50}{50 + 0 + 0}$ & $\textit{FNR}_1$ & $\dfrac{0 + 0}{0 + 0 + 50}$ \\
					\hline
					$\textit{Recall}_2$ & $\dfrac{47}{47 + 0 + 3}$ & $\textit{FNR}_2$ & $\dfrac{0 + 3}{0 + 3 + 47}$ \\
					\hline
					$\textit{Recall}_3$ & $\dfrac{48}{48 + 0 + 2}$ & $\textit{FNR}_3$ & $\dfrac{0 + 2}{0 + 2 + 48}$ \\
					\hline
				\end{tabular}};
		\end{scope}

		\begin{scope}[yshift=-3.5cm, inner sep=0, font=\footnotesize]
			\node[r] at (-1,  1)   {50};
			\node[r] at ( 0,  1)   {49};
			\node[r] at ( 1,  1)   {51};
			\node[r] at (-1, -0.5) {100\%};
			\node[r] at ( 0, -0.5) {95.9\%};
			\node[r] at ( 1, -0.5) {94.1\%};
			\node[r] at (-1, -1.6) {0\%};
			\node[r] at ( 0, -1.6) {4.1\%};
			\node[r] at ( 1, -1.6) {5.9\%};

			\node[r] at (2.5, 1) {150};

			\node at (-2.5,  1)   {$\sum$};
			\node at (-2.5, -0.5) {$\textit{Precision}_i$};
			\node at (-2.5, -1.6) {$\textit{FDR}_i$};

			\node at (-0.5, -5) {
				\renewcommand*{\arraystretch}{3}\begin{tabular}{c|c|c|c|}
					Eq.~\ref{equation:precision} & $\textit{Precision}_1$ & $\textit{Precision}_2$ & $\textit{Precision}_3$ \\
					$\dfrac{\textit{TP}}{\textit{TP} + \textit{FP}}$ & $\dfrac{50}{50 + 0 + 0}$ & $\dfrac{47}{47 + 0 + 2}$ & $\dfrac{48}{48 + 0 + 3}$ \\
					Eq.~\ref{equation:FDR} & $\textit{FDR}_1$ & $\textit{FDR}_2$ & $\textit{FDR}_3$ \\
					$\dfrac{\textit{FP}}{\textit{FP} + \textit{TP}}$ & $\dfrac{0 + 0}{0 + 0 + 50}$ & $\dfrac{0 + 2}{0 + 2 + 47}$ & $\dfrac{0 + 3}{0 + 3 + 48}$ \\
				\end{tabular}};
		\end{scope}

		\begin{scope}[shift={(9, -5)}, inner sep=0, font=\footnotesize]
			\node at (0, 0) {$\textit{Accuracy} = \dfrac{\textit{TP} + \textit{TN}}{\textit{TP} + \textit{TN} + \textit{FP} + \textit{FN}} = \dfrac{50 + 47 + 48}{50 + 47 + 48 + 0 + 0 + 0 + 3 + 0 + 2} = 96.7\%$};

			\node at (0, -3) {
				\begin{tabular}{|P{2cm}|P{1.75cm}|P{1.75cm}|P{1.75cm}|P{0.5cm}|}
					\hline
					Class & \textit{Precision} [\%] & \textit{Recall} [\%] & $F_1$ [\%] & $\sum$ \\
					\hline
					\noalign{\vskip 2pt}

					\hline
					1 & 100.00 & 100.00 & 100.00 & 50 \\
					\hline
					2 &  95.92 &  94.00 &  94.95 & 50 \\
					\hline
					3 &  94.12 &  96.00 &  95.05 & 50 \\
					\hline
					\noalign{\vskip 2pt}

					\cline{1-4}
					Macro average & 96.68 & 96.67 & 96.67 & \multicolumn{1}{c}{} \\
					\cline{1-4}
					Micro average & 96.67 & 96.67 & 96.67 & \multicolumn{1}{c}{} \\
					\cline{1-4}
					W. average    & 96.68 & 96.67 & 96.67 & \multicolumn{1}{c}{} \\
					\cline{1-4}
					\noalign{\vskip 2pt}

					\cline{2-4}
					\multicolumn{1}{c}{} & \multicolumn{1}{|c|}{Eq.~\{\ref{equation:precision},\ref{equation:MAAP}--\ref{equation:WAP}\}} & \multicolumn{1}{c|}{Eq.~\{\ref{equation:recall},\ref{equation:MAAR}--\ref{equation:WAR}\}} & \multicolumn{1}{c|}{Eq.~\{\ref{equation:F1-score},\ref{equation:MAAF}--\ref{equation:WAF}\}} & \multicolumn{1}{c}{} \\
					\cline{2-4}
				\end{tabular}};
		\end{scope}
	\end{tikzpicture}}

	\caption{Classification result example (2) showing a 3-class problem with balanced classes.}
	\label{figure:ML_example2}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Examples %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\clearpage




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Functions %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Functions}

The following table gives an overview of machine learning metrics commonly used with the Python programming language machine learning libraries scikit-learn\footnote{scikit-learn project page, \url{https://scikit-learn.org/stable/}}, TensorFlow\footnote{TensorFlow project page, \url{https://www.tensorflow.org/}} (and Keras\footnote{Keras project page, \url{https://keras.io/}}), and PyTorch\footnote{PyTorch project page, \url{https://pytorch.org/}}\textsuperscript{,}\footnote{TorchMetrics project page, \url{https://torchmetrics.readthedocs.io/en/stable/}}.


\begin{table}[H]
	\centering

	\resizebox{\textwidth}{!}{\begin{tabular}{|l|c|c|c|}
		\hline
		Equation & scikit-learn & TensorFlow & PyTorch \\
		\hline
		%
		\hline

		True Positives
		&
		/
		&
		\href{https://www.tensorflow.org/api_docs/python/tf/keras/metrics/TruePositives}{TruePositives}
		&
		/
		\\

		True Negatives
		&
		/
		&
		\href{https://www.tensorflow.org/api_docs/python/tf/keras/metrics/TrueNegatives}{TrueNegatives}
		&
		/
		\\

		False Positives
		&
		/
		&
		\href{https://www.tensorflow.org/api_docs/python/tf/keras/metrics/FalsePositives}{FalsePositives}
		&
		/
		\\

		False Negatives
		&
		/
		&
		\href{https://www.tensorflow.org/api_docs/python/tf/keras/metrics/FalseNegatives}{FalseNegatives}
		&
		/
		\\

		\hline
		%
		\hline

		Precision (\ref{equation:precision})
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html\#sklearn-metrics-precision-score}{precision\_score}
		&
		\href{https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Precision}{Precision}
		&
		\href{https://torchmetrics.readthedocs.io/en/latest/classification/precision.html}{Precision}
		\\

		Recall (\ref{equation:recall})
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html\#sklearn.metrics.recall_score}{recall\_score}
		&
		\href{https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Recall}{Recall}
		&
		\href{https://torchmetrics.readthedocs.io/en/latest/classification/recall.html}{Recall}
		\\

		True negative rate (TNR) (\ref{equation:TNR})
		&
		/
		&
		/
		&
		\href{https://torchmetrics.readthedocs.io/en/latest/classification/specificity.html}{Specificity}
		\\

		Macro average precision (AP\textsubscript{macro}) (\ref{equation:MAAP})
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html\#sklearn.metrics.average_precision_score}{average\_precision\_score}
		&
		/
		&
		\href{https://torchmetrics.readthedocs.io/en/latest/classification/average_precision.html}{AveragePrecision}
		\\

		Micro average precision (AP\textsubscript{micro}) (\ref{equation:MIAP})
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html\#sklearn.metrics.average_precision_score}{average\_precision\_score}
		&
		/
		&
		\href{https://torchmetrics.readthedocs.io/en/latest/classification/average_precision.html}{AveragePrecision}
		\\

		Weighted average precision (AP\textsubscript{weighted}) (\ref{equation:WAP})
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html\#sklearn.metrics.average_precision_score}{average\_precision\_score}
		&
		/
		&
		\href{https://torchmetrics.readthedocs.io/en/latest/classification/average_precision.html}{AveragePrecision}
		\\

		Accuracy (A) (\ref{equation:A})
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\#sklearn.metrics.accuracy_score}{accuracy\_score}
		&
		\href{https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Accuracy}{Accuracy}
		&
		\href{https://torchmetrics.readthedocs.io/en/latest/classification/accuracy.html}{Accuracy}
		\\

		Balanced accuracy (BA) (\ref{equation:BA_binary},\ref{equation:BA_multi})
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html\#sklearn.metrics.balanced_accuracy_score}{balanced\_accuracy\_score}
		&
		/
		&
		/
		\\

		F-score (\ref{equation:F-score})
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html\#sklearn.metrics.fbeta_score}{fbeta\_score}
		&
		\href{https://www.tensorflow.org/api_docs/python/tf/keras/metrics/FBetaScore}{FBetaScore}
		&
		\href{https://torchmetrics.readthedocs.io/en/latest/classification/fbeta_score.html}{FBetaScore}
		\\

		F1-score (\ref{equation:F1-score})
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\#sklearn.metrics.f1_score}{f1\_score}
		&
		\href{https://www.tensorflow.org/api_docs/python/tf/keras/metrics/F1Score}{F1Score}
		&
		\href{https://torchmetrics.readthedocs.io/en/latest/classification/f1_score.html}{F1Score}
		\\

		Positive likelihood ratio (LR$+$) (\ref{equation:LR+})
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.class_likelihood_ratios.html\#sklearn.metrics.class_likelihood_ratios}{class\_likelihood\_ratios}
		&
		/
		&
		/
		\\

		Negative likelihood ratio (LR$-$) (\ref{equation:LR-})
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.class_likelihood_ratios.html\#sklearn.metrics.class_likelihood_ratios}{class\_likelihood\_ratios}
		&
		/
		&
		/
		\\

		Fowlkes--Mallows index (FM) (\ref{equation:FM})
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fowlkes_mallows_score.html\#sklearn.metrics.fowlkes_mallows_score}{fowlkes\_mallows\_score}
		&
		/
		&
		/
		\\

		Matthews correlation coefficient (MCC) (\ref{equation:MCC})
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html\#sklearn.metrics.matthews_corrcoef}{matthews\_corrcoef}
		&
		/
		&
		\href{https://torchmetrics.readthedocs.io/en/latest/classification/matthews_corr_coef.html}{MatthewsCorrCoef}
		\\

		Jaccard index (JI) (\ref{equation:JI})
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html\#sklearn.metrics.jaccard_score}{jaccard\_score}
		&
		/
		&
		\href{https://torchmetrics.readthedocs.io/en/latest/classification/jaccard_index.html}{JaccardIndex}
		\\

		Receiver operating characteristic curve (ROC curve)
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html\#sklearn.metrics.roc_curve}{roc\_curve}
		&
		/
		&
		\href{https://torchmetrics.readthedocs.io/en/latest/classification/roc.html}{ROC}
		\\

		Area under the curve (AUC) (\ref{equation:AUC})
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html\#sklearn.metrics.auc}{auc}
		&
		\href{https://www.tensorflow.org/api_docs/python/tf/keras/metrics/AUC}{AUC}
		&
		\href{https://torchmetrics.readthedocs.io/en/latest/classification/auroc.html}{AUROC}
		\\

		Average precision (AP) (\ref{equation:AP})
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html\#sklearn.metrics.average_precision_score}{average\_precision\_score}
		&
		/
		&
		\href{https://torchmetrics.readthedocs.io/en/latest/classification/average_precision.html}{AveragePrecision}
		\\

		Cohen's kappa (\ref{equation:k})
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html\#sklearn.metrics.cohen_kappa_score}{cohen\_kappa\_score}
		&
		/
		&
		\href{https://torchmetrics.readthedocs.io/en/latest/classification/cohen_kappa.html}{CohenKappa}
		\\

		\hline
	\end{tabular}}

	\caption{Selection of function calls for the available metrics in scikit-learn, TensorFlow, and PyTorch. The call for the respective metrics follows the corresponding scheme: scikit-learn~-- sklearn.metrics.<metric>, TensorFlow~-- tf.keras.metrics.<metric>, and PyTorch~-- torchmetrics.<metric>.}
	\label{table:ML_functions}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Functions %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\clearpage




\section{Computer Vision}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In the realm of computer vision, the evaluation of algorithms through various error and similarity metrics is essential for ensuring the reliability and accuracy of models applied in areas such as image processing, object recognition, and scene understanding. These metrics serve as fundamental tools for quantifying the performance of vision systems across a range of real-world tasks.

The core of these evaluations begins with the concept of error measurement. The error, abbreviated as E, provides a basic indication of the deviation of predicted values from the ground truth. Based on E, the absolute error (AE) \cite{richardson2004h} quantifies the absolute difference between each predicted and actual value. By aggregating these errors, a sense of overall error magnitude without accounting for the direction of errors can be obtained. This metric is particularly useful in applications where errors of different orientations, or signs, are equally detrimental. Complementing AE, the relative absolute error (RAE) \cite{armstrong1992error, armstrong2000another, rodrigues2017machine} normalizes the absolute error by the magnitude of the true value, providing a scale-independent measure. The mean error (ME) \cite{fisher1920012, anjali2019temperature} and the mean absolute error (MAE) \cite{willmott2005advantages, hyndman2006another} extend these concepts by averaging errors across all predictions, with MAE providing a robust measure against outliers by using the absolute values of errors. For a percentage-based perspective, the mean percentage error (MPE) \cite{pearson1895x, jiang2008prediction} and the mean absolute percentage error (MAPE) \cite{armstrong1992error, hyndman2006another} express errors as a percentage of the actual values, which is valuable for comparing performance across data sets of varying scales. The mean absolute scaled error (MASE) \cite{hyndman2006another, mohan2018deep} offers an advanced normalization by scaling MAE against the in-sample MAE. It is especially suitable for time-series predictions in vision tasks involving motion or tracking.

Expanding on squared errors, the squared error (SE) \cite{draper1998applied} and the mean squared error (MSE) \cite{bickel2015mathematical} provide a measure where larger errors are exponentially penalized, making these metrics sensitive to outliers but valuable in applications where large errors are particularly undesirable. Following MSE, the root mean square error (RMSE) \cite{willmott2005advantages, hyndman2006another, pontius2008components} brings these scales back to the original units of measurement, while the normalized root mean square error (NRMSE) \cite{chang2004air, kim2005missing} offers a relative measure by normalizing RMSE against the range of observed data, potentially enhancing comparability across different scales and data sets. The root mean squared logarithmic error (RMSLE) \cite{nafees2021predictive} addresses scenarios where proportional differences are more significant than absolute differences. Therefore, it is more frequently used in depth and logarithmic scale predictions. In terms of assessing the quality of visual outputs, the peak signal-to-noise ratio (PSNR) \cite{salomon2004data, huynh2008scope} is pivotal in image processing, especially in lossy compression, by comparing the level of desired signals to the background noise. Likewise, the structural similarity (SSIM) \cite{wang2004image, ghodrati2019mr} and its counterpart, the structural dissimilarity (DSSIM) \cite{wang2004image, ghodrati2019mr}, evaluate the visual impact of changes to, e.g., luminance, contrast, and structure, providing a comprehensive measure of image quality degradation due to compression or other distortions.

In tasks including image segmentation, metrics such as the intersection over union (IoU) \cite{jaccard1912distribution, murphy1996finley, rezatofighi2019generalized, zou2023object}, the Dice coefficient (DC) \cite{dice1945measures, sorenson1948method}, and the overlap coefficient (OC) \cite{szymkiewicz1934conlribution, sempson1947holarctic, bell1962mutual, goodall1978sample} are indispensable. They assess the overlap between predicted and ground truth segments, which is crucial for evaluating the accuracy of boundary detection algorithms within domains such as medical imaging \cite{suzuki2017overview, esteva2021deep}, video surveillance \cite{buch2011review, brunetti2018computer}, and autonomous driving systems \cite{feng2021review, zablocki2022explainability}.

Together, these metrics provide a robust framework for diagnosing and enhancing the performance of computer vision systems, ensuring their efficiency and reliability in both controlled and erratic environments.


The following evaluation metrics within the context of machine learning are motivated by the contributions of \textit{Wang et al.} \cite{wang2003multiscale} and \textit{Hore and Ziou} \cite{hore2010image}.


\subsection{General}

Table~\ref{table:CV_general} gives an overview of our general definitions for CV-related metrics.

\begin{table}[H]
	\centering

	\begin{tabular}{|c|c|}
		\hline
		Abbreviation & Meaning \\
		\hline
		%
		\hline
		$\textit{GT}$ & Ground truth     \\
		$P$           & Prediction       \\
		$n$           & Number of values \\
		\hline
	\end{tabular}

	\caption{General definitions computer vision.}
	\label{table:CV_general}
\end{table}


\subsection{Error (E)}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{The amount by which a prediction differs from the ground truth. (range: $(-\infty, \infty)$)} \\
		\textcolor{Green}{$+$} & Intuitive and straightforward to apply and interpret. \\
		\textcolor{Red}{$-$}   & Does not distinguish between types of errors.
	\end{tabularx}
\end{table}

\begin{equation}
	E = \textit{GT} - P
%
	\label{equation:E}
\end{equation}
\myequations{Error (E)}


\subsection[Absolute error~/ sum of absolute errors (AE)]{Absolute error~/ sum of absolute errors (AE) \cite{richardson2004h}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Calculates the sum (total) of all absolute errors. (range: $[0, \infty)$)} \\
		\textcolor{Green}{$+$} & Intuitive and straightforward to apply and interpret. \\
		\textcolor{Green}{$+$} & Accounts for absolute magnitude of errors. \\
		\textcolor{Red}{$-$}   & No differentiation depending on the number of compared values is made. \\
		\textcolor{Red}{$-$}   & Large individual differences equal to many small ones (distribution problem).
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{AE} = \sum\nolimits_{i = 1}^n |E_i| = \sum\nolimits_{i = 1}^n |\textit{GT}_i - P_i|
%
	\label{equation:AE}
\end{equation}
\myequations{Absolute error / sum of absolute errors (AE)}


\subsection[Relative absolute error (RAE)]{Relative absolute error (RAE) \cite{armstrong1992error, armstrong2000another, rodrigues2017machine}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Normalization of the absolute error by dividing the total absolute error of the simple predictor. (range: $[0, \infty)$)} \\
		\textcolor{Green}{$+$} & Comparison of models that differ significantly. \\
		\textcolor{Red}{$-$}   & Not sensitive to outliers and scaling.
	\end{tabularx}
\end{table}

\begin{equation}
	\begin{aligned}
		\textit{RAE}           &= \dfrac{\textit{AE}}{\sum\nolimits_{i = 1}^n |\textit{GT}_i - \overline{\textit{GT}}|} = \dfrac{\sum\nolimits_{i = 1}^n |\textit{GT}_i - P_i|}{\sum\nolimits_{i = 1}^n |\textit{GT}_i - \overline{\textit{GT}}|} \\
		\overline{\textit{GT}} &= \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \textit{GT}_i
%
		\label{equation:RAE}
	\end{aligned}
\end{equation}
%
\begin{conditions}
	\overline{\textit{GT}} & average of the ground truth
\end{conditions}
\myequations{Relative absolute error (RAE)}


\subsection[Mean error (ME)]{Mean error (ME) \cite{fisher1920012, anjali2019temperature}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{The average over all error measurements. (range: $(-\infty, \infty)$)} \\
		\textcolor{Green}{$+$} & Intuitive and straightforward to apply. \\
		\textcolor{Red}{$-$}   & Positive and negative error values can cancel each other out.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{ME} = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n E_i = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n (\textit{GT}_i - P_i)
%
	\label{equation:ME}
\end{equation}
\myequations{Mean error (ME)}


\subsection[Mean percentage error (MPE)]{Mean percentage error (MPE) \cite{pearson1895x, jiang2008prediction}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{The average over all error measurements in percentage. (range: $(-\infty\%, \infty\%)$)} \\
		\textcolor{Green}{$+$} & Intuitive overview of the underlying situation. \\
		\textcolor{Red}{$-$}   & Undefined as soon as a single ground truth value is zero.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{MPE} = \dfrac{100}{n} \cdot \sum\nolimits_{i = 1}^n \dfrac{E_i}{\textit{GT}_i} = \dfrac{100}{n} \cdot \sum\nolimits_{i = 1}^n \dfrac{\textit{GT}_i - P_i}{\textit{GT}_i}
%
	\label{equation:MPE}
\end{equation}
\myequations{Mean percentage error (MPE)}


\subsection[Mean absolute error (MAE)]{Mean absolute error (MAE) \cite{willmott2005advantages, hyndman2006another}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Calculates the mean of the sum (total) of all absolute errors. (range: $[0, \infty)$)} \\
		\textcolor{Green}{$+$} & Partially solves the distribution problem. \\
		\textcolor{Red}{$-$}   & No differentiation depending on the maximum assumable error is made.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{MAE} = \dfrac{\textit{AE}}{n} = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n |\textit{GT}_i - P_i|
%
	\label{equation:MAE}
\end{equation}
\myequations{Mean absolute error (MAE)}


\subsection[Mean absolute percentage error (MAPE)]{Mean absolute percentage error (MAPE) \cite{armstrong1992error, hyndman2006another}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{The average over all absolute error measurements in percentage. (range: $[0\%, \infty\%)$)} \\
		\textcolor{Green}{$+$} & Intuitive and scale independent. \\
		\textcolor{Red}{$-$}   & Undefined as soon as a single ground truth value is zero.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{MPE} = \dfrac{100}{n} \cdot \sum\nolimits_{i = 1}^n \dfrac{|E_i|}{|\textit{GT}_i|} = \dfrac{100}{n} \cdot \sum\nolimits_{i = 1}^n \dfrac{|\textit{GT}_i - P_i|}{|\textit{GT}_i|}
%
	\label{equation:MAPE}
\end{equation}
\myequations{Mean absolute percentage error (MAPE)}


\subsection[Mean absolute scaled error (MASE)]{Mean absolute scaled error (MASE) \cite{hyndman2006another, mohan2018deep}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Mean absolute error of the measurements scaled by the mean absolute error of the ground truth. (range: $[0, \infty)$)} \\
		\textcolor{Green}{$+$} & Scale invariant. \\
		\textcolor{Red}{$-$}   & Less sensitive to outliers.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{MASE} = \dfrac{\textit{MPE}}{\left(\dfrac{1}{n} - 1\right) \cdot \sum\nolimits_{i = 2}^n |\textit{GT}_i - \textit{GT}_{i - 1}|} = \dfrac{\dfrac{100}{n} \cdot \sum\nolimits_{i = 1}^n \dfrac{\textit{GT}_i - P_i}{\textit{GT}_i}}{\left(\dfrac{1}{n} - 1\right) \cdot \sum\nolimits_{i = 2}^n |\textit{GT}_i - \textit{GT}_{i - 1}|}
%
	\label{equation:MASE}
\end{equation}
\myequations{Mean absolute scaled error (MASE)}


\subsection[Mean normalized bias (MNB)]{Mean normalized bias (MNB) \cite{yu2006new, tsigaridis2014aerocom}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Calculates the variance between the predicted values and the ground truth values. Divides by the reference variable, subsequently calculating the mean. (range: $(-\infty, \infty)$)} \\
		\textcolor{Green}{$+$} & Enables the specific evaluation of systematic errors across the entire model. \\
		\textcolor{Red}{$-$}   & Does not detect specific errors in individual parts of the model.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{MNB} = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \dfrac{E_i}{P_i} = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \dfrac{\textit{GT}_i - P_i}{P_i}
%
	\label{equation:MNB}
\end{equation}
\myequations{Mean normalized bias (MNB)}


\subsection[Normalized mean bias (NMB)]{Normalized mean bias (NMB) \cite{mebust2003models, yu2006new}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Calculates the average of the variances between the prediction and the reference variable, subsequently normalizing it by the reference variable. (range: $(-\infty, \infty)$)} \\
		\textcolor{Green}{$+$} & Comparison of models independently of scaling. \\
		\textcolor{Red}{$-$}   & Sensitive to outliers.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{NMB} = \dfrac{\sum\nolimits_{i = 1}^n E_i}{\sum\nolimits_{i = 1}^n P_i} = \dfrac{\sum\nolimits_{i = 1}^n (\textit{GT}_i - P_i)}{\sum\nolimits_{i = 1}^n P_i}
%
	\label{equation:NMB}
\end{equation}
\myequations{Normalized mean bias (NMB)}


\subsection[Squared error~/ sum of squared errors (SE)]{Squared error~/ sum of squared errors (SE) \cite{draper1998applied}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Calculates the sum (total) of all squared errors. (range: $[0, \infty)$)} \\
		\textcolor{Green}{$+$} & Emphasizes the contribution of large errors. \\
		\textcolor{Red}{$-$}   & No differentiation depending on the number of compared values is made.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{SE} = \sum\nolimits_{i = 1}^n E_i^2 = \sum\nolimits_{i = 1}^n (\textit{GT}_i - P_i)^2
%
	\label{equation:SE}
\end{equation}
\myequations{Squared error / sum of squared errors (SE)}


\subsection[Mean square error (MSE)]{Mean square error (MSE) \cite{bickel2015mathematical}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Calculates the mean of the sum (total) of all squared errors. (range: $[0, \infty)$)} \\
		\textcolor{Green}{$+$} & Partially solves the distribution problem. \\
		\textcolor{Red}{$-$}   & No differentiation depending on the maximum assumable error is made.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{MSE} = \dfrac{\textit{SE}}{n} = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n (\textit{GT}_i - P_i)^2
%
	\label{equation:MSE}
\end{equation}
\myequations{Mean square error (MSE)}


\subsection[Root mean square error (RMSE)]{Root mean square error (RMSE) \cite{willmott2005advantages, hyndman2006another, pontius2008components}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Calculates the root of the mean of the sum (total) of all squared errors. (range: $[0, \infty)$)} \\
		\textcolor{Green}{$+$} & Provides a result in the range of the compared values. \\
		\textcolor{Red}{$-$}   & No differentiation depending on the maximum assumable error is made.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{RMSE} = \sqrt{\textit{MSE}} = \sqrt{\dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n (\textit{GT}_i - P_i)^2}
%
	\label{equation:RMSE}
\end{equation}
\myequations{Root mean square error (RMSE)}


\subsection[Normalized root mean square error (NRMSE)]{Normalized root mean square error (NRMSE) \cite{chang2004air, kim2005missing}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Normalization of RMSE. (range: $[0, \infty)$)} \\
		\textcolor{Green}{$+$} & Comparison of models that differ significantly. \\
		\textcolor{Red}{$-$}   & Not sensitive to outliers and scaling.
	\end{tabularx}
\end{table}

\begin{equation}
	\begin{aligned}
		\textit{NRMSE}         &= \dfrac{\textit{RMSE}}{\overline{\textit{GT}}} = \dfrac{\sqrt{\dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n (\textit{GT}_i - P_i)^2}}{\overline{\textit{GT}}} \\
		\overline{\textit{GT}} &= \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \textit{GT}_i
%
		\label{equation:NRMSE}
	\end{aligned}
\end{equation}
%
\begin{conditions}
	\overline{\textit{GT}} & average of the ground truth
\end{conditions}
\myequations{Normalized root mean square error (NRMSE)}


\subsection[Root mean squared logarithmic error (RMSLE)]{Root mean squared logarithmic error (RMSLE) \cite{nafees2021predictive}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Calculates the mean squared error of the logarithmized ground truth in comparison to the logarithmized predictions. (range: $[0, \infty)$)} \\
		\textcolor{Green}{$+$} & Robust to outliers. \\
		\textcolor{Red}{$-$}   & Biased penalty. Underestimation is penalized more than overestimation.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{RMSLE} = \sqrt{\dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n (\ln(\textit{GT}_i + 1) - \ln(P_i + 1))^2}
%
	\label{equation:RMSLE}
\end{equation}
\myequations{Root mean squared logarithmic error (RMSLE)}


\subsection[Peak signal-to-noise ratio (PSNR)]{Peak signal-to-noise ratio (PSNR) \cite{salomon2004data, huynh2008scope}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Calculates the MSE in relation to the maximum assumable error. (range: $[0, \infty)$)} \\
		\textcolor{Green}{$+$} & A differentiation depending on the maximum assumable error is made. \\
		\textcolor{Red}{$-$}   & No differentiation depending on structural similarities is made.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{PSNR} = 10 \cdot \log_{10} \dfrac{E_\textit{max}^2}{\textit{MSE}} = 10 \cdot \log_{10} \dfrac{E_\textit{max}^2}{\frac{1}{n} \cdot \sum\nolimits_{i = 1}^n (\textit{GT}_i - P_i)^2}
%
	\label{equation:PSNR}
\end{equation}
%
\begin{conditions}
	E_\textit{max} & maximum possible error
\end{conditions}
\myequations{Peak signal-to-noise ratio (PSNR)}


\subsection[Structural similarity (SSIM)]{Structural similarity (SSIM) \cite{wang2004image, ghodrati2019mr}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Calculates the structural similarity using the mean, variance, and covariance. (range: $[-1, 1]$)} \\
		\textcolor{Green}{$+$} & Provides more accurate results by considering structural characteristics. \cite{wang2004image} \\
		\textcolor{Red}{$-$}   & Increased calculation complexity.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{SSIM} = \dfrac{(2 \mu_x \mu_y + c_1) (2 \sigma_{xy} + c_2)}{(\mu_x^2 + \mu_y^2 + c_1) (\sigma_x^2 + \sigma_y^2 + c_2)}
%
	\label{equation:SSIM}
\end{equation}
%
\begin{conditions}
	\mu_x$, $\mu_y       & mean       \\
	\sigma_x$, $\sigma_y & variance   \\
	\sigma_{xy}          & covariance \\
	c_1$, $c_2           & division stabilizers, e.g., $(0.01 \cdot 2^8 - 1)^2$ and $(0.03 \cdot 2^8 - 1)^2$ (8 bits per value)
\end{conditions}
\myequations{Structural similarity (SSIM)}


\subsection[Structural dissimilarity (DSSIM)]{Structural dissimilarity (DSSIM) \cite{wang2004image, ghodrati2019mr}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Calculates the structural dissimilarity using the mean, variance, and covariance. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Provides more accurate results by considering structural characteristics. \cite{wang2004image} \\
		\textcolor{Red}{$-$}   & Increased calculation complexity.
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{DSSIM} = \dfrac{1 - \textit{SSIM}}{2}
%
	\label{equation:DSSIM}
\end{equation}
\myequations{Structural dissimilarity (DSSIM)}


\subsection[Intersection over union (IoU)]{Intersection over union (IoU) \cite{jaccard1912distribution, murphy1996finley, rezatofighi2019generalized, zou2023object}}

Intersection over union (IoU) \cite{jaccard1912distribution, murphy1996finley, rezatofighi2019generalized, zou2023object} is a fundamental metric used primarily to evaluate the accuracy of object detection algorithms in tasks such as image segmentation and computer vision. IoU assesses the overlap between predicted and ground truth objects by calculating the ratio of the area of overlap to the area of union between the predicted and the actual annotations. It provides a clear, quantitative measure of how closely the contours of detected objects match the true object boundaries, encompassing both the correctness and precision of the detection in a single score. A higher IoU score indicates a greater degree of overlap and, consequently, an improved model performance. A perfect score of 1.0 represents an exact match between the predicted and the ground truth area. Because of its ability to accurately measure the effectiveness of detection models in capturing the true object space, IoU is extensively utilized in evaluating models for tasks such as autonomous driving, aerial image analysis \cite{al2018survey, akbari2021applications}, and medical imaging, where precise localization is critical. Its widespread adoption stems from its simplicity and effectiveness in providing a direct indicator of spatial accuracy.

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Calculates the similarity of two sets of values via the intersection over the union of both sets. In machine learning also known as the Jaccard index (JI). (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Captures overlap between predicted and true regions. \\
		\textcolor{Green}{$+$} & Commonly used in image segmentation and object detection tasks. \\
		\textcolor{Red}{$-$}   & Sensitive to small region misalignment. \\
		\textcolor{Red}{$-$}   & Does not distinguish between types of errors.
	\end{tabularx}
\end{table}

\begin{figure}[H]
	\centering

	% https://texample.net/tikz/examples/set-operations-illustrated-with-venn-diagrams/
	\begin{tikzpicture}
		\colorlet{draw_color}{LimeGreen}
		\colorlet{fill_color}{LimeGreen!50}
		\newcommand{\SetP}{(-1, 0) circle (1.5)}
		\newcommand{\SetQ}{( 1, 0) circle (1.5)}
		\tikzset{filled/.style={draw=draw_color, fill=fill_color}, outline/.style={draw=draw_color}}

		\node at (-4, 0) {$\textit{IoU} =$};
		\begin{scope}[yshift=2cm]
			\begin{scope}
				\clip \SetP;
				\fill[filled] \SetQ;
			\end{scope}
			\draw[outline] \SetP node {$\textit{GT}$};
			\draw[outline] \SetQ node {$P$};
			\node at (4, 0) {$|\textit{GT} \cap P|$};
		\end{scope}
		\draw (-3, 0) -- (3, 0);
		\begin{scope}[yshift=-2cm]
			\draw[filled] \SetP node {$\textit{GT}$} \SetQ node {$P$};
			\node at (4, 0) {$|\textit{GT} \cup P|$};
		\end{scope}
	\end{tikzpicture}

	\caption{Intersection over union (IoU).}
	\label{figure:IoU}
\end{figure}

\begin{equation}
	\textit{IoU} = \dfrac{|\textit{GT} \cap P|}{|\textit{GT} \cup P|}
%
	\label{equation:IoU}
\end{equation}
\myequations{Intersection over union (IoU)}


\subsection[Dice coefficient (DC)]{Dice coefficient (DC) \cite{dice1945measures, sorenson1948method}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Calculates the similarity of two sets of values via twice the intersection over the sum of both sets. In machine learning also known as the F1-score. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Can be well used for image segmentation and object detection. \\
		\textcolor{Red}{$-$}   & No differentiation depending on the size of both sets is made.
	\end{tabularx}
\end{table}

\begin{figure}[H]
	\centering

	% https://texample.net/tikz/examples/set-operations-illustrated-with-venn-diagrams/
	\begin{tikzpicture}
		\colorlet{draw_color}{LimeGreen}
		\colorlet{fill_color}{LimeGreen!50}
		\newcommand{\SetP}{(-1, 0) circle (1.5)}
		\newcommand{\SetQ}{( 1, 0) circle (1.5)}
		\tikzset{filled/.style={draw=draw_color, fill=fill_color}, outline/.style={draw=draw_color}}

		\node at (-5, 0) {$\textit{DC} =$};
		\begin{scope}[yshift=2cm]
			\node at (-3, 0) {$2 \cdot$};
			\begin{scope}
				\clip \SetP;
				\fill[filled] \SetQ;
			\end{scope}
			\draw[outline] \SetP node {$\textit{GT}$};
			\draw[outline] \SetQ node {$P$};
			\node at (5, 0) {$2 \cdot |\textit{GT} \cap P|$};
		\end{scope}
		\draw (-4, 0) -- (4, 0);
		\begin{scope}[yshift=-2cm]
			\draw[filled, xshift=-1cm] \SetP node {$\textit{GT}$};
			\node {$+$};
			\draw[filled, xshift=1cm] \SetQ node {$P$};
			\node at (5, 0) {$|\textit{GT}\,| + |P|$};
		\end{scope}
	\end{tikzpicture}

	\caption{Dice coefficient (DC).}
	\label{figure:DC}
\end{figure}

\begin{equation}
	\textit{DC} = \dfrac{2 \cdot |\textit{GT} \cap P|}{|\textit{GT}\,| + |P|}
%
	\label{equation:DC}
\end{equation}
\myequations{Dice coefficient (DC)}


\subsection[Overlap coefficient (OC)]{Overlap coefficient (OC) \cite{szymkiewicz1934conlribution, sempson1947holarctic, bell1962mutual, goodall1978sample}}

\begin{table}[H]\centering
	\begin{tabularx}{\textwidth}{@{}lX}
		\multicolumn{2}{@{}X}{Calculates the similarity of two sets of values via the intersection over the smaller set of both sets. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & A differentiation depending on the size of both sets is made. \\
	\end{tabularx}
\end{table}

\begin{equation}
	\textit{OC} = \dfrac{|\textit{GT} \cap P|}{\min(|\textit{GT}\,|, |P|)}
%
	\label{equation:OC}
\end{equation}
\myequations{Overlap coefficient (OC)}




\clearpage




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Functions %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Functions}

The following table gives an overview of computer vision metrics commonly used with the Python programming language machine learning libraries scikit-learn\footnote{scikit-learn project page, \url{https://scikit-learn.org/stable/}}, TensorFlow\footnote{TensorFlow project page, \url{https://www.tensorflow.org/}} (and Keras\footnote{Keras project page, \url{https://keras.io/}}), and PyTorch\footnote{PyTorch project page, \url{https://pytorch.org/}}\textsuperscript{,}\footnote{TorchMetrics project page, \url{https://torchmetrics.readthedocs.io/en/stable/}}.


\begin{table}[H]
	\centering

	\resizebox{\textwidth}{!}{\begin{tabular}{|l|c|c|c|}
		\hline
		Equation & scikit-learn & TensorFlow & PyTorch \\
		\hline
		%
		\hline

		Mean absolute error (MAE) (\ref{equation:MAE})
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html\#sklearn.metrics.mean_absolute_error}{mean\_absolute\_error}
		&
		\href{https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanAbsoluteError}{keras.losses.MeanAbsoluteError}
		&
		\href{https://torchmetrics.readthedocs.io/en/latest/regression/mean_absolute_error.html}{MeanAbsoluteError}
		\\

		Mean absolute percentage error (MAPE) (\ref{equation:MAPE})
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_percentage_error.html\#sklearn.metrics.mean_absolute_percentage_error}{mean\_absolute\_percentage\_error}
		&
		\href{https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanAbsolutePercentageError}{keras.losses.MeanAbsolutePercentageError}		
		&
		\href{https://torchmetrics.readthedocs.io/en/latest/regression/mean_absolute_percentage_error.html}{MeanAbsolutePercentageError}
		\\

		Mean square error (MSE) (\ref{equation:MSE})
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html\#sklearn.metrics.mean_squared_error}{mean\_squared\_error}
		&
		\href{https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanSquaredError}{keras.losses.MeanSquaredError}
		&
		\href{https://torchmetrics.readthedocs.io/en/latest/regression/mean_squared_error.html\#mean-squared-error-mse}{MeanSquaredError}
		\\

		Root mean square error (RMSE) (\ref{equation:RMSE})
		&
		/
		&
		\href{https://www.tensorflow.org/api_docs/python/tf/keras/metrics/RootMeanSquaredError}{keras.metrics.RootMeanSquaredError}
		&
		/
		\\

		Peak signal-to-noise ratio (PSNR) (\ref{equation:PSNR})
		&
		/
		&
		\href{https://www.tensorflow.org/api_docs/python/tf/image/psnr}{image.psnr}
		&
		/
		\\

		Structural similarity (SSIM) (\ref{equation:SSIM})
		&
		/
		&
		\href{https://www.tensorflow.org/api_docs/python/tf/image/ssim}{image.ssim}
		&
		/
		\\

		Intersection over union (IoU) (\ref{equation:IoU})
		&
		/
		&
		\href{https://www.tensorflow.org/api_docs/python/tf/keras/metrics/IoU}{keras.metrics.IoU}
		&
		\href{https://torchmetrics.readthedocs.io/en/latest/detection/intersection_over_union.html}{detection.iou.IntersectionOverUnion}
		\\

		Dice coefficient (DC) (\ref{equation:DC})
		&
		/
		&
		/
		&
		\href{https://torchmetrics.readthedocs.io/en/latest/classification/dice.html}{Dice}
		\\

		\hline
	\end{tabular}}

	\caption{Selection of function calls for the available metrics in scikit-learn, TensorFlow, and PyTorch. The call for the respective metrics follows the corresponding scheme: scikit-learn~-- sklearn.metrics.<metric>, TensorFlow~-- tf.<metric>, and PyTorch~-- torchmetrics.<metric>.}
	\label{table:CV_functions}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Functions %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\clearpage




\section*{Author contributions}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Tobias Schlosser, Michael Friedrich, and Trixy Meyer conducted this contribution's conceptualization and writing process with the help of Danny Kowerko in extending this manuscript with examples.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\clearpage




\bibliographystyle{IEEEtran}
\bibliography{library}




\end{document}

