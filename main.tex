%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Preamble %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass{article}




% General


\usepackage{lmodern, mathtools, microtype, textgreek}

\usepackage[main=english, ngerman]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}


\usepackage[hyphens]{url}
\usepackage[colorlinks, citecolor=LimeGreen, pagebackref=true]{hyperref}
\usepackage[dvipsnames]{xcolor}

% https://ctan.net/macros/latex/contrib/hyperref/doc/hyperref-doc.pdf
\renewcommand*{\backref}[1]{}
\renewcommand*{\backrefalt}[4]{%
	\ifnum #3 > 0 {%
		\ifnum #3 = 1 {%
			\ifnum #1 = 1 {%
				\\ (1 citation on 1 page: #2)
			} \else {%
				\\ (1 citation on #1 pages: #2)
			} \fi
		} \else {%
			\ifnum #1 = 1 {%
				\\ (#3 citations on 1 page: #2)
			} \else {%
				\\ (#3 citations on #1 pages: #2)
			} \fi
		} \fi
	} \fi
}


\usepackage{tikz}
\usetikzlibrary{arrows, backgrounds, calc, decorations, calligraphy, external, fit, positioning, shapes, spy}

\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usepgfplotslibrary{colorbrewer, fillbetween}

\usepackage[hang]{caption}

\usepackage{array, diagbox, float, multirow, subfig, xfrac}

\usepackage[export]{adjustbox}

\usepackage[defaultlines=3, all]{nowidow}

\setlength{\parskip}{   1em }
\setlength{\parindent}{ 0em }

\addtolength{\skip\footins}{1cm}

\renewcommand*{\arraystretch}{1.33}
\linespread{1.1}\selectfont

\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}




% ORCID

\usepackage{fontawesome5}
% \usepackage{hyperref}
% \usepackage{xcolor}

\newcommand{\ORCID}[1]{\thinspace\textsuperscript{\href{https://orcid.org/#1}{\textcolor[HTML]{A6CE39}{\faOrcid}}}}

\newcommand{\ORCIDFriedrich}{0000-0001-6326-4749} % ORCID Michael Friedrich
\newcommand{\ORCIDKowerko}{0000-0002-4538-7814}   % ORCID Danny Kowerko
\newcommand{\ORCIDMeyer}{0000-0002-3372-1619}     % ORCID Trixy Meyer
\newcommand{\ORCIDSchlosser}{0000-0002-0682-4284} % ORCID Tobias Schlosser


% \usepackage[left=2cm, right=2cm, top=2cm, bottom=2cm]{geometry}
\usepackage[left=3cm, right=3cm, top=3cm, bottom=3cm]{geometry}

\usepackage{fancyhdr, lastpage}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\fancypagestyle{plain}{\fancyfoot[C]{\thepage/\pageref{LastPage}}}
\fancyfoot[C]{\thepage/\pageref{LastPage}}

\usepackage[subfigure]{tocloft}

% https://tex.stackexchange.com/questions/95838/how-to-write-a-perfect-equation-parameters-description
\newenvironment{conditions}[1][where:]
	{\hspace{0.02\textwidth} #1 \begin{tabular}[t]{>{$}l<{$} @{${}={}$} l}}
	{\end{tabular}\\[\belowdisplayskip]}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Preamble %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\begin{document}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Title %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{\Huge A Consolidated Overview of Evaluation and Performance Metrics for Machine Learning and Computer Vision}

\author{
	Tobias Schlosser\ORCID{\ORCIDSchlosser}, Michael Friedrich\ORCID{\ORCIDFriedrich}, Trixy Meyer\ORCID{\ORCIDMeyer}, and Danny Kowerko\ORCID{\ORCIDKowerko} \\[1ex]
	Junior Professorship of Media Computing, \\
	Chemnitz University of Technology, \\
	09107 Chemnitz, Germany, \\
	\texttt{\{firstname.lastname\}@cs.tu-chemnitz.de}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Title %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\maketitle




\section*{Abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Motivated by the recent developments in machine learning (ML) and deep learning (DL) as well as a general need for big data analysis in object detection and classification as well as image generation by utilizing learning-based approaches and models such as deep neural networks (DNN), this manuscript aims at providing a consolidated overview of evaluation and performance metrics for machine learning and computer vision (CV). For this purpose, well-established evaluation metrics are presented, for which their (dis-)advantages as well as their origins are emphasized. Following, all to this manuscript related data, including our \LaTeX~source code, will be made publicly available and can be found under \url{https://github.com/TSchlosser13/Evaluation_Metrics}. As this manuscript is meant as a continuously consolidated overview, more evaluation metrics will be added over time.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\clearpage




\renewcommand{\contentsname}{Table of Contents}

\tableofcontents

\clearpage


% https://tex.stackexchange.com/questions/173102/table-of-equations-like-list-of-figures

\newcommand{\listequationsname}{\Large List of Equations}
\newlistof{myequations}{equ}{\listequationsname}
\newcommand{\myequations}[1]{\addcontentsline{equ}{myequations}{\protect\numberline{\theequation}#1}\par}
\setlength{\cftmyequationsnumwidth}{2.5em}

% \listofmyequations




\clearpage




\section{Machine Learning}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The following evaluation metrics within the context of machine learning are motivated by the contributions of \cite{metz1978basic, fawcett2006introduction}.

\vspace{1cm}
\hrule




\subsection*{General}

\begin{figure}[H]
	\centering

	\scalebox{0.9}{\begin{tikzpicture}[
	 c/.style={draw, circle},
	 r/.style={draw, rectangle}]
		\node[r, fill=Yellow!90,   minimum width=6.5cm, minimum height=7.5cm] (r) at (0, 0) {};
		\node[c, fill=YellowGreen, minimum size=5.5cm, thick]                 (c) at (0, 0) {};
		\draw (0, -3.75) -- (0, 3.75);

		\node (ae) at (6,  3) {all elements};
		\node (se) at (6, -3) {selected elements};
		\draw (ae) -- (r);
		\draw (se) -- (c);

		\node (P)  at (-2, 4.25) {P};
		\node (N)  at ( 2, 4.25) {N};
		\node (TP) at (-1, 0)    {TP};
		\node (TN) at ( 2, 3)    {TN};
		\node (FP) at ( 1, 0)    {FP};
		\node (FN) at (-2, 3)    {FN};
	\end{tikzpicture}}

	\caption{General definitions machine learning (reprinted from \cite[Figure 4.1]{Schlosser2023}).}
	\label{figure:ML_general}
\end{figure}

\begin{table}[H]
	\centering

	\begin{tabular}{|c|c|}
		\hline
		Abbreviation & Meaning \\
		\hline
		%
		\hline
		$\textit{T}$  & Total                       \\
		$\textit{P}$  & Positives                   \\
		$\textit{N}$  & Negatives                   \\
		$\textit{TP}$ & True Positives              \\
		$\textit{TN}$ & True Negatives              \\
		$\textit{FP}$ & False Positives             \\
		$\textit{FN}$ & False Negatives             \\
		$n$           & Number of values or classes \\
		\hline
	\end{tabular}

	\caption{General definitions machine learning.}
	\label{table:ML_general}
\end{table}

\vspace{1cm}
\hrule




\clearpage




\subsection[Precision~/ positive predictive value (PPV)]{Precision~/ positive predictive value (PPV) \cite{altman1994statistics, fletcher2019clinical}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Fraction of true positives and false positives. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Emphasizes the accuracy of classifying true positives in relation to false positives. \\
		\textcolor{Red}{$-$}   & Neglects true negatives and false negatives.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{Precision} = \dfrac{\textit{TP}}{\textit{TP} + \textit{FP}}
%
	\label{equation:precision}
\end{equation}
\myequations{Precision / positive predictive value (PPV)}

\hrule


\subsubsection[Macro average precision (APmacro)]{Macro average precision (AP\textsubscript{macro}) \cite{yang1999evaluation, sebastiani2002machine, zhu2004recall, he2018local}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Macro average precision calculates the average precision for each class separately and takes the average over all classes. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Each class is weighted equally regardless of its frequency in the data set, thus allowing each class to be evaluated individually. \\
		\textcolor{Green}{$+$} & Advantageous when the recognition rate of TP compared to FP is of interest. \\
		\textcolor{Red}{$-$}   & Rare classes have the same weight as frequent classes. This can lead to bias. \\
		\textcolor{Red}{$-$}   & Only partially evaluates a model's classification capabilities.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{AP}_\textit{macro} = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \textit{Precision}_i = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \dfrac{\textit{TP}_i}{\textit{TP}_i + \textit{FP}_i}
%
	\label{equation:MAAP}
\end{equation}
\myequations{Macro average precision (AP\textsubscript{macro})}

\hrule


\subsubsection[Micro average precision (APmicro)]{Micro average precision (AP\textsubscript{micro}) \cite{yang1999evaluation, sebastiani2002machine}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Micro average precision calculates the precision across all classes. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Use with inconsistent data sets / class distributions. Provides overview of overall model performance. \\
		\textcolor{Red}{$-$}   & Neglects infrequent classes and overestimates frequent classes.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{AP}_\textit{micro} = \dfrac{\sum\nolimits_{i = 1}^n \textit{TP}_i}{\sum\nolimits_{i = 1}^n (\textit{TP}_i + \textit{FP}_i)}
%
	\label{equation:MIAP}
\end{equation}
\myequations{Micro average precision (AP\textsubscript{micro})}

\hrule


\subsubsection[Weighted average precision (APweighted)]{Weighted average precision (AP\textsubscript{weighted}) \cite{han2014rule}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Weighted average precision calculates the weighted precision across all classes. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Use with inconsistent data sets / class distributions. Provides overview of overall model performance. \\
		\textcolor{Red}{$-$}   & Neglects under-weighted classes and overestimates over-weighted classes.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{AP}_\textit{weighted} = \dfrac{1}{\sum\nolimits_{i = 1}^n w_i} \cdot \sum\nolimits_{i = 1}^n w_i \cdot \textit{Precision}_i = \dfrac{1}{\sum\nolimits_{i = 1}^n w_i} \cdot \sum\nolimits_{i = 1}^n w_i \cdot \dfrac{\textit{TP}_i}{\textit{TP}_i + \textit{FP}_i}
%
	\label{equation:WAP}
\end{equation}
%
\begin{conditions}
	w_i & samples per class or the relation of samples per class to the total of all samples
\end{conditions}
\myequations{Weighted average precision (AP\textsubscript{weighted})}

\hrule


\subsection[Negative predictive value (NPV)]{Negative predictive value (NPV) \cite{altman1994statistics, fletcher2019clinical}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Fraction of true negatives and false negatives. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Emphasizes the accuracy of classifying true negatives in relation to false negatives. \\
		\textcolor{Red}{$-$}   & Neglects true positives and false positives.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{NPV} = \dfrac{\textit{TN}}{\textit{TN} + \textit{FN}}
%
	\label{equation:NPV}
\end{equation}
\myequations{Negative predictive value (NPV)}

\hrule


\subsection[Recall~/ true positive rate (TPR)~/ sensitivity~/ hit rate]{Recall~/ true positive rate (TPR)~/ sensitivity~/ hit rate \cite{yerushalmy1947statistical, altman1994diagnostic}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Fraction of true positives and false negatives. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Emphasizes the accuracy of classifying true positives in relation to false negatives. \\
		\textcolor{Red}{$-$}   & Neglects true negatives and false positives.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{Recall} = \dfrac{\textit{TP}}{\textit{P}} = \dfrac{\textit{TP}}{\textit{TP} + \textit{FN}}
%
	\label{equation:recall}
\end{equation}
\myequations{Recall / true positive rate (TPR) / sensitivity / hit rate}

\hrule


\subsubsection[Macro average recall (ARmacro)]{Macro average recall (AR\textsubscript{macro}) \cite{yang1999evaluation, sebastiani2002machine, rosenberg2012classifying, yang2020edgernn}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Macro average recall calculates the average recall for each class separately and takes the average over all classes. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Each class is weighted equally regardless of its frequency in the data set, thus allowing each class to be evaluated individually. \\
		\textcolor{Green}{$+$} & Advantageous when the recognition rate of TP compared to FN is of interest. \\
		\textcolor{Red}{$-$}   & Rare classes have the same weight as frequent classes. This can lead to bias. \\
		\textcolor{Red}{$-$}   & Only partially evaluates a model's classification capabilities.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{AR}_\textit{macro} = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \textit{Recall}_i = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \dfrac{\textit{TP}_i}{\textit{TP}_i + \textit{FN}_i}
%
	\label{equation:MAAR}
\end{equation}
\myequations{Macro average recall (AR\textsubscript{macro})}

\hrule


\subsubsection[Micro average recall (ARmicro)]{Micro average recall (AR\textsubscript{micro}) \cite{yang1999evaluation, sebastiani2002machine}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Micro average recall calculates the recall across all classes. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Use with inconsistent data sets / class distributions. Provides overview of overall model performance. \\
		\textcolor{Red}{$-$}   & Neglects infrequent classes and overestimates frequent classes.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{AR}_\textit{micro} = \dfrac{\sum\nolimits_{i = 1}^n \textit{TP}_i}{\sum\nolimits_{i = 1}^n (\textit{TP}_i + \textit{FN}_i)}
%
	\label{equation:MIAR}
\end{equation}
\myequations{Micro average recall (AR\textsubscript{micro})}

\hrule


\subsubsection[Weighted average recall (ARweighted)]{Weighted average recall (AR\textsubscript{weighted}) \cite{gordon1988effect, han2014rule}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Weighted average recall calculates the weighted recall across all classes. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Use with inconsistent data sets / class distributions. Provides overview of overall model performance. \\
		\textcolor{Red}{$-$}   & Neglects under-weighted classes and overestimates over-weighted classes.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{AR}_\textit{weighted} = \dfrac{1}{\sum\nolimits_{i = 1}^n w_i} \cdot \sum\nolimits_{i = 1}^n w_i \cdot \textit{Recall}_i = \dfrac{1}{\sum\nolimits_{i = 1}^n w_i} \cdot \sum\nolimits_{i = 1}^n w_i \cdot \dfrac{\textit{TP}_i}{\textit{TP}_i + \textit{FN}_i}
%
	\label{equation:WAR}
\end{equation}
%
\begin{conditions}
	w_i & samples per class or the relation of samples per class to the total of all samples
\end{conditions}
\myequations{Weighted average recall (AR\textsubscript{weighted})}

\hrule


\subsection[True negative rate (TNR)~/ specificity~/ selectivity]{True negative rate (TNR)~/ specificity~/ selectivity \cite{yerushalmy1947statistical, altman1994diagnostic}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Fraction of true negatives and false positives. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Emphasizes the accuracy of classifying true negatives in relation to false positives. \\
		\textcolor{Red}{$-$}   & Neglects true positives and false negatives.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{TNR} = \dfrac{\textit{TN}}{\textit{N}} = \dfrac{\textit{TN}}{\textit{TN} + \textit{FP}}
%
	\label{equation:TNR}
\end{equation}
\myequations{True negative rate (TNR) / specificity / selectivity}

\hrule


\subsection[Prevalence]{Prevalence \cite{rothman2012epidemiology, bruce2018quantitative}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Fraction of positives with positives and negatives. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Straightforward to calculate. \\
		\textcolor{Red}{$-$}   & Neglects TP, TN, FP, and FN.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{Prevalence} = \dfrac{\textit{P}}{\textit{T}} = \dfrac{\textit{P}}{\textit{P} + \textit{N}}
%
	\label{equation:prevalence}
\end{equation}
\myequations{Prevalence}

\hrule


\subsection[Accuracy (A)]{Accuracy (A) \cite{metz1978basic, taylor1997introduction}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Fraction of correct classifications. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Emphasizes the accuracy of classifying true positives and true negatives. \\
		\textcolor{Red}{$-$}   & Misleading for imbalanced data (see also balanced accuracy).
	\end{tabular}
\end{table}

\begin{equation}
	A = \dfrac{\textit{TP} + \textit{TN}}{\textit{T}} = \dfrac{\textit{TP} + \textit{TN}}{\textit{P} + \textit{N}} = \dfrac{\textit{TP} + \textit{TN}}{\textit{TP} + \textit{TN} + \textit{FP} + \textit{FN}}
%
	\label{equation:A}
\end{equation}
\myequations{Accuracy (A)}

\hrule


\subsection[Balanced accuracy (BA)]{Balanced accuracy (BA) \cite{brodersen2010balanced, kelleher2020fundamentals}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Balanced fraction of correct classifications. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Normalizes true positives and true negatives. \\
		\textcolor{Red}{$-$}   & Neglects, e.g., FPR and FNR.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{BA}_\textit{binary} = \dfrac{\textit{TPR} + \textit{TNR}}{2} = \dfrac{\dfrac{\textit{TP}}{\textit{TP} + \textit{FN}} + \dfrac{\textit{TN}}{\textit{TN} + \textit{FP}}}{2}
%
	\label{equation:BA_binary}
\end{equation}
\myequations{Balanced accuracy (BA) for binary classification}

\begin{equation}
	\textit{BA}_\textit{multi} = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \textit{Recall}_i
%
	\label{equation:BA_multi}
\end{equation}
\myequations{Balanced accuracy (BA) for multi-class classification}

\hrule


\subsection[Balanced accuracy weighted (BAW)]{Balanced accuracy weighted (BAW) \cite{salman2017detection, infante2023factors}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Building upon the balanced accuracy approach, an additional class weighting is added. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Suitable for multi-class problems, robust against class imbalance. \\
		\textcolor{Red}{$-$}   & More complex and rarely used metric.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{BAW} = \dfrac{1}{\sum\nolimits_{i = 1}^n w_i} \cdot \sum\nolimits_{i = 1}^n w_i \cdot \textit{Recall}_i = \dfrac{1}{\sum\nolimits_{i = 1}^n w_i} \cdot \sum\nolimits_{i = 1}^n w_i \cdot \dfrac{\textit{TP}_i}{\textit{TP}_i + \textit{FN}_i}
%
	\label{equation:BAW}
\end{equation}
%
\begin{conditions}
	w_i & samples per class or the relation of samples per class to the total of all samples
\end{conditions}
\myequations{Balanced accuracy weighted (BAW)}

\hrule


\subsection[Average accuracy (AA)]{Average accuracy (AA) \cite{brodersen2010balanced, huang2019ecg}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Average accuracy over all classes. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Suitable for data sets that consist of balanced classes. \\
		\textcolor{Red}{$-$}   & Can yield poor results when a biased classifier is tested on imbalanced data.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{AA} = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n A_i = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \dfrac{\textit{TP}_i + \textit{TN}_i}{\textit{TP}_i + \textit{FP}_i + \textit{TN}_i + \textit{FN}_i}
%
	\label{equation:AA}
\end{equation}
\myequations{Average accuracy (AA)}

\hrule


\subsection[Average class accuracy (ACA)]{Average class accuracy (ACA) \cite{bhowan2011developing, devarriya2020unbalanced}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Calculates a weighted average classification accuracy based on a minority accuracy corresponding to TPR and a majority accuracy corresponding to TNR. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Suitable when a significant class imbalance between minority and majority classes exists. \\
		\textcolor{Red}{$-$}   & Difficult to choose a good weighting factor.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{ACA} = w \cdot \textit{TPR} + (1 - w) \cdot \textit{TNR} = w \cdot \dfrac{\textit{TP}}{\textit{TP} + \textit{FN}} + (1 - w) \cdot \dfrac{\textit{TN}}{\textit{TN} + \textit{FP}}
%
	\label{equation:ACA}
\end{equation}
%
\begin{conditions}
	w & weight of the positive class in relation to the data set, $0 \le w \le 1$
\end{conditions}
\myequations{Average class accuracy (ACA)}

\hrule


\subsection[Error rate (ER)]{Error rate (ER) \cite{hand1986recent, asri2016using}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Complementary metric to accuracy. All faulty classifications are divided by the total number of classifications. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Suitable for problems where the evaluation of error recognition is of importance. \\
		\textcolor{Red}{$-$}   & Poor performance for imbalanced data.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{ER} = \dfrac{\textit{FP} + \textit{FN}}{\textit{T}} = \dfrac{\textit{FP} + \textit{FN}}{\textit{P} + \textit{N}} = \dfrac{\textit{FP} + \textit{FN}}{\textit{TP} + \textit{FP} + \textit{TN} + \textit{FN}}
%
	\label{equation:ER}
\end{equation}
\myequations{Error rate (ER)}

\hrule


\subsection[Average error rate (AER)]{Average error rate (AER) \cite{hamamoto1998gabor, han2016variable}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Average error rate over all classes. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Suitable for problems where the evaluation of error recognition is of importance. \\
		\textcolor{Red}{$-$}   & Poor performance for imbalanced data.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{AER} = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \textit{ER}_i = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \dfrac{\textit{FP}_i + \textit{FN}_i}{\textit{TP}_i + \textit{FP}_i + \textit{TN}_i + \textit{FN}_i}
%
	\label{equation:AER}
\end{equation}
\myequations{Average error rate (AER)}

\hrule


\subsection[F-score~/ F\textbeta-score]{F-score~/ F\textbeta-score \cite{van2004geometry, taha2015metrics}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Measures the user-defined classification effectiveness. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & The recall can be weighted $\beta$~times as important as the precision. \\
		\textcolor{Red}{$-$}   & Finding the correct $\beta$~can be different from application to application.
	\end{tabular}
\end{table}

\begin{equation}
	F_\beta = (1 + \beta^2) \cdot \dfrac{\textit{Precision} \cdot \textit{Recall}}{(\beta^2 \cdot \textit{Precision}) + \textit{Recall}} = \dfrac{(1 + \beta^2) \cdot \textit{TP}}{(1 + \beta^2) \cdot \textit{TP} + \beta^2 \cdot \textit{FN} + \textit{FP}}
%
	\label{equation:F-score}
\end{equation}
\myequations{F-score / F\textbeta-score}

\hrule


\subsubsection[Macro average F-score]{Macro average F-score \cite{mohammad2013nrc, takahashi2022confidence}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Averaged F-score over all classes. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Values all classes equally. \\
		\textcolor{Red}{$-$}   & Valuing all classes equally can be detrimental in case of class imbalance.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{Macro average F-score} = \dfrac{1}{\textit{n}} \cdot \sum\nolimits_{i = 1}^n \textit{F}_i
%
	\label{equation:MAAF}
\end{equation}
%
\begin{conditions}
	F_i & F-score of class $i$ with a chosen $\beta$
\end{conditions}
\myequations{Macro average F-score}

\hrule


\subsubsection[Micro average F-score]{Micro average F-score \cite{goutte2005probabilistic, takahashi2022confidence}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Averaged F-score based on the micro average of precision and recall. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Useful when the performance of larger classes is of importance. \\
		\textcolor{Red}{$-$}   & Prone to issues with class imbalance.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{Micro average F-score} = 2 \cdot \dfrac{\textit{AP}_\textit{micro} \cdot \textit{AR}_\textit{micro}}{\textit{AP}_\textit{micro} + \textit{AR}_\textit{micro}}
%
	\label{equation:MIAF}
\end{equation}
\myequations{Micro average F-score}

\hrule


\subsubsection[Weighted average F-score]{Weighted average F-score \cite{al2016lili, alswaidan2020hybrid}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Weighted averaged F-score over all classes. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & More robust against class imbalance. \\
		\textcolor{Red}{$-$}   & Not widely used within the literature.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{Weighted average F-score} = \dfrac{1}{\sum\nolimits_{i = 1}^n w_i} \cdot \sum\nolimits_{i = 1}^n (w_i \cdot F_i)
%
	\label{equation:WAF}
\end{equation}
%
\begin{conditions}
	w_i & samples per class or the relation of samples per class to the total of all samples \\
	F_i & F-score of class $i$
\end{conditions}
\myequations{Weighted average F-score}

\hrule


\subsection[F0-score]{F0-score \cite{van2004geometry, taha2015metrics}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{The recall is weighted 0 times as important as the precision. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & The importance of the recall is maximized. \\
		\textcolor{Red}{$-$}   & The importance of the precision is minimized.
	\end{tabular}
\end{table}

\begin{equation}
	F_0 = (1 + 0^2) \cdot \dfrac{\textit{Precision} \cdot \textit{Recall}}{(0^2 \cdot \textit{Precision}) + \textit{Recall}} = \dfrac{\textit{Precision} \cdot \textit{Recall}}{\textit{Recall}}
%
	\label{equation:F0-score}
\end{equation}
\myequations{F0-score}

\hrule


\subsection[F0.5-score]{F0.5-score \cite{van2004geometry, taha2015metrics}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{The recall is weighted 0.5 times as important as the precision. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & The importance of the recall is increased. \\
		\textcolor{Red}{$-$}   & The importance of the precision is decreased.
	\end{tabular}
\end{table}

\begin{equation}
	F_{0.5} = (1 + 0.5^2) \cdot \dfrac{\textit{Precision} \cdot \textit{Recall}}{(0.5^2 \cdot \textit{Precision}) + \textit{Recall}} = 1.25 \cdot \dfrac{\textit{Precision} \cdot \textit{Recall}}{(0.25 \cdot \textit{Precision}) + \textit{Recall}}
%
	\label{equation:F0.5-score}
\end{equation}
\myequations{F0.5-score}

\hrule


\subsection[F1-score]{F1-score \cite{van2004geometry, taha2015metrics}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Measures the harmonic mean of the precision and the recall. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Equal importance of precision and recall. \\
		\textcolor{Red}{$-$}   & Does not consider true negatives.
	\end{tabular}
\end{table}

\begin{equation}
	F_1 = (1 + 1^2) \cdot \dfrac{\textit{Precision} \cdot \textit{Recall}}{(1^2 \cdot \textit{Precision}) + \textit{Recall}} = 2 \cdot \dfrac{\textit{Precision} \cdot \textit{Recall}}{\textit{Precision} + \textit{Recall}}
%
	\label{equation:F1-score}
\end{equation}
\myequations{F1-score}

\hrule


\subsection[F2-score]{F2-score \cite{van2004geometry, taha2015metrics}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{The recall is weighted 2 times as important as the precision. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & The importance of the recall is decreased. \\
		\textcolor{Red}{$-$}   & The importance of the precision is increased.
	\end{tabular}
\end{table}

\begin{equation}
	F_2 = (1 + 2^2) \cdot \dfrac{\textit{Precision} \cdot \textit{Recall}}{(2^2 \cdot \textit{Precision}) + \textit{Recall}} = 5 \cdot \dfrac{\textit{Precision} \cdot \textit{Recall}}{(4 \cdot \textit{Precision}) + \textit{Recall}}
%
	\label{equation:F2-score}
\end{equation}
\myequations{F2-score}

\hrule


\subsection[False discovery rate (FDR)]{False discovery rate (FDR) \cite{benjamini1995controlling, benjamini2001control}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Fraction of false positives and true positives. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Emphasizes the accuracy of classifying false positives in relation to true positives. \\
		\textcolor{Red}{$-$}   & Neglects true negatives and false negatives.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{FDR} = \dfrac{\textit{FP}}{\textit{FP} + \textit{TP}}
%
	\label{equation:FDR}
\end{equation}
\myequations{False discovery rate (FDR)}

\hrule


\subsubsection{Macro average FDR (FDRmacro)}

\begin{equation}
	\textit{FDR}_\textit{macro} = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \textit{FDR}_i = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \dfrac{\textit{FP}_i}{\textit{FP}_i + \textit{TP}_i}
%
	\label{equation:MAAFDR}
\end{equation}
\myequations{Macro average FDR (FDR\textsubscript{macro})}

\hrule


\subsubsection{Micro average FDR (FDRmicro)}

\begin{equation}
	\textit{FDR}_\textit{micro} = \dfrac{\sum\nolimits_{i = 1}^n \textit{FP}_i}{\sum\nolimits_{i = 1}^n (\textit{FP}_i + \textit{TP}_i)}
%
	\label{equation:MIAFDR}
\end{equation}
\myequations{Micro average FDR (FDR\textsubscript{micro})}

\hrule


\subsubsection{Weighted average FDR (FDRweighted)}

\begin{equation}
	\textit{FDR}_\textit{weighted} = \dfrac{1}{\sum\nolimits_{i = 1}^n w_i} \cdot \sum\nolimits_{i = 1}^n w_i \cdot \textit{FDR}_i = \dfrac{1}{\sum\nolimits_{i = 1}^n w_i} \cdot \sum\nolimits_{i = 1}^n w_i \cdot \dfrac{\textit{FP}_i}{\textit{FP}_i + \textit{TP}_i}
%
	\label{equation:WAFDR}
\end{equation}
\myequations{Weighted average FDR (FDR\textsubscript{weighted})}

\hrule


\subsection[False omission rate (FOR)]{False omission rate (FOR) \cite{zafar2017fairness}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Fraction of false negatives and true negatives. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Emphasizes the accuracy of classifying false negatives in relation to true negatives. \\
		\textcolor{Red}{$-$}   & Neglects true positives and false positives.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{FOR} = \dfrac{\textit{FN}}{\textit{FN} + \textit{TN}}
%
	\label{equation:FOR}
\end{equation}
\myequations{False omission rate (FOR)}

\hrule


\subsection[False positive rate (FPR)]{False positive rate (FPR) \cite{banerjee2009hypothesis}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Fraction of false positives and true negatives. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Emphasizes the accuracy of classifying false positives in relation to true negatives. \\
		\textcolor{Red}{$-$}   & Neglects true positives and false negatives.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{FPR} = \dfrac{\textit{FP}}{\textit{N}} = \dfrac{\textit{FP}}{\textit{FP} + \textit{TN}}
%
	\label{equation:FPR}
\end{equation}
\myequations{False positive rate (FPR)}

\hrule


\subsection[False negative rate (FNR)~/ miss rate]{False negative rate (FNR)~/ miss rate \cite{banerjee2009hypothesis}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Fraction of false negatives and true positives. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Emphasizes the accuracy of classifying false negatives in relation to true positives. \\
		\textcolor{Red}{$-$}   & Neglects true negatives and false positives.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{FNR} = \dfrac{\textit{FN}}{\textit{P}} = \dfrac{\textit{FN}}{\textit{FN} + \textit{TP}}
%
	\label{equation:FNR}
\end{equation}
\myequations{False negative rate (FNR) / miss rate}

\hrule


\subsubsection{Macro average FNR (FNRmacro)}

\begin{equation}
	\textit{FNR}_\textit{macro} = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \textit{FNR}_i = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \dfrac{\textit{FN}_i}{\textit{FN}_i + \textit{TP}_i}
%
	\label{equation:MAAFNR}
\end{equation}
\myequations{Macro average FNR (FNR\textsubscript{macro})}

\hrule


\subsubsection{Micro average FNR (FNRmicro)}

\begin{equation}
	\textit{FNR}_\textit{micro} = \dfrac{\sum\nolimits_{i = 1}^n \textit{FN}_i}{\sum\nolimits_{i = 1}^n (\textit{FN}_i + \textit{TP}_i)}
%
	\label{equation:MIAFNR}
\end{equation}
\myequations{Micro average FNR (FNR\textsubscript{micro})}

\hrule


\subsubsection{Weighted average FNR (FNRweighted)}
\begin{equation}
	\textit{FNR}_\textit{weighted} = \dfrac{1}{\sum\nolimits_{i = 1}^n w_i} \cdot \sum\nolimits_{i = 1}^n w_i \cdot \textit{FNR}_i = \dfrac{1}{\sum\nolimits_{i = 1}^n w_i} \cdot \sum\nolimits_{i = 1}^n w_i \cdot \dfrac{\textit{FN}_i}{\textit{FN}_i + \textit{TP}_i}
%
	\label{equation:WAFNR}
\end{equation}
\myequations{Weighted average FNR (FNR\textsubscript{weighted})}

\hrule


\subsection[Positive likelihood ratio (LR+)]{Positive likelihood ratio (LR$+$) \cite{swets1973relative, deeks2004diagnostic}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{In medicine, LR$+$ describes the probability of, e.g., a positive result in a sick person (true positive) in relation to the probability of a positive result in a healthy person (false positive). (range: $[0, \infty)$)} \\
		\textcolor{Green}{$+$} & Emphasizes the accuracy of the TPR in relation to the FPR. \\
		\textcolor{Red}{$-$}   & Neglects, e.g., TNR and FNR (see also diagnostic odds ratio).
	\end{tabular}
\end{table}

\begin{equation}
	\textit{LR}+ = \dfrac{\textit{Sensitivity}}{1 - \textit{Specificity}} = \dfrac{\textit{TPR}}{\textit{FPR}}
%
	\label{equation:LR+}
\end{equation}
\myequations{Positive likelihood ratio (LR$+$)}

\hrule


\subsection[Negative likelihood ratio (LR-)]{Negative likelihood ratio (LR$-$) \cite{swets1973relative, deeks2004diagnostic}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{In medicine, LR$-$ describes the probability of, e.g., a negative result in a sick person (false negative) in relation to the probability of a negative result in a healthy person (true negative). (range: $[0, \infty)$)} \\
		\textcolor{Green}{$+$} & Emphasizes the accuracy of the FNR in relation to the TNR. \\
		\textcolor{Red}{$-$}   & Neglects, e.g., TPR and FPR (see also diagnostic odds ratio).
	\end{tabular}
\end{table}

\begin{equation}
	\textit{LR}- = \dfrac{1 - \textit{Sensitivity}}{\textit{Specificity}} = \dfrac{\textit{FNR}}{\textit{TNR}}
%
	\label{equation:LR-}
\end{equation}
\myequations{Negative likelihood ratio (LR$-$)}

\hrule


\subsection[Diagnostic odds ratio (DOR)]{Diagnostic odds ratio (DOR) \cite{glas2003diagnostic, doust2004systematic}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{In medicine, DOR measures the classification effectiveness of a diagnostic test. \cite{glas2003diagnostic} (range: $[0, \infty)$)} \\
		\textcolor{Green}{$+$} & Emphasizes the accuracy of the LR$+$ in relation to the LR$-$. \\
		\textcolor{Green}{$+$} & Independent of the prevalence. \\
		\textcolor{Red}{$-$}   & Increased calculation and interpretation complexity.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{DOR} = \dfrac{\textit{LR}+}{\textit{LR}-} = \dfrac{\textit{TP} \cdot \textit{TN}}{\textit{FP} \cdot \textit{FN}}
%
	\label{equation:DOR}
\end{equation}
\myequations{Diagnostic odds ratio (DOR)}

\hrule


\subsection[Fowlkes--Mallows index (FM)]{Fowlkes--Mallows index (FM) \cite{fowlkes1983method, halkidi2001clustering}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Measures the geometric mean of the precision and the recall. In clustering, FM measures the similarity of two clusters. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Increased robustness to noise. \\
		\textcolor{Red}{$-$}   & F1-score geometric mean equivalent (no weighting factor $\beta$~provided). \\
		\textcolor{Red}{$-$}   & Less well known and used.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{FM} = \sqrt{\textit{PPV} \cdot \textit{TPR}} = \sqrt{\textit{Precision} \cdot \textit{Recall}}
%
	\label{equation:FM}
\end{equation}
\myequations{Fowlkes--Mallows index (FM)}

\hrule


\subsection[Informedness~/ bookmaker informedness (BM)~/ Youden's J statistic~/ Youden's index]{Informedness~/ bookmaker informedness (BM)~/ Youden's J statistic~/ Youden's index \cite{peirce1884numerical, youden1950index}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Quantifies the probability of an >>informed decision<<. (range: $[-1, 1]$)} \\
		\textcolor{Green}{$+$} & Takes all predictions into account. \\
		\textcolor{Red}{$-$}   & Its result ranges from $-1$ to $1$.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{Informedness} = \textit{Sensitivity} + \textit{Specificity} - 1 = \textit{TPR} + \textit{TNR} - 1
%
	\label{equation:informedness}
\end{equation}
\myequations{Informedness / bookmaker informedness (BM) / Youden's J statistic / Youden's index}

\hrule


\subsection[Markedness (MK)]{Markedness (MK) \cite{powers2020evaluation}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Quantifies the >>markedness<<, i.e., the state of being irregular or uncommon. (range: $[-1, 1]$)} \\
		\textcolor{Green}{$+$} & Takes all predictions into account. \\
		\textcolor{Red}{$-$}   & Its result ranges from $-1$ to $1$.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{MK} = \textit{PPV} + \textit{NPV} - 1
%
	\label{equation:MK}
\end{equation}
\myequations{Markedness (MK)}

\hrule


\subsection[Matthews correlation coefficient (MCC)~/ phi coefficient~/ Yule phi coefficient]{Matthews correlation coefficient (MCC)~/ phi coefficient~/ Yule phi coefficient \cite{yule1912methods, matthews1975comparison, cramer1999mathematical}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{A balanced measure of TPR, TNR, PPV, and NPV. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Takes all predictions into account. \\
		\textcolor{Red}{$-$}   & Increased calculation and interpretation complexity.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{MCC} = \sqrt{\textit{TPR} \cdot \textit{TNR} \cdot \textit{PPV} \cdot \textit{NPV}}
%
	\label{equation:MCC}
\end{equation}
\myequations{Matthews correlation coefficient (MCC) / phi coefficient / Yule phi coefficient}

\hrule


\subsection[Jaccard index (JI)~/ threat score (TS)~/ critical success index (CSI)]{Jaccard index (JI)~/ threat score (TS)~/ critical success index (CSI) \cite{jaccard1912distribution, murphy1996finley}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Fraction of TP with TP, FN, and FP. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Emphasizes the accuracy of classifying TP in relation to FN and FP. \\
		\textcolor{Red}{$-$}   & Neglects TN.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{JI} = \dfrac{\textit{TP}}{\textit{TP} + \textit{FN} + \textit{FP}}
%
	\label{equation:JI}
\end{equation}
\myequations{Jaccard index (JI) / threat score (TS) / critical success index (CSI)}

\hrule


\subsection[Receiver operating characteristic curve (ROC curve) and area under the curve (AUC)]{Receiver operating characteristic curve (ROC curve) and area under the curve (AUC) \cite{green1966signal, zweig1993receiver, fawcett2006introduction}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Calculates the area under the so-called receiver operating characteristic curve by adjusting the confidence threshold for, e.g., classification, detection, or segmentation. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Multiple results allow for a more meaningful evaluation. \\
		\textcolor{Red}{$-$}   & Dependent on TPR and FPR. \\
		\textcolor{Red}{$-$}   & Increased calculation complexity.
	\end{tabular}
\end{table}

\begin{figure}[H]
	\centering

	\begin{tikzpicture}
		\begin{axis}[
		 width=0.5\textwidth, height=0.5\textwidth,
		 xlabel={False positive rate (FPR)}, ylabel={Recall / true positive rate (TPR) / sensitivity},
		 ticklabel style={/pgf/number format/.cd, fixed, fixed zerofill, precision=1},
		 legend columns=-1, legend style={at={(0.5, 1.25)}, anchor=north, column sep=1ex}]
			\addplot[domain=0:1, dashed] {x};
			\addplot[name path=f, domain=0:1, samples=512, smooth, LimeGreen] {1 - (x - 1)^2};
			\path[name path=axis] (0, 0) -- (1, 0);
			\addplot[LimeGreen!50] fill between[of=f and axis];
			\legend{x, Exemplary ROC curve, AUC}
		\end{axis}
	\end{tikzpicture}

	\caption{Receiver operating characteristic curve (ROC curve) and area under the curve (AUC).}
	\label{figure:ROC_AUC}
\end{figure}

\begin{equation}
	\textit{AUC} = \int_{x = 0}^1 \textit{TPR}(\textit{FPR}^{\,-1}(x)) \,dx = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \textit{TPR}(\textit{FPR}_i)
%
	\label{equation:AUC}
\end{equation}
\myequations{Area under the curve (AUC)}

\hrule


\subsection[Average precision (AP)]{Average precision (AP) \cite{manning2009introduction, everingham2010pascal}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Calculates the area under the precision-recall curve by adjusting the confidence threshold for, e.g., classification, detection, or segmentation. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Multiple results allow for a more meaningful evaluation. \\
		\textcolor{Red}{$-$}   & Dependent on precision and recall. \\
		\textcolor{Red}{$-$}   & Increased calculation complexity.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{AP} = \int_{x = 0}^1 \textit{Precision}(\textit{Recall}^{\,-1}(x)) \,dx = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \textit{Precision}(\textit{Recall}_i)
%
	\label{equation:AP}
\end{equation}
\myequations{Average precision (AP)}

\hrule


\subsection[Mean average precision (mAP)]{Mean average precision (mAP) \cite{manning2009introduction, everingham2010pascal}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Averages the area under the precision-recall curve over multiple classes. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Allows for an even more meaningful evaluation over multiple classes. \\
		\textcolor{Red}{$-$}   & Increased calculation complexity.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{mAP} = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \textit{AP}_i
%
	\label{equation:mAP}
\end{equation}
\myequations{Mean average precision (mAP)}

\hrule


\subsection[Cohen's kappa for binary classification]{Cohen's kappa for binary classification \cite{cohen1960coefficient, ranganathan2017common, chicco2021matthews}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Ranges from $-1$ for a fully wrong prediction to $+1$ for a completely correct prediction. (range: $[-1, 1]$)} \\
		\textcolor{Green}{$+$} & Robust against class imbalance. \\
		\textcolor{Red}{$-$}   & More difficult to interpret than other metrics.
	\end{tabular}
\end{table}

\begin{equation}
	\begin{aligned}
		\kappa &= \dfrac{P_o - P_e}{1 - P_e} \\
		P_e    &= \left(\dfrac{\textit{TP} + \textit{FP}}{T}\right) \cdot \left(\dfrac{\textit{TP} + \textit{FN}}{T}\right) + \left(\dfrac{\textit{TN} + \textit{FN}}{T}\right) \cdot \left(\dfrac{\textit{TN} + \textit{FP}}{T}\right)
%
		\label{equation:k}
	\end{aligned}
\end{equation}
%
\begin{conditions}
	\kappa & Cohen's kappa \\
	P_e    & expected accuracy \\
	P_o    & model accuracy
\end{conditions}
\myequations{Cohen's kappa for binary classification}

\hrule


\subsection[Gini impurity]{Gini impurity \cite{gini1912variabilita, breiman1984classification, manek2017aspect}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{The Gini impurity indicates the probability that two randomly selected samples from a data set have different original label types. A lower value indicates a higher purity of the data set. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Well-suited for application in decision tree algorithms. \\
		\textcolor{Red}{$-$}   & Favors binary decisions, which can result in decision trees of reduced complexity.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{Gini impurity}(D) = \sum\nolimits_{i = 1}^n \sum\nolimits_{i' \not= i} p_i p_{i'} = 1 - \sum\nolimits_{i = 1}^n p^2_i
%
	\label{equation:Gini_impurity}
\end{equation}
%
\begin{conditions}
	D & data set \\
	p & probability of samples
\end{conditions}
\myequations{Gini impurity}

\hrule


\subsection[P4 metric]{P\textsubscript{4} metric \cite{sitarz2023extending}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{The P\textsubscript{4} metric covers four probabilities, precision, recall, specificity, and NPV, at once, forming their harmonic mean. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Tends to zero when any of the four conditional probabilities tends to zero. Tends to one when all four conditional probabilities tend to one. \\
		\textcolor{Green}{$+$} & It is symmetrical with respect to data set labels swapping. \\
		\textcolor{Red}{$-$}   & Does not take weighting. \\
		\textcolor{Red}{$-$}   & Currently barely used metric.
	\end{tabular}
\end{table}

\begin{equation}
	P_4 = \dfrac{4}{\frac{1}{\textit{Precision}} + \frac{1}{\textit{Recall}} + \frac{1}{\textit{Specificity}} + \frac{1}{\textit{NPV}}} = \dfrac{4 \cdot \textit{TP} \cdot \textit{TN}}{4 \cdot \textit{TP} \cdot \textit{TN} + (\textit{TP} + \textit{TN}) \cdot (\textit{FP} + \textit{FN})}
%
	\label{equation:P4_metric}
\end{equation}
\myequations{P\textsubscript{4} metric}

\hrule


\subsection[Skill score (SS)]{Skill score (SS) \cite{murphy1988skill}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Measures the quality of a prediction against a reference. (range: $(-\infty, 1]$)} \\
		\textcolor{Green}{$+$} & Intuitive way to rank model performance. \\
		\textcolor{Red}{$-$}   & Scales indefinitely into the negative for larger variations.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{SS} = 1 - \dfrac{\textit{Metric}_{\textit{GT}}}{\textit{Metric}_P}
%
	\label{equation:SS}
\end{equation}
%
\begin{conditions}
	\textit{Metric}_{\textit{GT}} & best possible expectation for a model based on a given metric \\
	\textit{Metric}_P             & actual prediction of a model based on a given metric
\end{conditions}
\myequations{Skill score (SS)}

\hrule


\subsection[Relative improvement factor]{Relative improvement factor \cite{schlosser2022improving}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Measures the relative quality of a prediction against a reference. (range: $[0, \infty)$)} \\
		\textcolor{Green}{$+$} & Intuitive way to rank model performance. \\
		\textcolor{Red}{$-$}   & Scales indefinitely into the positive for larger variations.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{Relative improvement factor} = \dfrac{1 - \textit{Metric}_{\textit{GT}}}{1 - \textit{Metric}_P}
%
	\label{equation:relative_improvement_factor}
\end{equation}
%
\begin{conditions}
	\textit{Metric}_{\textit{GT}} & best possible expectation for a model based on a given metric \\
	\textit{Metric}_P             & actual prediction of a model based on a given metric
\end{conditions}
\myequations{Relative improvement factor}

\hrule




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Examples %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\subsection*{Examples}

Figure~\ref{figure:ML_example1} shows an example result for a classification problem with 3 imbalanced classes, while Fig.~\ref{figure:ML_example2} illustrates an example result for a classification problem with 3 balanced classes created from the Iris flower data set of R. A. Fisher \cite{fisher1936use}. Both examples show an exemplary confusion matrix on the top left together with the number of samples per class. The metrics recall and false negative rate (FNR) with their equations and example calculations are aligned to the rows while precision and false discovery rate (FDR) and their respective equations and calculations are aligned with the columns of the confusion matrix. Accuracy as well as micro, macro, and weighted precision, recall, and F1-scores are shown on the bottom right. Note that micro and macro average recall, precision, and F1-score remain constant for the balanced dataset while this is not the case for the imbalanced dataset. Since the examples discuss 3-class problems, we set $n = 3$ in the equations for, e.g., macro average precision, recall, and F1-score (Eq.~\ref{equation:MAAP}, \ref{equation:MAAR}, and \ref{equation:MAAF}, respectively).


\begin{figure}[H]
	\centering

	\resizebox{\textwidth}{!}{\begin{tikzpicture}[r/.style={draw, rectangle, minimum size=1cm}]
		\begin{scope}
			\node[r, fill=RoyalBlue, text=white] at (-1,  1) {41};
			\node[r, fill=Apricot!50]            at ( 0,  1) {11};
			\node[r, fill=Apricot!50]            at ( 1,  1) {12};
			\node[r, fill=Apricot!50]            at (-1,  0) {62};
			\node[r, fill=RoyalBlue, text=white] at ( 0,  0) {22};
			\node[r, fill=Apricot!50]            at ( 1,  0) {23};
			\node[r, fill=Apricot!50]            at (-1, -1) {31};
			\node[r]                             at ( 0, -1) {0};
			\node[r, fill=RoyalBlue, text=white] at ( 1, -1) {52};

			\node            at ( 0, 2) {Predicted class};
			\node[rotate=90] at (-2, 0) {Ground truth class};

			\node[rotate=60] at (-1, 3) {1: \textit{cat}};
			\node[rotate=60] at ( 0, 3) {2: \textit{fox}};
			\node[rotate=60] at ( 1, 3) {3: \textit{dog}};

			\node at (-3,  1) {1: \textit{cat}};
			\node at (-3,  0) {2: \textit{fox}};
			\node at (-3, -1) {3: \textit{dog}};
		\end{scope}

		\begin{scope}[xshift=3.5cm, inner sep=0, font=\footnotesize]
			\node[r] at (-1,    1) {64};
			\node[r] at ( 0.5,  1) {64.1\%};
			\node[r] at ( 1.6,  1) {35.9\%};
			\node[r] at (-1,    0) {107};
			\node[r] at ( 0.5,  0) {20.6\%};
			\node[r] at ( 1.6,  0) {79.4\%};
			\node[r] at (-1,   -1) {83};
			\node[r] at ( 0.5, -1) {62.7\%};
			\node[r] at ( 1.6, -1) {37.4\%};

			\node at (-1,   2) {$\sum$};
			\node at ( 0.5, 2) {$\textit{Recall}_i$};
			\node at ( 1.6, 2) {$\textit{FNR}_i$};

			\node at (6, 0.5) {
				\renewcommand*{\arraystretch}{3}\begin{tabular}{cccc}
					{Eq.~\{\ref{equation:recall}\}} & $\dfrac{\textit{TP}}{\textit{TP} + \textit{FN}}$ &  {Eq.~\{\ref{equation:FNR}\}} & $\dfrac{\textit{FN}}{\textit{FN} + \textit{TP}}$ \\
					\hline
					$\textit{Recall}_1$ & $\dfrac{41}{41 + 11 + 12}$ & $\textit{FNR}_1$ & $\dfrac{11 + 12}{11 + 12 + 41}$ \\
					\hline
					$\textit{Recall}_2$ & $\dfrac{22}{22 + 62 + 23}$ & $\textit{FNR}_2$ & $\dfrac{62 + 23}{62 + 23 + 22}$ \\
					\hline
					$\textit{Recall}_3$ & $\dfrac{52}{52 + 31 + 0}$ & $\textit{FNR}_3$ & $\dfrac{31 + 0}{31 + 0 + 52}$ \\
					\hline
				\end{tabular}};
		\end{scope}

		\begin{scope}[yshift=-3.5cm, inner sep=0, font=\footnotesize]
			\node[r] at (-1,  1)   {134};
			\node[r] at ( 0,  1)   {33};
			\node[r] at ( 1,  1)   {87};
			\node[r] at (-1, -0.5) {30.6\%};
			\node[r] at ( 0, -0.5) {66.7\%};
			\node[r] at ( 1, -0.5) {59.8\%};
			\node[r] at (-1, -1.6) {69.4\%};
			\node[r] at ( 0, -1.6) {33.3\%};
			\node[r] at ( 1, -1.6) {40.2\%};

			\node[r] at (2.5, 1) {254};

			\node at (-2.5,  1)   {$\sum$};
			\node at (-2.5, -0.5) {$\textit{Precision}_i$};
			\node at (-2.5, -1.6) {$\textit{FDR}_i$};

			\node at (-0.5, -5) {
				\renewcommand*{\arraystretch}{3}\begin{tabular}{c|c|c|c|}
					Eq.~\ref{equation:precision} & $\textit{Precision}_1$ & $\textit{Precision}_2$ & $\textit{Precision}_3$ \\
					$\dfrac{\textit{TP}}{\textit{TP} + \textit{FP}}$ & $\dfrac{41}{41 + 62 + 31}$ & $\dfrac{22}{22 + 11 + 0}$ & $\dfrac{52}{52 + 12 + 23}$ \\
					Eq.~\ref{equation:FDR} & $\textit{FDR}_1$ & $\textit{FDR}_2$ & $\textit{FDR}_3$ \\
					$\dfrac{\textit{FP}}{\textit{FP} + \textit{TP}}$ & $\dfrac{62 + 31}{62 + 31 + 41}$ & $\dfrac{11 + 0}{11 + 0 + 22}$ & $\dfrac{12 + 23}{12 + 23 + 52}$ \\
				\end{tabular}};
		\end{scope}

		\begin{scope}[shift={(9, -5)}, inner sep=0, font=\footnotesize]
			\node at (0, 0) {$\textit{Accuracy} = \dfrac{\textit{TP} + \textit{TN}}{\textit{TP} + \textit{TN} + \textit{FP} + \textit{FN}} = \dfrac{41 + 22 + 52}{41 + 22 + 52 + 11 + 12 + 62 + 23 + 31 + 0} = 45.28\%$};

			\node at (0, -3) {
				\begin{tabular}{|P{2cm}|P{1.75cm}|P{1.75cm}|P{1.75cm}|P{0.5cm}|}
					\hline
					Class & \textit{Precision} [\%] & \textit{Recall} [\%] & $F_1$ [\%] & $\sum$ \\
					\hline
					\noalign{\vskip 2pt}

					\hline
					1 & 30.60 & 64.06 & 41.41 & 64 \\
					\hline
					2 & 66.67 & 20.56 & 31.43 & 107 \\
					\hline
					3 & 59.77 & 62.65 & 61.18 & 83 \\
					\hline
					\noalign{\vskip 2pt}

					\cline{1-4}
					Macro average & 52.34 & 49.09 & 44.67 & \multicolumn{1}{c}{} \\
					\cline{1-4}
					Micro average & 45.28 & 45.28 & 45.28 & \multicolumn{1}{c}{} \\
					\cline{1-4}
					W. average    & 55.32 & 45.28 & 43.67 & \multicolumn{1}{c}{} \\
					\cline{1-4}
					\noalign{\vskip 2pt}

					\cline{2-4}
					\multicolumn{1}{c}{} & \multicolumn{1}{|c|}{Eq.~\{\ref{equation:precision},\ref{equation:MAAP}--\ref{equation:WAP}\}} & \multicolumn{1}{c|}{Eq.~\{\ref{equation:recall},\ref{equation:MAAR}--\ref{equation:WAR}\}} & \multicolumn{1}{c|}{Eq.~\{\ref{equation:F1-score},\ref{equation:MAAF}--\ref{equation:WAF}\}} & \multicolumn{1}{c}{} \\
					\cline{2-4}
				\end{tabular}};
		\end{scope}
	\end{tikzpicture}}

	\caption{Classification result example (1) showing a 3-class problem with imbalanced classes.}
	\label{figure:ML_example1}
\end{figure}


\begin{figure}[H]
	\centering

	\resizebox{\textwidth}{!}{\begin{tikzpicture}[r/.style={draw, rectangle, minimum size=1cm}]
		\begin{scope}
			\node[r, fill=RoyalBlue, text=white] at (-1,  1) {50};
			\node[r]                             at ( 0,  1) {0};
			\node[r]                             at ( 1,  1) {0};
			\node[r]                             at (-1,  0) {0};
			\node[r, fill=RoyalBlue, text=white] at ( 0,  0) {47};
			\node[r, fill=Apricot!50]            at ( 1,  0) {3};
			\node[r]                             at (-1, -1) {0};
			\node[r, fill=Apricot!50]            at ( 0, -1) {2};
			\node[r, fill=RoyalBlue, text=white] at ( 1, -1) {48};

			\node            at ( 0, 2) {Predicted class};
			\node[rotate=90] at (-2, 0) {Ground truth class};

			\node[rotate=60] at (-1, 3.5) {1: \textit{setosa}};
			\node[rotate=60] at ( 0, 3.5) {2: \textit{versicolor}};
			\node[rotate=60] at ( 1, 3.5) {3: \textit{virginica}};

			\node at (-3.5,  1) {1: \textit{setosa}};
			\node at (-3.5,  0) {2: \textit{versicolor}};
			\node at (-3.5, -1) {3: \textit{virginica}};
		\end{scope}

		\begin{scope}[xshift=3.5cm, inner sep=0, font=\footnotesize]
			\node[r] at (-1,    1) {50};
			\node[r] at ( 0.5,  1) {100\%};
			\node[r] at ( 1.6,  1) {0\%};
			\node[r] at (-1,    0) {50};
			\node[r] at ( 0.5,  0) {94\%};
			\node[r] at ( 1.6,  0) {6\%};
			\node[r] at (-1,   -1) {50};
			\node[r] at ( 0.5, -1) {96\%};
			\node[r] at ( 1.6, -1) {4\%};

			\node at (-1,   2) {$\sum$};
			\node at ( 0.5, 2) {$\textit{Recall}_i$ };
			\node at ( 1.6, 2) {$\textit{FNR}_i$};

			\node at (6, 0.5) {
				\renewcommand*{\arraystretch}{3}\begin{tabular}{cccc}
					Eq.~\ref{equation:recall} & $\dfrac{\textit{TP}}{\textit{TP} + \textit{FN}}$ & Eq.~\ref{equation:FNR} & $\dfrac{\textit{FN}}{\textit{FN} + \textit{TP}}$ \\
					\hline
					$\textit{Recall}_1$ & $\dfrac{50}{50 + 0 + 0}$ & $\textit{FNR}_1$ & $\dfrac{0 + 0}{0 + 0 + 50}$ \\
					\hline
					$\textit{Recall}_2$ & $\dfrac{47}{47 + 0 + 3}$ & $\textit{FNR}_2$ & $\dfrac{0 + 3}{0 + 3 + 47}$ \\
					\hline
					$\textit{Recall}_3$ & $\dfrac{48}{48 + 0 + 2}$ & $\textit{FNR}_3$ & $\dfrac{0 + 2}{0 + 2 + 48}$ \\
					\hline
				\end{tabular}};
		\end{scope}

		\begin{scope}[yshift=-3.5cm, inner sep=0, font=\footnotesize]
			\node[r] at (-1,  1)   {50};
			\node[r] at ( 0,  1)   {49};
			\node[r] at ( 1,  1)   {51};
			\node[r] at (-1, -0.5) {100\%};
			\node[r] at ( 0, -0.5) {95.9\%};
			\node[r] at ( 1, -0.5) {94.1\%};
			\node[r] at (-1, -1.6) {0\%};
			\node[r] at ( 0, -1.6) {4.1\%};
			\node[r] at ( 1, -1.6) {5.9\%};

			\node[r] at (2.5, 1) {150};

			\node at (-2.5,  1)   {$\sum$};
			\node at (-2.5, -0.5) {$\textit{Precision}_i$};
			\node at (-2.5, -1.6) {$\textit{FDR}_i$};

			\node at (-0.5, -5) {
				\renewcommand*{\arraystretch}{3}\begin{tabular}{c|c|c|c|}
					Eq.~\ref{equation:precision} & $\textit{Precision}_1$ & $\textit{Precision}_2$ & $\textit{Precision}_3$ \\
					$\dfrac{\textit{TP}}{\textit{TP} + \textit{FP}}$ & $\dfrac{50}{50 + 0 + 0}$ & $\dfrac{47}{47 + 0 + 2}$ & $\dfrac{48}{48 + 0 + 3}$ \\
					Eq.~\ref{equation:FDR} & $\textit{FDR}_1$ & $\textit{FDR}_2$ & $\textit{FDR}_3$ \\
					$\dfrac{\textit{FP}}{\textit{FP} + \textit{TP}}$ & $\dfrac{0 + 0}{0 + 0 + 50}$ & $\dfrac{0 + 2}{0 + 2 + 47}$ & $\dfrac{0 + 3}{0 + 3 + 48}$ \\
				\end{tabular}};
		\end{scope}

		\begin{scope}[shift={(9, -5)}, inner sep=0, font=\footnotesize]
			\node at (0, 0) {$\textit{Accuracy} = \dfrac{\textit{TP} + \textit{TN}}{\textit{TP} + \textit{TN} + \textit{FP} + \textit{FN}} = \dfrac{50 + 47 + 48}{50 + 47 + 48 + 0 + 0 + 0 + 3 + 0 + 2} = 96.7\%$};

			\node at (0, -3) {
				\begin{tabular}{|P{2cm}|P{1.75cm}|P{1.75cm}|P{1.75cm}|P{0.5cm}|}
					\hline
					Class & \textit{Precision} [\%] & \textit{Recall} [\%] & $F_1$ [\%] & $\sum$ \\
					\hline
					\noalign{\vskip 2pt}

					\hline
					1 & 100.00 & 100.00 & 100.00 & 50 \\
					\hline
					2 &  95.92 &  94.00 &  94.95 & 50 \\
					\hline
					3 &  94.12 &  96.00 &  95.05 & 50 \\
					\hline
					\noalign{\vskip 2pt}

					\cline{1-4}
					Macro average & 96.68 & 96.67 & 96.67 & \multicolumn{1}{c}{} \\
					\cline{1-4}
					Micro average & 96.67 & 96.67 & 96.67 & \multicolumn{1}{c}{} \\
					\cline{1-4}
					W. average    & 96.68 & 96.67 & 96.67 & \multicolumn{1}{c}{} \\
					\cline{1-4}
					\noalign{\vskip 2pt}

					\cline{2-4}
					\multicolumn{1}{c}{} & \multicolumn{1}{|c|}{Eq.~\{\ref{equation:precision},\ref{equation:MAAP}--\ref{equation:WAP}\}} & \multicolumn{1}{c|}{Eq.~\{\ref{equation:recall},\ref{equation:MAAR}--\ref{equation:WAR}\}} & \multicolumn{1}{c|}{Eq.~\{\ref{equation:F1-score},\ref{equation:MAAF}--\ref{equation:WAF}\}} & \multicolumn{1}{c}{} \\
					\cline{2-4}
				\end{tabular}};
		\end{scope}
	\end{tikzpicture}}

	\caption{Classification result example (2) showing a 3-class problem with balanced classes.}
	\label{figure:ML_example2}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Examples %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Functions %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\subsection*{Functions}

The following table gives an overview of machine learning metrics commonly used with the Python programming language machine learning libraries scikit-learn\footnote{scikit-learn project page, \url{https://scikit-learn.org/stable/}}, TensorFlow\footnote{TensorFlow project page, \url{https://www.tensorflow.org/}} (and Keras\footnote{Keras project page, \url{https://keras.io/}}), and PyTorch\footnote{PyTorch project page, \url{https://pytorch.org/}}\textsuperscript{,}\footnote{TorchMetrics project page, \url{https://torchmetrics.readthedocs.io/en/stable/}}.


\begin{table}[H]
	\centering

	\resizebox{\textwidth}{!}{\begin{tabular}{|l|c|c|c|}
		\hline
		Equation & scikit-learn & TensorFlow & PyTorch \\
		\hline
		%
		\hline

		True Positives
		&
		/
		&
		\href{https://www.tensorflow.org/api_docs/python/tf/keras/metrics/TruePositives}{TruePositives}
		&
		/
		\\

		True Negatives
		&
		/
		&
		\href{https://www.tensorflow.org/api_docs/python/tf/keras/metrics/TrueNegatives}{TrueNegatives}
		&
		/
		\\

		False Positives
		&
		/
		&
		\href{https://www.tensorflow.org/api_docs/python/tf/keras/metrics/FalsePositives}{FalsePositives}
		&
		/
		\\

		False Negatives
		&
		/
		&
		\href{https://www.tensorflow.org/api_docs/python/tf/keras/metrics/FalseNegatives}{FalseNegatives}
		&
		/
		\\

		\hline
		%
		\hline

		Precision (\ref{equation:precision})
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html\#sklearn-metrics-precision-score}{precision\_score}
		&
		\href{https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Precision}{Precision}
		&
		\href{https://torchmetrics.readthedocs.io/en/latest/classification/precision.html}{Precision}
		\\

		Recall (\ref{equation:recall})
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html\#sklearn.metrics.recall_score}{recall\_score}
		&
		\href{https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Recall}{Recall}
		&
		\href{https://torchmetrics.readthedocs.io/en/latest/classification/recall.html}{Recall}
		\\

		True negative rate (TNR) (\ref{equation:TNR})
		&
		/
		&
		/
		&
		\href{https://torchmetrics.readthedocs.io/en/latest/classification/specificity.html}{Specificity}
		\\

		Macro average precision (AP\textsubscript{macro}) (\ref{equation:MAAP})
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html\#sklearn.metrics.average_precision_score}{average\_precision\_score}
		&
		/
		&
		\href{https://torchmetrics.readthedocs.io/en/latest/classification/average_precision.html}{AveragePrecision}
		\\

		Micro average precision (AP\textsubscript{micro}) (\ref{equation:MIAP})
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html\#sklearn.metrics.average_precision_score}{average\_precision\_score}
		&
		/
		&
		\href{https://torchmetrics.readthedocs.io/en/latest/classification/average_precision.html}{AveragePrecision}
		\\

		Weighted average precision (AP\textsubscript{weighted}) (\ref{equation:WAP})
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html\#sklearn.metrics.average_precision_score}{average\_precision\_score}
		&
		/
		&
		\href{https://torchmetrics.readthedocs.io/en/latest/classification/average_precision.html}{AveragePrecision}
		\\

		Accuracy (A) (\ref{equation:A})
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\#sklearn.metrics.accuracy_score}{accuracy\_score}
		&
		\href{https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Accuracy}{Accuracy}
		&
		\href{https://torchmetrics.readthedocs.io/en/latest/classification/accuracy.html}{Accuracy}
		\\

		Balanced accuracy (BA) (\ref{equation:BA_binary},\ref{equation:BA_multi})
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html\#sklearn.metrics.balanced_accuracy_score}{balanced\_accuracy\_score}
		&
		/
		&
		/
		\\

		F-score (\ref{equation:F-score})
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html\#sklearn.metrics.fbeta_score}{fbeta\_score}
		&
		\href{https://www.tensorflow.org/api_docs/python/tf/keras/metrics/FBetaScore}{FBetaScore}
		&
		\href{https://torchmetrics.readthedocs.io/en/latest/classification/fbeta_score.html}{FBetaScore}
		\\

		F1-score (\ref{equation:F1-score})
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\#sklearn.metrics.f1_score}{f1\_score}
		&
		\href{https://www.tensorflow.org/api_docs/python/tf/keras/metrics/F1Score}{F1Score}
		&
		\href{https://torchmetrics.readthedocs.io/en/latest/classification/f1_score.html}{F1Score}
		\\

		Positive likelihood ratio (LR$+$) (\ref{equation:LR+})
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.class_likelihood_ratios.html\#sklearn.metrics.class_likelihood_ratios}{class\_likelihood\_ratios}
		&
		/
		&
		/
		\\

		Negative likelihood ratio (LR$-$) (\ref{equation:LR-})
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.class_likelihood_ratios.html\#sklearn.metrics.class_likelihood_ratios}{class\_likelihood\_ratios}
		&
		/
		&
		/
		\\

		Fowlkes--Mallows index (FM) (\ref{equation:FM})
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fowlkes_mallows_score.html\#sklearn.metrics.fowlkes_mallows_score}{fowlkes\_mallows\_score}
		&
		/
		&
		/
		\\

		Matthews correlation coefficient (MCC) (\ref{equation:MCC})
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html\#sklearn.metrics.matthews_corrcoef}{matthews\_corrcoef}
		&
		/
		&
		\href{https://torchmetrics.readthedocs.io/en/latest/classification/matthews_corr_coef.html}{MatthewsCorrCoef}
		\\

		Jaccard index (JI) (\ref{equation:JI})
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html\#sklearn.metrics.jaccard_score}{jaccard\_score}
		&
		/
		&
		\href{https://torchmetrics.readthedocs.io/en/latest/classification/jaccard_index.html}{JaccardIndex}
		\\

		Receiver operating characteristic curve (ROC curve)
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html\#sklearn.metrics.roc_curve}{roc\_curve}
		&
		/
		&
		\href{https://torchmetrics.readthedocs.io/en/latest/classification/roc.html}{ROC}
		\\

		Area under the curve (AUC) (\ref{equation:AUC})
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html\#sklearn.metrics.auc}{auc}
		&
		\href{https://www.tensorflow.org/api_docs/python/tf/keras/metrics/AUC}{AUC}
		&
		\href{https://torchmetrics.readthedocs.io/en/latest/classification/auroc.html}{AUROC}
		\\

		Average precision (AP) (\ref{equation:AP})
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html\#sklearn.metrics.average_precision_score}{average\_precision\_score}
		&
		/
		&
		\href{https://torchmetrics.readthedocs.io/en/latest/classification/average_precision.html}{AveragePrecision}
		\\

		Cohen's kappa (\ref{equation:k})
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html\#sklearn.metrics.cohen_kappa_score}{cohen\_kappa\_score}
		&
		/
		&
		\href{https://torchmetrics.readthedocs.io/en/latest/classification/cohen_kappa.html}{CohenKappa}
		\\

		\hline
	\end{tabular}}

	\caption{Selection of function calls for the available metrics in scikit-learn, TensorFlow, and PyTorch. The call for the respective metrics follows the corresponding scheme: scikit-learn~-- sklearn.metrics.<metric>, TensorFlow~-- tf.keras.metrics.<metric>, and PyTorch~-- torchmetrics.<metric>.}
	\label{table:ML_functions}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Functions %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\clearpage




\section{Computer Vision}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The following evaluation metrics within the context of machine learning are motivated by the contributions of \cite{wang2003multiscale, hore2010image}.

\vspace{1cm}
\hrule




\subsection*{General}

\begin{table}[H]
	\centering

	\begin{tabular}{|c|c|}
		\hline
		Abbreviation & Meaning \\
		\hline
		%
		\hline
		$\textit{GT}$ & Ground truth     \\
		$P$           & Prediction       \\
		$n$           & Number of values \\
		\hline
	\end{tabular}

	\caption{General definitions computer vision.}
	\label{table:CV_general}
\end{table}

\vspace{1cm}
\hrule




\clearpage




\subsection{Error (E)}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{The amount by which a prediction differs from the ground truth. (range: $(-\infty, \infty)$)} \\
		\textcolor{Green}{$+$} & Intuitive and straightforward to apply.
	\end{tabular}
\end{table}

\begin{equation}
	E = \textit{GT} - P
%
	\label{equation:E}
\end{equation}
\myequations{Error (E)}

\hrule


\subsection[Absolute error~/ sum of absolute errors (AE)]{Absolute error~/ sum of absolute errors (AE) \cite{richardson2004h}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Calculates the sum (total) of all absolute errors. (range: $[0, \infty)$)} \\
		\textcolor{Green}{$+$} & Straightforward to calculate. \\
		\textcolor{Red}{$-$}   & No differentiation depending on the number of compared values is made. \\
		\textcolor{Red}{$-$}   & Large individual differences equal to many small ones (distribution problem).
	\end{tabular}
\end{table}

\begin{equation}
	\textit{AE} = \sum\nolimits_{i = 1}^n |E_i| = \sum\nolimits_{i = 1}^n |\textit{GT}_i - P_i|
%
	\label{equation:AE}
\end{equation}
\myequations{Absolute error / sum of absolute errors (AE)}

\hrule


\subsection[Relative absolute error (RAE)]{Relative absolute error (RAE) \cite{armstrong1992error, armstrong2000another, rodrigues2017machine}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Normalization of the absolute error by dividing the total absolute error of the simple predictor. (range: $[0, \infty)$)} \\
		\textcolor{Green}{$+$} & Comparison of models that differ significantly. \\
		\textcolor{Red}{$-$}   & Not sensitive to outliers and scaling.
	\end{tabular}
\end{table}

\begin{equation}
	\begin{aligned}
		\textit{RAE}           &= \dfrac{\textit{AE}}{\sum\nolimits_{i = 1}^n |\textit{GT}_i - \overline{\textit{GT}}|} = \dfrac{\sum\nolimits_{i = 1}^n |\textit{GT}_i - P_i|}{\sum\nolimits_{i = 1}^n |\textit{GT}_i - \overline{\textit{GT}}|} \\
		\overline{\textit{GT}} &= \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \textit{GT}_i
%
		\label{equation:RAE}
	\end{aligned}
\end{equation}
%
\begin{conditions}
	\overline{\textit{GT}} & average of the ground truth
\end{conditions}
\myequations{Relative absolute error (RAE)}

\hrule


\subsection[Mean error (ME)]{Mean error (ME) \cite{fisher1920012, anjali2019temperature}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{The average over all error measurements. (range: $(-\infty, \infty)$)} \\
		\textcolor{Green}{$+$} & Intuitive and straightforward to apply. \\
		\textcolor{Red}{$-$}   & Positive and negative error values can cancel each other out.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{ME} = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n E_i = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n (\textit{GT}_i - P_i)
%
	\label{equation:ME}
\end{equation}
\myequations{Mean error (ME)}

\hrule


\subsection[Mean percentage error (MPE)]{Mean percentage error (MPE) \cite{pearson1895x, jiang2008prediction}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{The average over all error measurements in percentage. (range: $(-\infty\%, \infty\%)$)} \\
		\textcolor{Green}{$+$} & Intuitive overview of the underlying situation. \\
		\textcolor{Red}{$-$}   & Undefined as soon as a single ground truth value is zero.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{MPE} = \dfrac{100}{n} \cdot \sum\nolimits_{i = 1}^n \dfrac{E_i}{\textit{GT}_i} = \dfrac{100}{n} \cdot \sum\nolimits_{i = 1}^n \dfrac{\textit{GT}_i - P_i}{\textit{GT}_i}
%
	\label{equation:MPE}
\end{equation}
\myequations{Mean percentage error (MPE)}

\hrule


\subsection[Mean absolute error (MAE)]{Mean absolute error (MAE) \cite{willmott2005advantages, hyndman2006another}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Calculates the mean of the sum (total) of all absolute errors. (range: $[0, \infty)$)} \\
		\textcolor{Green}{$+$} & Partially solves the distribution problem. \\
		\textcolor{Red}{$-$}   & No differentiation depending on the maximum assumable error is made.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{MAE} = \dfrac{\textit{AE}}{n} = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n |\textit{GT}_i - P_i|
%
	\label{equation:MAE}
\end{equation}
\myequations{Mean absolute error (MAE)}

\hrule


\subsection[Mean absolute percentage error (MAPE)]{Mean absolute percentage error (MAPE) \cite{armstrong1992error, hyndman2006another}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{The average over all absolute error measurements in percentage. (range: $[0\%, \infty\%)$)} \\
		\textcolor{Green}{$+$} & Intuitive and scale independent. \\
		\textcolor{Red}{$-$}   & Undefined as soon as a single ground truth value is zero.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{MPE} = \dfrac{100}{n} \cdot \sum\nolimits_{i = 1}^n \dfrac{|E_i|}{|\textit{GT}_i|} = \dfrac{100}{n} \cdot \sum\nolimits_{i = 1}^n \dfrac{|\textit{GT}_i - P_i|}{|\textit{GT}_i|}
%
	\label{equation:MAPE}
\end{equation}
\myequations{Mean absolute percentage error (MAPE)}

\hrule


\subsection[Mean absolute scaled error (MASE)]{Mean absolute scaled error (MASE) \cite{hyndman2006another, mohan2018deep}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Mean absolute error of the measurements scaled by the mean absolute error of the ground truth. (range: $[0, \infty)$)} \\
		\textcolor{Green}{$+$} & Scale invariant. \\
		\textcolor{Red}{$-$}   & Less sensitive to outliers.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{MASE} = \dfrac{\textit{MPE}}{\left(\dfrac{1}{n} - 1\right) \cdot \sum\nolimits_{i = 2}^n |\textit{GT}_i - \textit{GT}_{i - 1}|} = \dfrac{\dfrac{100}{n} \cdot \sum\nolimits_{i = 1}^n \dfrac{\textit{GT}_i - P_i}{\textit{GT}_i}}{\left(\dfrac{1}{n} - 1\right) \cdot \sum\nolimits_{i = 2}^n |\textit{GT}_i - \textit{GT}_{i - 1}|}
%
	\label{equation:MASE}
\end{equation}
\myequations{Mean absolute scaled error (MASE)}

\hrule


\subsection[Mean normalized bias (MNB)]{Mean normalized bias (MNB) \cite{yu2006new, tsigaridis2014aerocom}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Calculates the variance between the predicted values and the ground truth values. Divides by the reference variable, subsequently calculating the mean. (range: $(-\infty, \infty)$)} \\
		\textcolor{Green}{$+$} & Enables the specific evaluation of systematic errors across the entire model. \\
		\textcolor{Red}{$-$}   & Does not detect specific errors in individual parts of the model.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{MNB} = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \dfrac{E_i}{P_i} = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \dfrac{\textit{GT}_i - P_i}{P_i}
%
	\label{equation:MNB}
\end{equation}
\myequations{Mean normalized bias (MNB)}

\hrule


\subsection[Normalized mean bias (NMB)]{Normalized mean bias (NMB) \cite{mebust2003models, yu2006new}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Calculates the average of the variances between the prediction and the reference variable, subsequently normalizing it by the reference variable. (range: $(-\infty, \infty)$)} \\
		\textcolor{Green}{$+$} & Comparison of models independently of scaling. \\
		\textcolor{Red}{$-$}   & Sensitive to outliers.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{NMB} = \dfrac{\sum\nolimits_{i = 1}^n E_i}{\sum\nolimits_{i = 1}^n P_i} = \dfrac{\sum\nolimits_{i = 1}^n (\textit{GT}_i - P_i)}{\sum\nolimits_{i = 1}^n P_i}
%
	\label{equation:NMB}
\end{equation}
\myequations{Normalized mean bias (NMB)}

\hrule


\subsection[Squared error~/ sum of squared errors (SE)]{Squared error~/ sum of squared errors (SE) \cite{draper1998applied}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Calculates the sum (total) of all squared errors. (range: $[0, \infty)$)} \\
		\textcolor{Green}{$+$} & Emphasizes the contribution of large errors. \\
		\textcolor{Red}{$-$}   & No differentiation depending on the number of compared values is made. \\
		\textcolor{Red}{$-$}   & Large individual differences equal to many small ones (distribution problem).
	\end{tabular}
\end{table}

\begin{equation}
	\textit{SE} = \sum\nolimits_{i = 1}^n E_i^2 = \sum\nolimits_{i = 1}^n (\textit{GT}_i - P_i)^2
%
	\label{equation:SE}
\end{equation}
\myequations{Squared error / sum of squared errors (SE)}

\hrule


\subsection[Mean square error (MSE)]{Mean square error (MSE) \cite{bickel2015mathematical}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Calculates the mean of the sum (total) of all squared errors. (range: $[0, \infty)$)} \\
		\textcolor{Green}{$+$} & Partially solves the distribution problem. \\
		\textcolor{Red}{$-$}   & No differentiation depending on the maximum assumable error is made.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{MSE} = \dfrac{\textit{SE}}{n} = \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n (\textit{GT}_i - P_i)^2
%
	\label{equation:MSE}
\end{equation}
\myequations{Mean square error (MSE)}

\hrule


\subsection[Root mean square error (RMSE)]{Root mean square error (RMSE) \cite{willmott2005advantages, hyndman2006another, pontius2008components}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Calculates the root of the mean of the sum (total) of all squared errors. (range: $[0, \infty)$)} \\
		\textcolor{Green}{$+$} & Provides a result in the range of the compared values. \\
		\textcolor{Red}{$-$}   & No differentiation depending on the maximum assumable error is made.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{RMSE} = \sqrt{\textit{MSE}} = \sqrt{\dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n (\textit{GT}_i - P_i)^2}
%
	\label{equation:RMSE}
\end{equation}
\myequations{Root mean square error (RMSE)}

\hrule


\subsection[Normalized root mean square error (NRMSE)]{Normalized root mean square error (NRMSE) \cite{chang2004air, kim2005missing}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Normalization of RMSE. (range: $[0, \infty)$)} \\
		\textcolor{Green}{$+$} & Comparison of models that differ significantly. \\
		\textcolor{Red}{$-$}   & Not sensitive to outliers and scaling.
	\end{tabular}
\end{table}

\begin{equation}
	\begin{aligned}
		\textit{NRMSE}         &= \dfrac{\textit{RMSE}}{\overline{\textit{GT}}} = \dfrac{\sqrt{\dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n (\textit{GT}_i - P_i)^2}}{\overline{\textit{GT}}} \\
		\overline{\textit{GT}} &= \dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n \textit{GT}_i
%
		\label{equation:NRMSE}
	\end{aligned}
\end{equation}
%
\begin{conditions}
	\overline{\textit{GT}} & average of the ground truth
\end{conditions}
\myequations{Normalized root mean square error (NRMSE)}

\hrule


\subsection[Root mean squared logarithmic error (RMSLE)]{Root mean squared logarithmic error (RMSLE) \cite{nafees2021predictive}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Calculates the mean squared error of the logarithmized ground truth in comparison to the logarithmized predictions. (range: $[0, \infty)$)} \\
		\textcolor{Green}{$+$} & Robust to outliers. \\
		\textcolor{Red}{$-$}   & Biased penalty. Underestimation is penalized more than overestimation.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{RMSLE} = \sqrt{\dfrac{1}{n} \cdot \sum\nolimits_{i = 1}^n (\ln(\textit{GT}_i + 1) - \ln(P_i + 1))^2}
%
	\label{equation:RMSLE}
\end{equation}
\myequations{Root mean squared logarithmic error (RMSLE)}

\hrule


\subsection[Peak signal-to-noise ratio (PSNR)]{Peak signal-to-noise ratio (PSNR) \cite{salomon2004data, huynh2008scope}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Calculates the MSE in relation to the maximum assumable error. (range: $[0, \infty)$)} \\
		\textcolor{Green}{$+$} & A differentiation depending on the maximum assumable error is made. \\
		\textcolor{Red}{$-$}   & No differentiation depending on structural similarities is made.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{PSNR} = 10 \cdot \log_{10} \dfrac{E_\textit{max}^2}{\textit{MSE}} = 10 \cdot \log_{10} \dfrac{E_\textit{max}^2}{\frac{1}{n} \cdot \sum\nolimits_{i = 1}^n (\textit{GT}_i - P_i)^2}
%
	\label{equation:PSNR}
\end{equation}
%
\begin{conditions}
	E_\textit{max} & maximum possible error
\end{conditions}
\myequations{Peak signal-to-noise ratio (PSNR)}

\hrule


\subsection[Structural similarity (SSIM)]{Structural similarity (SSIM) \cite{wang2004image, ghodrati2019mr}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Calculates the structural similarity using the mean, variance, and covariance. (range: $[-1, 1]$)} \\
		\textcolor{Green}{$+$} & Provides more accurate results by considering structural characteristics. \cite{wang2004image} \\
		\textcolor{Red}{$-$}   & Increased calculation complexity.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{SSIM} = \dfrac{(2 \mu_x \mu_y + c_1) (2 \sigma_{xy} + c_2)}{(\mu_x^2 + \mu_y^2 + c_1) (\sigma_x^2 + \sigma_y^2 + c_2)}
%
	\label{equation:SSIM}
\end{equation}
%
\begin{conditions}
	\mu_x$, $\mu_y       & mean       \\
	\sigma_x$, $\sigma_y & variance   \\
	\sigma_{xy}          & covariance \\
	c_1$, $c_2           & division stabilizers, e.g., $(0.01 \cdot 2^8 - 1)^2$ and $(0.03 \cdot 2^8 - 1)^2$ (8 bits per value)
\end{conditions}
\myequations{Structural similarity (SSIM)}

\hrule


\subsection[Structural dissimilarity (DSSIM)]{Structural dissimilarity (DSSIM) \cite{wang2004image, ghodrati2019mr}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Calculates the structural dissimilarity using the mean, variance, and covariance. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Provides more accurate results by considering structural characteristics. \cite{wang2004image} \\
		\textcolor{Red}{$-$}   & Increased calculation complexity.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{DSSIM} = \dfrac{1 - \textit{SSIM}}{2}
%
	\label{equation:DSSIM}
\end{equation}
\myequations{Structural dissimilarity (DSSIM)}

\hrule


\subsection[Intersection over union (IoU)]{Intersection over union (IoU) \cite{jaccard1912distribution, murphy1996finley, rezatofighi2019generalized, zou2023object}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Calculates the similarity of two sets of values via the intersection over the union of both sets. In machine learning also known as the Jaccard index (JI). (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Can be well used for image segmentation and object detection. \\
		\textcolor{Red}{$-$}   & No differentiation depending on the size of both sets is made.
	\end{tabular}
\end{table}

\begin{figure}[H]
	\centering

	% https://texample.net/tikz/examples/set-operations-illustrated-with-venn-diagrams/
	\begin{tikzpicture}
		\colorlet{draw_color}{LimeGreen}
		\colorlet{fill_color}{LimeGreen!50}
		\newcommand{\SetP}{(-1, 0) circle (1.5)}
		\newcommand{\SetQ}{( 1, 0) circle (1.5)}
		\tikzset{filled/.style={draw=draw_color, fill=fill_color}, outline/.style={draw=draw_color}}

		\node at (-4, 0) {$\textit{IoU} =$};
		\begin{scope}[yshift=2cm]
			\begin{scope}
				\clip \SetP;
				\fill[filled] \SetQ;
			\end{scope}
			\draw[outline] \SetP node {$\textit{GT}$};
			\draw[outline] \SetQ node {$P$};
			\node at (4, 0) {$|\textit{GT} \cap P|$};
		\end{scope}
		\draw (-3, 0) -- (3, 0);
		\begin{scope}[yshift=-2cm]
			\draw[filled] \SetP node {$\textit{GT}$} \SetQ node {$P$};
			\node at (4, 0) {$|\textit{GT} \cup P|$};
		\end{scope}
	\end{tikzpicture}

	\caption{Intersection over union (IoU).}
	\label{figure:IoU}
\end{figure}

\begin{equation}
	\textit{IoU} = \dfrac{|\textit{GT} \cap P|}{|\textit{GT} \cup P|}
%
	\label{equation:IoU}
\end{equation}
\myequations{Intersection over union (IoU)}

\hrule


\subsection[Dice coefficient (DC)]{Dice coefficient (DC) \cite{dice1945measures, sorenson1948method}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Calculates the similarity of two sets of values via twice the intersection over the sum of both sets. In machine learning also known as the F1-score. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Can be well used for image segmentation and object detection. \\
		\textcolor{Red}{$-$}   & No differentiation depending on the size of both sets is made.
	\end{tabular}
\end{table}

\begin{figure}[H]
	\centering

	% https://texample.net/tikz/examples/set-operations-illustrated-with-venn-diagrams/
	\begin{tikzpicture}
		\colorlet{draw_color}{LimeGreen}
		\colorlet{fill_color}{LimeGreen!50}
		\newcommand{\SetP}{(-1, 0) circle (1.5)}
		\newcommand{\SetQ}{( 1, 0) circle (1.5)}
		\tikzset{filled/.style={draw=draw_color, fill=fill_color}, outline/.style={draw=draw_color}}

		\node at (-5, 0) {$\textit{DC} =$};
		\begin{scope}[yshift=2cm]
			\node at (-3, 0) {$2 \cdot$};
			\begin{scope}
				\clip \SetP;
				\fill[filled] \SetQ;
			\end{scope}
			\draw[outline] \SetP node {$\textit{GT}$};
			\draw[outline] \SetQ node {$P$};
			\node at (5, 0) {$2 \cdot |\textit{GT} \cap P|$};
		\end{scope}
		\draw (-4, 0) -- (4, 0);
		\begin{scope}[yshift=-2cm]
			\draw[filled, xshift=-1cm] \SetP node {$\textit{GT}$};
			\node {$+$};
			\draw[filled, xshift=1cm] \SetQ node {$P$};
			\node at (5, 0) {$|\textit{GT}\,| + |P|$};
		\end{scope}
	\end{tikzpicture}

	\caption{Dice coefficient (DC).}
	\label{figure:DC}
\end{figure}

\begin{equation}
	\textit{DC} = \dfrac{2 \cdot |\textit{GT} \cap P|}{|\textit{GT}\,| + |P|}
%
	\label{equation:DC}
\end{equation}
\myequations{Dice coefficient (DC)}

\hrule


\subsection[Overlap coefficient (OC)]{Overlap coefficient (OC) \cite{szymkiewicz1934conlribution, sempson1947holarctic, bell1962mutual, goodall1978sample}}

\begin{table}[H]\centering
	\begin{tabular}{m{0.05\textwidth}m{0.85\textwidth}}
		\multicolumn{2}{m{0.95\textwidth}}{Calculates the similarity of two sets of values via the intersection over the smaller set of both sets. (range: $[0, 1]$)} \\
		\textcolor{Green}{$+$} & Can be well used for image segmentation and object detection. \\
		\textcolor{Green}{$+$} & A differentiation depending on the size of both sets is made. \\
		\textcolor{Red}{$-$}   & A differentiation based on set size can be detrimental.
	\end{tabular}
\end{table}

\begin{equation}
	\textit{OC} = \dfrac{|\textit{GT} \cap P|}{\min(|\textit{GT}\,|, |P|)}
%
	\label{equation:OC}
\end{equation}
\myequations{Overlap coefficient (OC)}

\hrule




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Functions %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\subsection*{Functions}

The following table gives an overview of computer vision metrics commonly used with the Python programming language machine learning libraries scikit-learn\footnote{scikit-learn project page, \url{https://scikit-learn.org/stable/}}, TensorFlow\footnote{TensorFlow project page, \url{https://www.tensorflow.org/}} (and Keras\footnote{Keras project page, \url{https://keras.io/}}), and PyTorch\footnote{PyTorch project page, \url{https://pytorch.org/}}\textsuperscript{,}\footnote{TorchMetrics project page, \url{https://torchmetrics.readthedocs.io/en/stable/}}.


\begin{table}[H]
	\centering

	\resizebox{\textwidth}{!}{\begin{tabular}{|l|c|c|c|}
		\hline
		Equation & scikit-learn & TensorFlow & PyTorch \\
		\hline
		%
		\hline

		Mean absolute error (MAE) (\ref{equation:MAE})
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html\#sklearn.metrics.mean_absolute_error}{mean\_absolute\_error}
		&
		\href{https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanAbsoluteError}{keras.losses.MeanAbsoluteError}
		&
		\href{https://torchmetrics.readthedocs.io/en/latest/regression/mean_absolute_error.html}{MeanAbsoluteError}
		\\

		Mean absolute percentage error (MAPE) (\ref{equation:MAPE})
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_percentage_error.html\#sklearn.metrics.mean_absolute_percentage_error}{mean\_absolute\_percentage\_error}
		&
		\href{https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanAbsolutePercentageError}{keras.losses.MeanAbsolutePercentageError}		
		&
		\href{https://torchmetrics.readthedocs.io/en/latest/regression/mean_absolute_percentage_error.html}{MeanAbsolutePercentageError}
		\\

		Mean square error (MSE) (\ref{equation:MSE})
		&
		\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html\#sklearn.metrics.mean_squared_error}{mean\_squared\_error}
		&
		\href{https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanSquaredError}{keras.losses.MeanSquaredError}
		&
		\href{https://torchmetrics.readthedocs.io/en/latest/regression/mean_squared_error.html\#mean-squared-error-mse}{MeanSquaredError}
		\\

		Root mean square error (RMSE) (\ref{equation:RMSE})
		&
		/
		&
		\href{https://www.tensorflow.org/api_docs/python/tf/keras/metrics/RootMeanSquaredError}{keras.metrics.RootMeanSquaredError}
		&
		/
		\\

		Peak signal-to-noise ratio (PSNR) (\ref{equation:PSNR})
		&
		/
		&
		\href{https://www.tensorflow.org/api_docs/python/tf/image/psnr}{image.psnr}
		&
		/
		\\

		Structural similarity (SSIM) (\ref{equation:SSIM})
		&
		/
		&
		\href{https://www.tensorflow.org/api_docs/python/tf/image/ssim}{image.ssim}
		&
		/
		\\

		Intersection over union (IoU) (\ref{equation:IoU})
		&
		/
		&
		\href{https://www.tensorflow.org/api_docs/python/tf/keras/metrics/IoU}{keras.metrics.IoU}
		&
		\href{https://torchmetrics.readthedocs.io/en/latest/detection/intersection_over_union.html}{detection.iou.IntersectionOverUnion}
		\\

		Dice coefficient (DC) (\ref{equation:DC})
		&
		/
		&
		/
		&
		\href{https://torchmetrics.readthedocs.io/en/latest/classification/dice.html}{Dice}
		\\

		\hline
	\end{tabular}}

	\caption{Selection of function calls for the available metrics in scikit-learn, TensorFlow, and PyTorch. The call for the respective metrics follows the corresponding scheme: scikit-learn~-- sklearn.metrics.<metric>, TensorFlow~-- tf.<metric>, and PyTorch~-- torchmetrics.<metric>.}
	\label{table:CV_functions}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Functions %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\clearpage




\section*{Author contributions}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Tobias Schlosser, Michael Friedrich, and Trixy Meyer conducted this contribution's conceptualization and writing process with the help of Danny Kowerko in extending this manuscript with examples.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\clearpage
\bibliographystyle{IEEEtran}
\bibliography{library}




\end{document}

